# Vorhersage-Verteilung




![Bayes:Start!](img/Golem_hex.png){width=5%}

```{r}
#| include: false
library(tidyverse)
library(gt)
library(patchwork)
library(easystats)
```




## Lernsteuerung


### Lernziele

Nach Absolvieren des jeweiligen Kapitels sollen folgende Lernziele erreicht sein.

Sie k√∂nnen ...

- erl√§utern, was eine Posteriori-Pr√§diktiv-Verteilung (PPV) ist, und inwiefern Sie vor √úbergewissheit sch√ºtzt
- eine informelle Modellpr√ºfung f√ºr das Beispiel aus dem Unterricht anhand der Posteriori-Pr√§diktiv-Verteilung durchf√ºhren



### Vorbereitung im Eigenstudium

- [Statistik1, Kap. "Daten verbildlichen"](https://statistik1.netlify.app/040-verbildlichen#vertiefung)


### Ben√∂tigte R-Pakete





```{r}
library(tidyverse)
library(ggpubr)  # dataviz
```





```{r QM2-Thema2-kleineModelle-28}
#| echo: false
n <- 10
n_success <- 6
n_trials  <- 9

d <-
  tibble(p_grid = seq(from = 0, to = 1, length.out = n),
         prior  = 1) %>% 
  mutate(likelihood = dbinom(n_success, 
                             size = n_trials, 
                             prob = p_grid)) %>% 
  mutate(unstand_post = (likelihood * prior),
         post = unstand_post / sum(unstand_post))

samples <-
  d %>%  # nimmt die Tabelle mit Posteriori-Daten,
  slice_sample(  # Ziehe daraus eine Stichprobe,
    n = 1e4,  # mit insgesamt n=10000 Elementen,
    weight_by = post,  # Gewichte nach Spalte mit Post-Wskt.,
    replace = T)  # Ziehe mit Zur√ºcklegen

```




## Der zwielichte Dozent: Stichproben-Vert. vs. Post-Vert.

In einer dunklen Gasse fordert Sie ein Statistik-Dozent zu einem Gl√ºcksspiel heraus^[Hier br√§uchte es ein passendes Meme; Vorschl√§ge bitte an mich.]. M√ºnzwurf; wenn er gewinnt, m√ºssen Sie 10 Euro zahlen. Gewinnen Sie, bekommen Sie 11 Euro. Klingt nach einer guten Partie, nicht war? Nat√ºrlich nehmen Sie sofort an. 

Sie spielen also M√ºnzwurf; der Dozent setzt auf Zahl. Sie spielen 10 Runden. Leider gewinnt der Dozent 9 von 10 Mal^[was er mit lautem Gel√§chter quittiert].

*Ist die M√ºnze fair oder zieht der mich √ºber den Tisch?*, das ist die Frage, 
die Ihnen brennend durch den Kopf zieht.

"Sind 9 von 10 Treffern noch realistisch erwartbar, wenn es mit rechten Dingen zugeht,
oder beweist das Ergebnis, dass die M√ºnze gezinkt ist?"

W√ºtend (und mit leeren Taschen) ziehen Sie von dannen.


Zusammengefasst: Daten: 9 von 10 Treffern beim M√ºnzwurf. Forschungsfrage: Ist die M√ºnze fair?


Schauen wir uns zun√§chst einmal an, wie wahrscheinlich 9 von 10 Treffern sind,
*wenn* die M√ºnze fair ist, s. @fig-stiprovert-post-zwielicht, links.


```{r QM2-Thema3-Post-befragen-21}
#| echo: false

p_stripro_vert <-
  tibble(
  Trefferzahl = rbinom(n = 1e4, size = 10, prob = 1/2)
) %>% 
  mutate(signifikant = ifelse(Trefferzahl %in% c(9,10), TRUE, FALSE)) %>% 
  ggplot() +
  aes(x = Trefferzahl, fill = signifikant) +
  geom_bar() +
  scale_x_continuous(breaks = 0:10) +
  theme(legend.position = c(0.1, 0.8)) +
  geom_vline(xintercept = 9) +
  labs(title = "Stichprobenverteilung f√ºr p=0.5")
```

Die *Stichprobenverteilung* zeigt, wie wahrscheinlich die empirischen Daten $D$ (z.B. 9 von 10 Treffer) sind, *gegeben* eines Parameterwerts $\pi$ (z.B. $p=0.5$): $Pr(D|\pi)$^[Das griechische kleine p wird "pi" genannt und $\pi$ geschrieben. Zur Erinnerung: Parameter- oder Populationskennwerte werden in der Statistik h√§ufig mit griechischen Buchstaben benannt, um sie von Stichprobenkennwerten abzugrenzen.].

Anders gesagt, die Stichprobenverteilung zeigt die Verteilung der Likelihoods eines bestimmten Parameterwerts.

:::callout-note
*Der p-Wert*

Der p-Wert ist die zentrale Statistik der Inferenzstatistik.
Er wird genutzt, um √ºber die Ablehnung einer Hypothese zu entscheiden.
In diesem Fall entspricht der p-Wert dem t√ºrkis markierten Fl√§chenanteil in @fig-stiprovert.
Ist dieser Anteil kleiner als 5% (der Gesamtfl√§che im Balkendiagramm),
so wird die Hypothese (hier: faire M√ºnze) verworfen.
Allgemeiner gesprochen berechnet sich der p-Wert als Summe der Likelihoods, 
die mindestens so extrem sind wie das beobachtete empirische Ergebnis.
:::


In der Bayes-Statistik ist die Post-Verteilung Dreh- und Angelpunkt der Entscheidung √ºber eine Hypothese.
In @fig-stiprovert-post-zwielicht (rehcts) ist die Posteriori-Verteilung f√ºr die Daten zum zwielichten Dozent dargestellt.



```{r zwielicht-daten}
#| echo: true

# Post-Verteilung:
d_zwielicht <-
  tibble(
    p_grid = seq( from=0 , to=1 , length.out=100),
    prior = 1,  # Priori-Gewichte
    likelihood = dbinom(8, size = 10, prob=p_grid) ,
    unstandardisierte_posterior = likelihood * prior ,
    posterior = unstandardisierte_posterior / sum(unstandardisierte_posterior))

# Stichproben ziehen aus der Posteriori-Verteilung:
samples_zwielicht <- 
  tibble(
    gewinnchance_muenze = sample(
      d_zwielicht$p_grid , 
      prob=d_zwielicht$posterior, 
      size=1e4, 
      replace=TRUE)) %>% 
  mutate(
    id = row_number())
```

```{r QM2-Thema3-Post-befragen-22}
#| echo: false
#| fig-cap:
#|   - "Post-Verteilung f√ºr den Parameter p zu den Daten des zwielichten Dozenten (9 von 10 Treffern im wiederholten M√ºnzwurf)"
#   fig-cap: "Stichprobenverteilung einer fairen M√ºnze"
#| label: fig-stiprovert-post-zwielicht
# layout-ncol: 2


p_samples_zwielicht <- 
samples_zwielicht %>% 
  ggplot() +
  aes(x = gewinnchance_muenze) +
  geom_histogram(fill = "grey60", bins = 20) +
  #geom_vline(xintercept = 0.9) +
  #geom_label(x = 0.8, y= 0, label = "Emp. Ergebnis") +
  labs(title = "Posteriori-Verteilung f√ºr p",
       subtitle = "Priori: Gleichverteilung; Daten: 9 von 10 Treffern, binomialverteilt",
       caption = "Das Dreieck zeigt die Wskt. eines Treffers bei einer fairen M√ºnze",
       x = "Gewinnchance der M√ºnze") +
  annotate("point", x = .5, y = 0, size = 5, color = "grey40", shape = 17)

p_samples_zwielicht
```

```{r}
#| eval: false
gghistogram(samples_zwielicht,
            x = "gewinnchance_muenze",
            title = "Posteriori-Verteilung f√ºr p",
            subtitle = "Priori: Gleichverteilung; Daten: 9 von 10 Treffern, binomialverteilt",
            xlab = "p (Gewinchance der M√ºnze)",
            fill = "grey60") +
  geom_vline(xintercept = 0.5)

```

Hilfe f√ºr die Funktion `gghistogram()` finden Sie auf der [Hilfeseite der Funktion](https://rpkgs.datanovia.com/ggpubr/reference/gghistogram.html).

Die *Posteriori-Verteilung* gibt die Wahrscheinlichkeit jedes Parameterwerts $p$ wider, gegeben der empirischen Daten $D$: $Pr(p|D)$. 


Die meisten Forschungsfragen lassen sich mit der Post-Verteilung beantworten, nicht mit der Stichprobenverteilung.


Jetzt k√∂nnen wir wieder die Post-Verteilung auslesen,
um die Hypothese zu beantworten.
Schauen wir uns einige Beispiel dazu an.


:::{#exm-zwielicht1}

### Einigerma√üen fair?

Wie wahrscheinlich ist es, dass die M√ºnze "einigerma√üen" fair ist,
sagen wir, eine Trefferwahrscheinlichkeit $0.45 < \pi < 0.55$^[zwischen 45% und 55% mit anderen Worten] aufweist?


```{r}
samples_zwielicht %>% 
  count(gewinnchance_muenze > 0.45 & gewinnchance_muenze < 0.55) %>% 
  mutate(prop = n/sum(n))
```

Die Wahrscheinlichkeit f√ºr eine "einigerma√üen faire" M√ºnze ist klein, etwa 5%!

:::


:::{#exr-vis-zwieliecht}
### Visualisieren Sie @fig-stiprovert-post-zwielicht
Sie finden Details zur Aufgabenstellung (sowie die L√∂sung üôà) [hier](https://datenwerk.netlify.app/posts/postvert-vis-zwielicht/postvert-vis-zwielicht).$\square$
:::


:::{#exm-zwielicht2}

### M√ºnze gezinkt?

Schauen wir uns an, wie wahrscheinlich es ist - gegeben der Daten und unserem Modell -
dass die M√ºnze massiv gezinkt ist. "Massiv" definieren wir dabei mit "mindestens 70% Trefferwahrscheinlichkeit"^[ja, das ist subjektiv], also $\pi >= .7$^[F√ºhrende Nullen bei Anteilen werden oft weggelassen, man schreibt also oft .7 wenn man 0.7 bzw. 70% meint. Das ist nicht nur k√ºrzer, sondern man wei√ü auch direkt dass es sich um einen Anteil handelt. Beh√§lt man die f√ºhrende Null bei, etwa 0.77, so w√ºrde das signalisieren, dass die Zahl auch gr√∂√üer als Null sein k√∂nnte.].

```{r}
samples_zwielicht %>% 
  count(gewinnchance_muenze > .7) %>% 
  mutate(prop = n / sum(n))
```

Wir finden eine recht hohe Wahrscheinlichkeit f√ºr eine "massive" Manipulation der M√ºnze.

:::


:::callout-important
Ist es nicht einfach und sch√∂n, wie wir mit Hilfe des Stichprobenziehens allerlei Forschungsfragen beantworten k√∂nnen?
Eine Post-Verteilung aus Stichproben erlaubt uns, viele Fragen mit einfachen Methoden,
n√§mlich schlichtes Z√§hlen, zu beantworten.
:::

Nat√ºrlich k√∂nnte (und sollte?) man unser Modell kritisieren.
Ist es wirklich sinnvoll, die Trefferwahrscheinlichkeit apriori als gleichverteilt anzunehmen?
Das hei√üt ja, wir glauben, dass eine Trefferwahrscheinlichkeit von 99,99999% genauso wahrscheinlich ist wie 50,55555%.
Auf der anderen Seite: Der Charme einer Gleichverteilung ist, dass sie objektiv ist,
in dem Sinne, dass wir keinerlei Information einflie√üen lassen.
Wir sind indifferent gegen√ºber dem Parameter $\pi$, der Trefferwahrscheinlichkeit.



:::callout-note
In einem zweiten Versuch k√∂nnten wir jetzt unsere Post-Verteilung als Priori-Verteilung 
nutzen. 
Das Ergebnis des ersten Versuchs wird dann hergenommen als Ausgangspunkt f√ºr 
einen zweiten Versuch.
Damit wird das Wissen der Wissenschaft weitergegeben^[√ºbrigens auf mathematisch gesehen ideale Art und Weise.], so wie es sein sollte.
:::



## Mit Stichproben neue Beobachtungen simulieren


Zur Erinnerung: Der Likelihood (L) zeigt die Wahrscheinlichkeit eine Trefferzahl gegeben eines bestimmten Parameterwerts.
In unseren Beispiel k√∂nnten wir z.B. die drei Likelihoods f√ºr $w=0,1,2$ ausrechnen, gegeben $N=2$ und $p = 0.5$:

```{r QM2-Thema3-Post-befragen-23}
L <- dbinom(0:2, size = 2, prob = 0.5)
L
```


Ah, die Wahrscheinlichkeit f√ºr 0 oder 2 Treffer betr√§gt 50%, wenn $pi=1/2$; f√ºr 1 Treffer betr√§gt sie entsprechend 50%^[das sollte uns bekannt vorkommen].



### Wir simulieren die Wasserzahl bei Globusw√ºrfen {#sec-rbinom}

Zur√ºck zu unserem Globusversuch!

Wir k√∂nnten uns jetzt Globusb√§lle basteln mit verschiedenen Wasseranteilen, 
und diese oft hochwerfen.
Damit k√∂nnten wir herausfinden, welche Trefferzahlen sich bei verschiedenen Wasseranteilen finden lassen w√ºrden.

Wer gerne bastelt, freut sich darauf. Kritischere Geister^[oder weniger bastelfreundliche] w√ºrden den Aufwand bem√§ngeln und die Frage nach dem Zweck der √úbung stellen^[bravo!].

:::callout-important
Wenn wir wissen, welche Trefferzahlen laut einem Modell zu erwarten sind,
k√∂nnen wir die *echten* (beobachteten) Trefferzahlen mit den laut Modell zu erwartenden vergleichen.
Damit haben wir eine Methode, mit dem wir ein Modell auf Herz und Nieren pr√ºfen k√∂nnen.
Ein schlechtes Modell wird mit seinen Vorhersagen an der Realit√§t scheitern: Erwartung des Modells und beobachtete Daten passen nicht zusammen.
Sagt ein Modell etwa $W=9$ vorher bei $N=9$, aber wir finden $W=0$,
so wird unser Vertrauen in das Modell ersch√ºttert sein.
Simulation von Trefferzahlen sind also ein Modell, um die Glaubw√ºrdigkeit unseres Golems zu pr√ºfen. (Nicht nur) bei Golems gilt: Vertrauen ist gut, Kontrolle ist besser.
:::


Los geht's: 
Wir simulieren $n=1$ neuen Globusversuch mit $N=2, p=0.7$ und z√§hlen die (Wasser-)Treffer:

```{r QM2-Thema3-Post-befragen-25}
set.seed(42)  # Zufallszahlen festlegen
rbinom(n = 1, size = 2, prob = .7)  # 0 Treffer (Wasser)
```


Das geht wie man sieht mit `rbinom`: *r* wie *random* (zuf√§llig) und *binom* wie binomial verteilt,
die M√ºnzwurfverteilung.

Hier sind die Argumente der Funktion `rbinom` noch etwas n√§her erkl√§rt:

```{r}
#| eval: false

rbinom(n = Wie oft soll der Versuch wiederholt werden?,
       size = Wie viele Globusw√ºrfe pro Versuch (Stichprobengr√∂√üe),
       prob = Wie hoch ist die Wahrscheinlichkeit f√ºr Wasser (bzw. f√ºr einen Treffer))
```


Weiter: Warum nicht $n=10$ neue Globusversuche simulieren?

```{r QM2-Thema3-Post-befragen-27, echo = TRUE}
rbinom(n = 10, size = 2, prob = 0.7)
```


"Simulieren" hei√üt hier, wir lassen den Computer den Globus werfen,
ganz anschaulich gesprochen.
Nat√ºrlich wirft der Computer nicht in Wirklichkeit einen Globus oder eine M√ºnze,
sondern er zieht aus der Menge `{0,1}` eine Zahl, und wir geben die Wahrscheinlichkeit f√ºr jedes der beiden Elemente vor, z.B. jeweils 50%.^[√úbrigens k√∂nnen Computer nicht echten Zufall erzeugen (das kann vermutlich niemand), aber durch gewisse verzwickte Rechnungen sind die Zahlen, die der Computer uns pr√§sentiert, nicht oder kaum vom "Zufall" zu unterscheiden, also z.B. gleichverteilt ohne besondere Muster.].

:::callout-important
Simulationsdaten geben Aufschluss, welche Daten (wie oft Wasser) man bei einem bestimmten Modell, $p,N$, erwarten kann. 
M√ºnzw√ºrfe - und analoge Versuche, wie Globusw√ºrfe - kann man in R mit `rbinom` erstellen (simulieren).
:::



### Traue niemals einem Golem (einem Modell)





![Never trust a Golem](img/5qmhmo.jpg){width=25%}


Quelle: https://imgflip.com/i/5qmhmo



Immer pr√ºfen und wachsam bleiben:

- (Inwieweit) decken sich die simulierten Daten mit den tats√§chlichen Beobachtungen?
- Wie realistisch sind die Modellannahmen?
- Kann man das Modell aus verschiedenen Perspektiven pr√ºfen?



## Mit guten Simulationen kommt man den wahren Werten nahe


Warum nicht $n=10^6$ neue Globusversuche simulieren^[Wer R nicht mag, ist eingeladen, diesen Versuch von Hand mit selbstgebastelten Globusb√§llen zu wiederholen.]:

```{r QM2-Thema3-Post-befragen-28, echo = TRUE}
draws <- 
  tibble(
    draws = rbinom(1e6, size = 2, prob = .7))

draws %>% 
  count(draws) %>% 
  mutate(prop = n / sum(n))
```

Diese simulierten H√§ufigkeiten sind sehr √§hnlich zu den theoretisch bestimmten H√§ufigkeiten mit `dbinom`: Unser Modell liefert plausible Vorhersagen^[Braver Golem!].

```{r QM2-Thema3-Post-befragen-29, echo = TRUE}
dbinom(0:2, size = 2, prob = .7)
```


## Stichprobenverteilung

Wir ziehen viele ($n=10^6$) Stichproben f√ºr unseren typischen Globusversuch: $N=9$ Globusw√ºrfe mit $p=0.7$. 

Wie viele Wasser (W) erhalten wir wohl typischerweise in diesem Versuch?
Die Verteilung der zu erwartenden Treffer ist in @fig-globus-striprovert dargestellt.


```{r QM2-Thema3-Post-befragen-30, echo = TRUE, results = "hide", eval = FALSE}
n_draws <- 1e6

draws_df <- 
  tibble(draws = rbinom(n_draws, size = 9, prob = .7))

draws_df %>% gghistogram(x = "draws")
```



```{r QM2-Thema3-Post-befragen-31}
#| echo: false
#| label: fig-globus-striprovert
#| out-width: "50%"
#| fig-cap: Anteile (Wahrscheinlichkeit), die man f√ºr jede Wasserzahl in unserem Globusversuch erwarten kann
n_draws <- 1e6
draws <- tibble(draws = rbinom(n_draws, 
                               size = 9, 
                               prob = .7))

# the histogram
draws %>% 
  ggplot(aes(x = draws)) +
  geom_histogram(binwidth = 1, center = 0,
                 color = "grey92", size = 1/10) +
  scale_x_continuous("Anzahl Wasser (W) pro Versuch",
                     breaks = seq(from = 0, to = 9, by = 1)) +
  scale_y_continuous("H√§ufigkeit",
                     labels = scales::scientific) +
  coord_cartesian(xlim = c(0, 9)) +
  theme(panel.grid = element_blank()) +
  labs(title = "Stichprobenverteilung f√ºr n=9 und p=.7 (binomial verteilt)")
```

Die *Stichprobenverteilung* zeigt, welche Stichprobendaten laut unserem Modell (einem bestimmten Parameterwert) zu erwarten sind. Wir k√∂nnen jetzt pr√ºfen, ob die echten Daten zu den Vorhersagen des Modells passen.

:::callout-note
Die Stichprobenverteilung ist keine empirische Verteilung: Wir f√ºhren diese vielen Versuche nicht wirklich durch^[Nur die extremen Bastelfreunde machen das], wir simulieren sie nur am Computer.
:::



## Die Posterior-Pr√§diktiv-Verteilung (PPV)


### Was ist die PPV und wozu ist sie gut?


Unsere Stichprobenverteilung zeigt, welche Trefferzahlen bei einem bestimmten Parameterwert, z.B. $\pi=.7$ in welchen Anteilen zu erwarten sind.
Allerdings sind wir uns ja nicht sicher, dass der Wasseranteil genau 70% betr√§gt.
Unser (Un-)Wissen √ºber den Wasseranteil wird ja gerade in der Post-Verteilung gespeichert.

Um eine ehrliche(re) Antwort auf die Frage zu erhalten, wie viele Treffer^[Im Globusversuch ist Wasser der "Treffer"; in einem M√ºnzwurf-Versuch k√∂nnte "Kopf" der Treffer und die Anzahl der geworfenen K√∂pfe die Trefferzahl sein] zu erhalten ist, m√ºssen wir die Post-Verteilung ber√ºcksichtigen.

Wir brauche ein Stichprobenverteilung f√ºr *jeden* Wert der Post-Verteilung.
Wenn wir dann die resultierenden Strichprobenverteilungen mitteln,
haben wir einen ehrlichen √úberblick √ºber die zu erwartenden Trefferzahlen.
Dabei sollten wir nat√ºrlich wahrscheinliche Parameterwerte h√∂her gewichten als unwahrscheinliche.
So sollte etwa der (hoch wahrscheinliche) Wasseranteil von 70% ein hohes Gewicht beim Mitteln der Stichprobenverteilung erhalten;
der sehr unwahrscheinliche Wasseranteil^[zumindest laut unserer Post-Verteilung] von 1% Wasser, sollte entsprechend weniger gewichtet werden, beim Zusammenfassen (d.h. Mittelwert bilden) der Stichprobenverteilungen.

Die resultierende Verteilung - gemittelte Stichprobenverteilungen √ºber alle Werte der Post-Verteilungen - nennt man *Posterior-Pr√§diktiv-Verteilung* (PPV).


:::callout-important

Die PPV entsteht als gewichteter Mittelwert der Stichprobenverteilungen. Die Gewichte sind die Wahrscheinlichkeiten (bzw. Wahrscheinlichkeitsdichten) der Post-Verteilung.
:::



:::{#exm-wozu-ppv}

## Magnus Lagrande braucht die PPV


Im Jahre $10^{99}$ wird das Universum von Magnus Lagrande regiert.
Die Weltraumbeh√∂rde, f√ºr die Sie arbeiten, ist ihm unterstellt.
Der Regent findet Ihre Untersuchungen zwar ganz nett,
aber leider versteht er keine Wahrscheinlichkeit.
Ist ihm zu abstrakt, sagt er. 
"Oder k√∂nnen Sie mir mal so eine Wahrscheinlichkeit in die Hand geben?
K√∂nnen Sie sagen, Achtung, da hinten rennt eine Wahrscheinlichkeit, fang sie!"
Magnus ist also ein Freund f√ºr des Konkreten. 
Einige einflussreiche Gruppen an Statistikis [unterst√ºtzen diese Haltung](https://www.routledge.com/Predictive-Inference/Geisser/p/book/9780367449919)

Jedenfalls h√§tte Magnus gerne eine Aussage wie 
"Vermutlich sehen wir beim n√§chsten Versuch irgendwas zwischen 4 und 7 Treffern".

Nat√ºrlich haben Sie den Anspruch,
eine wissenschaftlich belastbare Aussage zu t√§tigen.

Was nun? Sie m√ºssen sozusagen die Post-Verteilung in eine Post-Verteilung der Beobachtungen,
also der konkreten Werte - in diesem Fall die Anzahl der Wassertreffer - √ºbersetzen.
Genau das macht die PPV f√ºr Sie!
:::






### Visualisierung der PPV

Der Prozess des gewichteten Zusammenfassens der Stichprobenverteilungen ist in @fig-ppv dargestellt.

![PPV als gewichtetes Kombinieren der Stichprobenverteilungen](img/ppv.png){#fig-ppv}

Quelle: @mcelreath_statistical_2020



### PPV berechnen



<!-- Hier fehlt `posterior_predict` aus rstanarm oder `estimate_prediction` aus modelbased: https://easystats.github.io/modelbased/reference/estimate_expectation.html -->



Die PPV f√ºr unseren Standard-Globusversuch ($N=9$) berechnen wir so:

Wir berechnen viele (z.B. $10^4$) Stichprobenverteilungen.
Dabei m√ºssen wir jedes Mal fragen, wie gro√ü die Wahrscheinlichkeit $\pi$ f√ºr Wasser^[d.h. einen Treffer] ist.
Wasseranteile $\pi$,  die laut Post-Verteilung *wahrscheinlich* sind,
m√ºssen wir entsprechend *oft* als Parameterwert ($\pi$) der Stichprobenverteilung verwenden;
umgekehrt d√ºrfen  wir nur wenige Stichprobenverteilungen f√ºr unwahrscheinliche Parameterwerte erstellen.

Beispielsweise w√ºrden wir *viele* Stichprobenverteilungen f√ºr $\pi=.7$ erstellen;
f√ºr $\pi=0.01$ w√ºrden wir *wenige* Stichprobenverteilungen erstellen, s. @fig-ppv.

Gl√ºcklicherweise spiegelt unsere Stichproben-Postverteilung `samples` wahrscheinlichere Parameterwerte wieder, indem wahrscheinlichere Parameterwerte h√§ufiger vorkommen.


::: callout-note
Wahrscheinliche Parameterwerte kommen in der Stichproben-Postverteilung `samples` h√§ufiger vor.
Die H√§ufigkeit der Parameterwerte spiegelt die Wahrscheinlichkeit der jeweiligen Parameterwerte in der (theoretischen) Postverteilung wider.
:::


Schauen Sie sich vielleicht zur Erinnerung noch einmal die Definition von `samples` an,  s. @lst-post-sample. 
Tabelle `samples`, die aus Stichproben aus der Post-Verteilung besteht, ist (in Ausz√ºgen) in @tbl-postsample1 dargestellt. 
Wie die Post-Verteilung auf Basis von Stichproben dann aussieht sieht man in @fig-samples1.
Globusversuche kann man mit `rbinom` simulieren, s. @sec-rbinom.



Wir simulieren also viele (z.B $10^4$) Globusversuche, jeweils mit $N=9$ W√ºrfen.
Wahrscheinliche Parameterwerte, etwa $\pi=7$, sollen h√§ufiger verwendet werden (bei unseren vielen Globusversuchen) als unwahrscheinliche.

Praktischerweise sind die Werte in der Spalte `p_grid` in `samples` so h√§ufig vertreten, wie ihre Wahrscheinlichkeit es erwarten l√§sst. Hier ist ein Auszug aus `samples`:

```{r}
#| eval: false
samples %>% 
  select(p_grid) %>% 
  slice_head(n = 10)
```



```{r}
#| echo: false
samples %>% 
  select(p_grid) %>% 
  slice_head(n = 10) |> 
  print_md()
```

Wie man sieht, sind wahrscheinliche Parameterwerte h√§ufiger vertreten.^[An dieser Stelle sollten Sie sichd die ganze Spalte `p_grid` anschauen, um sich von dieser Behauptung mit eigenen Augen zu √ºberzeugen.]



`p_grid` ist also eine Liste^[technisch in R ein *Vektor*] von Parameterwerten, deren H√§ufigkeit die Wahrscheinlichkeit der Parameterwerte gewichtet.

Auf dieser Basis k√∂nnen wir die PPV erstellen:





```{r ppv1, echo = TRUE, eval = TRUE}
ppv <- 
  rbinom(1e4, 
         size = 9, 
         prob = samples$p_grid) %>% 
  as_tibble()

head(ppv)
```


Schauen wir uns ein Histogramm aller Trefferzahlen an, s. @fig-ppv2.^[Es kann auch dem Verst√§ndnis helfen, dass Sie sich alle Werte der Tabelle `ppv` selber in Ruhe anschauen, um sich zu √ºberzeugen, welche Wasserzahlen (Trefferzahlen) h√§ufiger und welche seltener vorkommen.]

```{r}
#| fig-cap: Die PPV f√ºr unseren Standard-Globusversuch (N=9)
#| label: fig-ppv2
#| echo: false
ppv_plot2 <-
  ppv %>% 
  ggplot() +
  aes(x = value) +
  geom_bar() +
  scale_x_continuous(
    breaks = 0:9)

ppv_plot2
```





Die PPV unseres Modells zeigt uns (@fig-ppv2), dass wir in k√ºnftigen Versuchen zumeist 6 Treffer zu erwarten haben. 
Aber ein relativer breiter Bereich an Treffern ist ebenfalls gut laut unserer PPV erwartbar.


:::callout-important
Die PPV zeigt, welche Beobachtungen laut unserem Modell h√§ufig und welche selten sind.
Die PPV zeigt keine Parameterwerte, sondern welche Daten (Beobachtungen, Wasserzahlen) wir in k√ºnftigen Versuchen wie h√§ufig erwarten k√∂nnen.
:::


:::{#exm-ppv1}

## Der n√§chste Planet

Nur zum Spa√ü spulen wir kurz die Zeit im Universum vor, sagen wir so $10^{99}$ Jahre.
Sie arbeiten bei einer Raumfahrtbeh√∂rde, die nach neuen Planeten sucht.
Nun wurde ein aussichtsreicher Planet gesichtet. 
Ihre Beh√∂rde hat eine Studie gestartet, im Rahmen derer 9 Sonden zu diesem (weit entfernten) Planeten geschossen sind. Von den 9 Sonden sind 6 im Wasser gelandet,
was aus Gr√ºnden intergalaktischer Wasserknappheit eine gute Nachricht ist.


>   "Der n√§chste Planet wird sicher 6 von 9 Wassertreffer erzielen!"

-- Presse-Chefi der intergalaktischer SpaceY Raumfahrtsbeh√∂rde



Jetzt plant Ihre Beh√∂rde den Versuch zu wiederholen: Wieder sollen 9 Sonden zu diesem Planeten geschossen werden.
Dis Presse-Chefi^[In der Zeit dieses Beispiels ist es √ºblich, kein fixes Geschlecht zu haben] t√∂nt vollmundig: "Ich bin sicher, dass wir wieder 6 von 9 Treffer, also 6 von 9 Mal Wasser, haben werden!".

Kann man diese Aussage mit (hoher) Sicherheit leisten? Perfekte Sicherheit gibt es bekanntlich nur, was Tod und Steuern betrifft, aber kann diese Aussage mit zumindest hoher Sicherheit geleistet werden?

Nein, die PPV (@fig-ppv2) zeigt deutlich, dass unser Wissen nicht ausreicht, um pr√§zise Vorhersagen √ºber k√ºnftige Ausg√§nge des Versuchs zu leisten. So sind auch 5 oder 7 Treffer gut m√∂glich. Auch 4 oder 8 Treffer sind nicht so selten. Sogar 9 Treffer sind nicht super selten.

Dis Presse-Chefi Ihrer Beh√∂rde sollte also den Mund nicht so voll nehmen.

:::



## Fazit


### Vorhersagen sind schwierig


... gerade wenn sie die Zukunft betreffen, so ein Sprichwort.

Das zeigt uns die PPV: Der PPV unseres Modells gelingt es zwar, der theoretisch wahrscheinlichste Parameterwert ist auch der h√§ufigste in unseren Stichproben, aber die Vorhersagen haben eine gro√üe Streuung, bergen also recht hohe Ungewissheit.
Die PPV zeigt also, welche Beobachtungen laut unserem Modell k√ºnftig zu erwarten sind, s. @fig-ppv2.

W√ºrde man die Vorhersagen nur anhand eines bestimmten Parameterwertes (z.B $p=0.6$) vornehmen, h√§tten die Vorhersagen *zu wenig Streuung* in den Vorhersagen, w√ºrden also die Ungewissheit nicht ausreichend abbilden. Es w√ºrde *√úbergewissheit* (Overconfidence, Overfitting) resultieren.

[Wir brauchen die PPV](https://imgflip.com/i/6zm1hh). Ohne die PPV k√∂nnen wir nicht seri√∂s absch√§tzen, wie viel Ungewissheit in unseren Vorhersagen steckt.






### Zwei Arten von Ungewissheit in Vorhersagen von Modellen


1. *Ungewissheit innerhalb des Modells* ("intrinsische" Ungewissheit): Auch wenn der (oder die) Modellparameter eines Modells mit Sicherheit bekannt sind, so bleibt Unsicherheit, welche Beobachtung eintreten wird: Auch wenn man sicher wei√ü, dass $p=1/4$ Murmeln blau sind, so kann man nicht sicher sagen, welche Farbe die n√§chste Murmel haben wird (Ausnahme: $p=1$ oder $p=0$).

2. *Ungewissheit in den Modellparametern*: Wir sind uns nicht sicher, welchen Wert $p$ (bzw. die Modellparameter) haben. Diese Unsicherheit ist in der Post-Verteilung dargestellt. 

Um zu realistischen Vorhersagen zu kommen, m√∂chte man beide Arten von Ungewissheit ber√ºcksichtigen: Das macht die *Posteriori-Pr√§diktiv-Verteilung (PPV)*.



Die PPV zeigt, welche Daten das Modell vorhersagt (pr√§diktiv) und mit welcher H√§ufigkeit, basierend auf der Post-Verteilung.


:::callout-note
Der Unterschied zwischen der Post-Verteilung und der PPV ist erstmal,
dass die PPV *Auspr√§gungen* in ihrer Wahrscheinlichkeit bemisst,
also z.B. wie wahrscheinlich 4 von 9 Wassertreffern sind.
Die Post-Verteilung bemisst die Wahrscheinlichkeit von Parameterwerten,
also z.B. des Wasseranteils.

Etwas tiefer betrachtet zeigt die PPV zwei Arten von Ungewissheit,
die Post-Verteilung nur eine. 
Die PPV zeigt erstens die Ungewissheit zur Verteilung des Parameters (wie die Post-Verteilung),
aber auch noch die intrinsische Ungewissheit.
Denn auch wenn wir keine Ungewissheit zum Parameter h√§tten,
bliebe Ungewissheit, welche Beobachtungen sich manifestieren.
Insofern ist die PVV "ehrlicher",
sie spiegelt die Ungewissheit zu den Beobachtungen wider.
:::



### Vergleich der Verteilungen

@fig-post-pred-ppv-anim stellt die in diesem Kapitel diskutierten Verteilungen gegen√ºber:



- Links - *Posterior-Verteilung*: Wahrscheinlichkeiten der Parameterwerte
- Mitte - *Stichprobenverteilung*: Wahrscheinlichkeiten der Beobachtungen gegeben eines bestimmten Parameterwertes
- Rechts - *Posterior-Pr√§diktiv-Verteilung*: Wahrscheinlichkeiten der Beobachtungen unter Ber√ºcksichtigung der Unsicherheit der Posteriori-Verteilung

```{r}
#| echo: false
#| label: fig-post-pred-ppv-anim
#| fig-cap: Post- vs. Stichproben- vs. PP-Verteilungen
if (knitr:::is_html_output()) {
  knitr::include_graphics("img/post-pred-ppv-anim.gif")
}
```






[Quelle: R. McElreath](https://twitter.com/rlmcelreath/status/1448978045247893505)






### So viele Verteilungen... 




- Die *Posteriori-Verteilung* gibt Aufschluss zur H√§ufigkeit (Wahrscheinlichkeit) von Parameterwerten:
    - Wie wahrscheinlich ist es, dass "in Wirklichkeit" der Wasseranteil 70% betr√§gt, also $\pi=.7$
    - In der Wissenschaft ist man meist an den Parametern interessiert.
    
- Die *PPV* gibt Aufschluss zur H√§ufigkeit von neuen Beobachtungen:
    - Welche Beobachtungen (wie viele Wasser/Treffer) sind in Zukunft, bei erneuter Durchf√ºhrung, zu erwarten.
    - F√ºr die Praxis kann das eine interessante Frage sein.
    
- Der *Likelihood* gibt Aufschluss, wie gut eine bestimmte Hypothese die Datenlage erkl√§rt.
    - Wie gut passt die Hypothese $\pi=0.7$ auf die Datenlage 6 von 9 Treffern beim Globusversuch?
    - Der Likelihood kann aus der Stichprobenverteilung herausgelesen werden. 

```{r QM2-Thema3-Post-befragen-33}
#| echo: false
n_draws <- 1e5

simulate_binom <- function(probability) {
  set.seed(3)
  rbinom(n_draws, size = 9, prob = probability) 
}

d_small <-
  tibble(probability = seq(from = .1, to = .9, by = .1)) %>% 
  mutate(draws = purrr::map(probability, simulate_binom)) %>% 
  unnest(draws) %>% 
  mutate(label = str_c("p = ", probability))
```

```{r PP2}
#| echo: false
ppv2_plot <- 
d_small %>%
  ggplot(aes(x = draws)) +
  geom_histogram(binwidth = 1, center = 0,
                 color = "grey92") +
  scale_x_continuous("Wasser", breaks = seq(from = 0, to = 9, by = 3)) +
  scale_y_continuous(NULL, breaks = NULL) +
  labs(subtitle = "Stichprobenverteilungen") +
  coord_cartesian(xlim = c(0, 9)) +
  theme(panel.grid = element_blank()) +
  facet_wrap(~ label, ncol = 9) 
```



## Aufgaben


1. [Zwielichter Dozent-Bayes](https://datenwerk.netlify.app/posts/zwielichter-dozent-bayes/zwielichter-dozent-bayes)
2. [Warum Bayes?](https://datenwerk.netlify.app/posts/warum-bayes/warum-bayes)
3. [subjektiv-Bayes](https://datenwerk.netlify.app/posts/subjektiv-bayes/subjektiv-bayes)
4. [Likelihood2](https://datenwerk.netlify.app/posts/likelihood2/likelihood2)
5. [Anteil-Apple](https://datenwerk.netlify.app/posts/anteil-apple/anteil-apple)
6. [ReThink3m1](https://datenwerk.netlify.app/posts/rethink3m1/rethink3m1)
7. [ReThink3m2](https://datenwerk.netlify.app/posts/rethink3m2/rethink3m2)
8. [ReThink3m3](https://datenwerk.netlify.app/posts/rethink3m3/rethink3m3)
9. [ReThink3m4](https://datenwerk.netlify.app/posts/rethink3m4/rethink3m4)
10. [ReThink3m5](https://datenwerk.netlify.app/posts/rethink3m5/rethink3m5)
11. [Quiz zu Verteilungen](https://datenwerk.netlify.app/#category=Verteilungen-Quiz)
12. [postvert-vis-zwielicht](https://datenwerk.netlify.app/posts/postvert-vis-zwielicht/postvert-vis-zwielicht)











## ---



![](img/outro-07.jpg){width=100%}





