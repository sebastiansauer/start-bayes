{
  "hash": "e693b3b71207fb9acb7d7e61fc875896",
  "result": {
    "markdown": "# Globusversuch\n\n\n\n\n\n![Bayes:Start](img/Golem_hex.png){width=5%}\n\n\n\n\n## Lernsteuerung\n\n\n### Lernziele\n\nNach Absolvieren des jeweiligen Kapitel sollen folgende Lernziele erreicht sein.\n\nSie k√∂nnen ...\n\n\n- Unterschiede zwischen Modellen und der Realit√§t erl√§utern\n- die Binomialverteilung heranziehen, um geeignete (einfache) Modelle zu erstellen\n- die weite Einsetzbarkeit anhand mehrerer Beispiele exemplifizieren\n- Post-Wahrscheinlichkeiten anhand der Gittermethode berechnen\n\n\n\n\n\n\n\n\n\n## Von Welten und Golems\n\n### Kleine Welt, gro√üe Welt\n\nBekanntlich segelte Kolumbus 1492 los, und entdeckte Amerika. Das war aber ein gl√ºcklicher Zufall, denn auf seinem Globus existierte Amerika gar nicht. Vielleicht sah sein Globus so aus wie der von Behaim, s. Abb @fig-behaim.\n\n![Behaims Globus: Kein Amerika](img/Behaim.jpg){#fig-behaim}\n\n[Quelle: Ernst Ravenstein, Wikimedia, Public Domain](https://commons.wikimedia.org/wiki/File:RavensteinBehaim.jpg)\n\nDie *kleine Welt des Modells* entsprach hier nicht *der gro√üen Welt, der echten Erdkugel*.\n\nDas ist ein Beispiel, das zeigt, wie Modellieren schiefgehen kann. Es ist aber auch ein Beispiel f√ºr, sagen wir, die Komplexit√§t wissenschaftlicher (und sonstiger) Erkenntnis. Einfach gesagt: Gl√ºck geh√∂rt halt auch dazu.\n::: callout-note\nBehaims Globus ist nicht gleich der Erde. Die kleine Welt ist nicht die gro√üe Welt.\n:::\n\nWas in der kleinen Welt funktioniert, muss nicht in der gro√üen Welt funktionieren. Modelle zeigen immer nur die kleine Welt: Vorsicht vor schnellen Schl√ºssen und vermeintlicher Gewissheit.\n\nüèã Nennen Sie ein Beispiel, in dem ein Modell nicht (exakt) der Wirklichkeit entspricht!\n\n\n\n\n\n\n### Der Golem von Prag\n\n![Der Golem von Prag](img/170px-Golem_and_Loew.jpg)\n\n[Quelle](https://de.wikipedia.org/wiki/Golem)\n\nDer Golem von Prag, eine vom Menschen geschaffene Kreatur gewaltiger Kraft, die Befehle w√∂rtlich ausf√ºhrt.\nBei kluger F√ºhrung kann ein Golem N√ºtzliches vollbringen.\nBei un√ºberlegter Verwendung wird er jedoch gro√üen Schaden anrichten.\n\n### Wissenschaftliche Modelle sind wie Golems\n\n**Golem**\n\n![\"Yeah, ich bin ein Golem!\" - Bildquelle: Klara Schaumann](img/Golem_hex.png){width=25%}\n\n\nEigenschaften des Golems:\n\n-   Besteht aus Lehm\n-   Belebt durch \"Wahrheit\"\n-   M√§chtig\n-   dumm\n-   F√ºhrt Befehle w√∂rtlich aus\n-   Missbrauch leicht m√∂glich\n-   M√§rchen\n\n**Modell**\n\nEigenschaften des Modells:\n\n\n```{mermaid}\nflowchart LR\nX --> Y\n```\n\n\n-   Besteht aus ~~Lehm~~Silikon\n-   Belebt durch Wahrheit (?)\n-   Manchmal m√§chtig\n-   simpler als die Realit√§t\n-   F√ºhrt Befehle w√∂rtlich aus\n-   Missbrauch leicht m√∂glich\n-   Nicht einmal falsch\n\n::: callout-important\nWir bauen Golems.\n:::\n\n\nVergleichen wir die kleine Welt unserer Modellen, wie Behaims Globus, mit der Gro√üen Welt, die Kolumbus und wir befahren.\n\n\n\n\n| Kleine Welt                                                | Gro√üe Welt                                 |\n|-----------------------------------------|-------------------------------|\n| Die Welt, wie sie der Golem sieht                          | Die Welt, wie sie in Wirklichkeit ist      |\n| ist das Modell, aber nicht (zwangsl√§ufig) die Wirklichkeit | entspricht nicht (zwangsl√§ufig) dem Modell |\n| Verwenden wir beim Modellieren                             | Ist das, was wir modellieren               |\n\n: Kleine Welt vs. gro√üe Welt\n\n\n\n\n### So denkt unser Bayes-Golem\n\n![So denkt unser Bayes-Golem](img/bayesupdate2.png)\n\nüèã Bayes-Inferenz √§hnelt dem Lernen von Menschen. Geben Sie ein Beispiel von Lernen bei Menschen, das oben dargestelltem Prozess √§hnelt!\n\n## Ein erster Versuch: Wir werfen den Globus\n\n\n\n\n\n### Welcher Anteil der Erdoberfl√§che ist mit Wasser bedeckt?\n\nUnsere Hypothese bzw. unsere Forschungsfrage lautet, mit welchem Anteil die Erde wohl mit Wasser bedeckt ist?\n\n![Der Erdball](img/earth.png){width=\"50%\"}\n\n[Quelle](https://pngimg.com/image/25340) CC 4.0 BY-NC\n\nSie werden einen Globus-Ball in die Luft und fangen in wieder auf. Sie notieren dann, ob die Stelle unter Ihrem Zeigefinger Wasser zeigt (W) oder Land (L). Den Versuch wiederholen Sie 9 Mal.\n\nSo sah mein Ergebnis aus:\n\n$$W \\quad L \\quad W \\quad W \\quad W \\quad L \\quad W \\quad L \\quad W$$\n\nüèãÔ∏èÔ∏è Besorgen Sie sich einen Globus (zur Not eine M√ºnze) und stellen Sie den Versuch nach!\n\n### Wie entstanden die Daten?\n\nDer physikalische Prozess, der zur Entstehung der Daten f√ºhrt, nennt man den *den datengenierende Prozess*.\n\nIn diesem Fall kann man ihn so beschreiben:\n\n1.  Der wahre Anteil von Wasser, $W$, der Erdoberfl√§che ist $p$ (und $1-p$ ist der Anteil Land, $L$).\n2.  Ein Wurf des Globusballes hat die Wahrscheinlichkeit $p$, eine $W$-Beobachtung zu erzeugen.\n3.  Die W√ºrfe des Globusballes sind unabh√§ngig voneinander.\n4.  Wir haben kein Vorwissen √ºber $p$; jeder Wert ist uns gleich wahrscheinlich.\n\nüèã Welche Annahmen w√ºrden Sie √§ndern? Welche k√∂nnte man wegnehmen? Welche hinzuf√ºgen? Was w√§ren die Konsequenzen?\n\n\n\n\n\n\n### Ein paar Fachbegriffe\n\n-   F√ºr jede Hypothese haben wir ein Vorab-Wissen, das die jeweilige Plausibilit√§t der Hypothese angibt: *Priori-Verteilung*.\n\n-   F√ºr jede Hypothese (d.h. jeden *Parameterwert* $p$) m√∂chten wir wie wahrscheinlich die Daten sind (unter der Annahme, dass die Hypothese richtig ist). Das gibt uns den *Likelihood*.\n\n-   Dann gewichten wir den Likelihood mit dem Vorabwissen, so dass wir die *Posteriori-Verteilung*^[ Anstatt von *Priori* liest man auch *Prior*; anstatt *Posteriori* auch *Posterior*] bekommen.\n\n![Updating mit Bayes](img/bayesupdate.png)\n\n### Die Binomialverteilung\n\nWir nehmen an, dass die Daten unabh√§ngig voneinander entstehen und sich der Parameterwert nicht zwischenzeitlich √§ndert^[Die sog. \"iid-Annahme\", *i*ndependently and *i*dentically distributed: Jeder Wurf der Globusballes ist eine Realisation der gleichen Zufallsvariablen. Jeder Wurf ist unabh√§ngig von allen anderen: Das Ergebnis eines Wurfes hat keinen (stochastischen) Einfluss auf ein Ergebnis anderer W√ºrfe. Die Wahrscheinlichkeitsverteilung ist bei jedem Wurf identisch.].\n\nLassen Sie uns im Folgenden die Wahrscheinlichkeit ($Pr$), $W$ mal Wasser und $L$ mal Land zu beobachten, wenn die Wahrscheinlichkeit f√ºr Wasser $p$ betr√§gt, so bezeichnen: $(Pr(W,L | p))$.\nDiese Wahrscheinlichkeit - $(Pr(W,L | p))$ - kann man mit der *Binomialverteilung* berechnen.\n\nM√∂chte man die Wahrscheinlichkeit ansprechen f√ºr das Ereignis \"5 mal Wasser und 2 mal Land, wenn wir von einem Wasseranteil von 70% ausgehen\", so w√ºrden wir kurz schreiben: $Pr(W=5, L=2 | p=.7)$.\n\nDie Binomialverteilung zeigt die Verteilung der H√§ufigkeit (Wahrscheinlichkeit) der Ereignisse (z.B. 2 Mal Kopf) beim wiederholten M√ºnzwurf (und allen vergleichbaren Zufallsexperimenten): \"M√ºnzwurfverteilung\"\n\n$$Pr(W,L|p) = \\frac{(W+L)!}{W!L!}p^W(1-p)^L$${#eq-binomial}\n\n\nFormel @eq-binomial kann wie folgt auf Deutsch √ºbersetzen:\n\n>   Die Wahrscheinlichkeit f√ºr das Ereignis \"W,L\" gegeben p berechnet als Produkt von zwei Termen. Erstens der Quotient von der Fakult√§t von W plus L im Z√§hler und im Nenner das Produkt von erstens der Fakult√§t von W mit zweitens der Fakult√§t von L. Der zweite Term ist das Produkt von p hoch W mal der komplement√§ren Wahrscheinlichkeit von p hoch L.\n\n\nPuh, Formeln sind vielleicht doch ganz praktisch. \nNoch praktischer ist es aber, dass es Rechenmaschinen gibt, die die Formel kennen und f√ºr uns ausrechnen. \nLos, R, mach mal.\n\n### Binomialverteilung mit R\n\n\nWas ist der Anteil der g√ºltigen Pfade in einem Baumdiagramm (Wahrscheinlichkeit), um 2 mal $W$ bei $N=W+L=3$ W√ºrfen zu bekommen, wenn wir von $p=1/2$ ausgehen?^[Allgemeiner spricht man auch von 2 Treffern bei 3 W√ºrfen (d.h. 1 \"Nicht-Treffer\", den wir als \"Niete\" bezeichnen). Treffer werden oft mit `1`  und Niten mit `0` bezeichnet].\n\n\n\n::: {.cell hash='Globusversuch_cache/html/QM2-Thema2-kleineModelle-21a_468cf76ebafcb26bbe3f2dc9d18ad54a'}\n\n```{.r .cell-code}\ndbinom(x = 2, size = 3, prob = 1/2)\n## [1] 0.375\n```\n:::\n\n\n\nVon den 8 Endkonten bzw. Pfaden sind 3 g√ºnstig. \nDemnach ist die Wahrscheinlichkeit des gesuchten Ereignis (2 Treffer bei 3 W√ºrfen, binomialverteilt) gleich 3 von 8.\n\n\n\n\n\n\n```{mermaid}\n%%| echo: false\n%%| ref-label: fig-binom1\n%%| fig-cap: Wir werfen den Globus (oder eine M√ºnze) 3 Mal\nflowchart TD\n  A[A - Start] -. 1/2 .-> B[B - 0]\n  A -. 1/2 .-> C[C - 1]\n  B -. 1/2 .-> D[D - 0]\n  B -. 1/2 .-> E[E - 1]\n  C -. 1/2 .-> F[F - 0]\n  C -. 1/2 .-> G[G - 1]\n  D -. 1/2 .-> H[H - 0]\n  D -. 1/2 .-> J[I - 1]\n  E -. 1/2 .-> K[K - 0]\n  E -. 1/2 .-> L[L - 1]\n  F -. 1/2 .-> M[M - 0]\n  F -. 1/2 .-> N[N - 1]\n  G -. 1/2 .-> O[O - 0]\n  G -. 1/2 .-> P[P - 1]\n```\n\n\n\nAbb. @fig-binom1 stellt einen einfachen Baum f√ºr 3 Globusw√ºrfe mit je zwei m√∂glichen Ereignissen (W vs. L) dar.\nIn der ersten (obersten) Zeile (Knoten A; \"Start\") ist Ausgangspunkt dargestellt: Der Globus ruht wurfbereit in unserer Hand.\nJezt Achtung: Sie werfen den Globusball hoch.\nDie Pfeile zeigen zu den (zwei) m√∂gliche Ergebnissen.\nDie zweite Zeile (Knoten B und C) stellt die beiden Ergebnisse des Wurfes dar. \nDie Ergebnisse sind hier mit `0` und `1` bezeichnet (das eine eine einfache und weiteinsetzbare Notation).\nDie dritte Zeile (Knoten D bis G) stellt die Ergebnisse des des zweiten Wurfes dar.\nDie vierte Zeile (Knoten H bis P)  stellt die Ergebnisse des des dritten Wurfes dar.\n\n\nF√ºr mehr W√ºrfe w√ºrde das Diagramm irgendwann un√ºbersichtlich werden.\n\n\n\nWas ist der Anteil der g√ºltigen Pfade in einem Baumdiagramm (Wahrscheinlichkeit), um 6 mal $W$ bei $N=W+L=9$ W√ºrfen zu bekommen, wenn wir von $p=1/2$ ausgehen?\n\n\n::: {.cell hash='Globusversuch_cache/html/QM2-Thema2-kleineModelle-21_e4f6cd369bca193610d456446d344c98'}\n\n```{.r .cell-code}\ndbinom(x = 6, size = 9, prob = 1/2)\n## [1] 0.1640625\n```\n:::\n\n\n\nAbb @fig-binom2 ist ein vergeblicher Versuch, so einen gro√üen Baum darzustellen.\n\n:::callout-note\nVisualisierungen wie Baumdiagramme sind eine praktische Hilfe zum Verst√§ndnis,\nkommen aber bei gr√∂√üeren Daten schnell an ihre Grenze.\n:::\n\n\n\n::: {.cell hash='Globusversuch_cache/html/unnamed-chunk-4_58b24db80ea7929717150cdae90a94c2'}\n::: {.cell-output-display}\n![Wir werfen den Globus (oder eine M√ºnze) 3 Mal](Globusversuch_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n:::\n\n\n\nGeb\n\nWas ist die Wahrscheinlichkeit f√ºr $W=9$ bei $N=9$ und $p=1/2$?\n\n\n::: {.cell hash='Globusversuch_cache/html/QM2-Thema2-kleineModelle-22_6996052ebd49820f906dec4483347378'}\n\n```{.r .cell-code}\ndbinom(x = 9, size = 9, prob = 1/2)\n## [1] 0.001953125\n```\n:::\n\n\n### Beispiele zur Berechnung einer binomial verteilten Wahrscheinlichkeit\n\nEi Professi stellt einen Klausur mit 20 Richtig-Falsch-Fragen. Wie gro√ü ist die Wahrscheinlichkeit, durch blo√ües M√ºnze werfen genau 15 Fragen richtig zu raten?^[Hey, endlich mal was f√ºr echte Leben!].\n\n\n::: {.cell hash='Globusversuch_cache/html/QM2-Thema2-kleineModelle-23_917bf399b8b80ee19da8834a73a0f92b'}\n\n```{.r .cell-code}\ndbinom(x = 15, size = 20, prob = .5)\n## [1] 0.01478577\n```\n:::\n\n\nWas ist die Wahrscheinlichkeit bei 3 M√ºnzw√ºrfen (genau) 3 Treffer (Kopf) zu erzielen?\n\n\n::: {.cell hash='Globusversuch_cache/html/QM2-Thema2-kleineModelle-24_6107f93c4e1de2dbe4915092227abc31'}\n\n```{.r .cell-code}\ndbinom(3, 3, 1/2)\n## [1] 0.125\n```\n:::\n\n\n### Unser Modell ist geboren\n\nWir fassen das Globusmodell so zusammen:\n\n$$W \\sim \\text{Bin}(N,p),$$\n\nLies: \"W ist *bin*omial verteilt mit den Parametern $N$ und $p$\". $N$ gibt die Anzahl der Globusw√ºrfe an: $N=W+L$.\n\nUnser Vorab-Wissen zu $p$ sei, dass uns alle Werte gleich plausibel erscheinen (\"uniform\"):\n\n$$p \\sim \\text{Unif}(0,1).$$\n\nLies: \"$p$ ist gleich (uniform) verteilt mit der Untergrenze 0 und der Obergrenze 1\".\n\n\n\n\n\n\n\n\n\n\n### So sehen die Verteilungen aus\n\nAbb. @fig-bin zeigt die Binomialverteilung.\n\n\n::: {.cell hash='Globusversuch_cache/html/fig-bin_54c13b29ba2b2c87705e370cd09a7c19'}\n::: {.cell-output-display}\n![Ein Beispiel f√ºr eine Binomialverteilung](Globusversuch_files/figure-html/fig-bin-1.png){#fig-bin width=672}\n:::\n:::\n\n\n$N=9, p = 1/2$\n\nAbb. @fig-unif zeigt ein Beispiel f√ºr eine Gleichverteilung (uniform distribution).\n\n\n::: {.cell hash='Globusversuch_cache/html/fig-unif_8df4099f2f804c4edf2cff08cadb08d5'}\n::: {.cell-output-display}\n![Gleichverteilung](Globusversuch_files/figure-html/fig-unif-1.png){#fig-unif width=672}\n:::\n:::\n\n\n$Min = 0, Max = 1$\n\nüèãÔ∏èÔ∏è Was f√§llt Ihnen bei der Binomialverteilung auf? Ist sie symmetrisch? Ver√§ndert sich die Wahrscheinlichkeit linear? Was f√§llt Ihnen bei der Gleichverteilung auf?\n\n## Zur Erinnerung: Bayes Theorem\n\n### Herleitung Bayes' Theorem 1/2: Gemeinsame Wahrscheinlichkeit\n\nDie Wahrscheinlichkeit f√ºr *Regen* und *kalt* ist gleich der Wahrscheinlihckeit von *Regen*, *gegeben kalt* mal der Wahrscheinlicht von *kalt*. Entsprechend gilt: Die Wahrscheinlichkeit von $W$, $L$ und $p$ ist das Produkt von $Pr(W,L|p)$ und der Prior-Wahrscheinlichkeit $Pr(p)$:\n\n$$Pr(W,L,p) = Pr(W,L|p) \\cdot Pr(p)$$\n\nGenauso gilt: Die Wahrscheinlichkeit von *Regen* und *kalt* ist gleich der Wahrscheinlichkeit *kalt, wenn's regnet* mal der Wahrscheinlichkeit von *Regen*:\n\n$$Pr(W,L,p) = Pr(p|W,L) \\cdot Pr(W, L)$$\n\n### Herleitung Bayes' Theorem 2/2: Posteriori-Wahrscheinlichkeit\n\nWir setzen die letzten beiden Gleichungen gleich:\n\n$$Pr(W,L|p) \\cdot Pr(p) = Pr(p|W,L) \\cdot (W,L)$$\n\nUnd l√∂sen auf nach der Posteriori-Wahrscheinlichkeit^[k√ºrzen wir mit Post-Wahrscheinlichkeit or $Pr(Post)$ ab], $Pr(p|W,L)$:\n\n$$Pr(p|W,L) = \\frac{Pr(W,L|p) Pr(p)}{Pr(W,L)}$$\n\n$Pr(W,L)$ nennt man die *mittlere Wahrscheinlichkeit der Daten* oder *Evidenz*. Die Evidenz berechnet sich als Mittelwert der Likelihoods √ºber alle Werte von $p$. Die Aufgabe dieser Gr√∂√üe ist nur daf√ºr zu sorgen, dass insgesamt Werte zwischen 0 und 1 herauskommen.\n\n### Bayes' Theorem als Formel\n\n$$Pr(H|D) = \\frac{Pr(D|H) Pr(H)}{Pr(D)}$$\n\n-   Bestandteile:\n\n    -   Posteriori-Wahrscheinlichkeit: $Pr_{Post} := Pr(H|D)$\n\n    -   Likelihood: $L := Pr(D|H)$\n\n    -   Priori-Wahrscheinlichkeit: $Pr_{Priori} := Pr(H)$\n\n    -   Evidenz: $E := Pr(D)$\n\n-   Bayes' Theorem gibt die $Pr_{Post}$ an, wenn man die Gleichung mit der $Pr_{Priori}$ und dem $L$ f√ºttert.\n\n-   Bayes' Theorem wird h√§ufig verwendet, um die $Pr_{Post}$ zu quantifizieren.\n\n-   Die $Pr_{Post}$ ist proportional zu $L \\times Pr_{Priori}$.\n\n### Posteriori als Produkt von Priori und Likelihood\n\nDie unstandardisierte Post-Wahrscheinlichkeit ist einfach das Produkt von Likelihood und Priori.\n\nDas Standardisieren dient nur dazu, einen Wert zwischen 0 und 1 zu erhalten. \nDies erreichen wir, indem wir durch die Summe aller Post-Wahrscheinlichkeiten dividieren.\nDie Summe der Post-Wahrscheinlichkeiten bezeichnet man (auch) als Evidenz, vgl. Gleichung @eq-post.\n\n\n$$\\text{Posteriori} = \\frac{\\text{Likelihood} \\times \\text{Priori}}{\\text{Evidenz}}$${#eq-post}\n\n\n\nAbb. @fig-post3 visualisiert, dass die Post-Verteilung eine Gewichtung von Priori und Likelihood ist.\nMathematisch gesprochen beruht diese Gewichtung auf einer einfachen Multiplikationen der beiden genannten Terme.\n\n\n\n![Prior mal Likelihood = Post](img/img241.png){#fig-post3}\n\n\n\n\n\n\n\n### Wissen updaten: Wir f√ºttern Daten in das Modell\n\nGolemns k√∂nnen lernen?! Abb. @fig-lernen-golem zeigt die Post-Verteilung, nach $n=1, 2, ...,n=9$ Datenpunkten, d.h. W√ºrfen mit dem Globusball.\nMan sieht: Am Anfang, apriori, also bevor die Daten haben, vor dem ersten Wurf also, ist jeder Parameterwert gleich wahrscheinlich f√ºr den Golem (das Modell).\nJe nach Ergebnis des Wurfes ver√§ndert sich die Wahrscheinlichkeit der Parameterwerte,\nkurz gesagt, die Post-Verteilung ver√§ndert sich in Abh√§ngigkeit von den Daten.\n\n\n![Unser Golem lernt](img/img221.png){#fig-lernen-golem}\n\n\nInsofern kann man sagen: Unser Golem (das Modell) lernt. Ob das Modell n√ºtzlich ist (pr√§zise Vorhersagen liefert), steht auf einem anderen Blatt.\n\n\n\n## Bayes berechnen mit mit der Gitter-Methode\n\nDie Methode *Gitter-Ann√§herung* nennt man auch Grid Approximation\\*.\n\n### Idee\n\n1.  Teile den Wertebereich des Parameter in ein \"Gitter\" auf, z.B. $0.1, 0.2, ..., 0.9, 1$ (\"Gitterwerte\").\n2.  Bestimme den Priori-Wert des Parameters f√ºr jeden Gitterwert.\n3.  Berechne den Likelihood f√ºr Gitterwert.\n4.  Berechne den unstandardisierten Posteriori-Wert f√ºr jeden Gitterwert (Produkt von Priori und Likelihood).\n5.  Standardisiere den Posteriori-Wert durch teilen anhand der Summe alle unstand. Posteriori-Werte.\n\n\nF√ºr jeden \"Gitterwert\" berechnen wir eine (Post-)Wahrscheinlichkeit. Ein Gitterwert ist eine m√∂gliche Auspr√§gung des Parameters. H√§ufig entspricht eine Hypothese einem Gitterwert, etwa wenn man sagt: \"Ich glaube, die M√ºnze ist fair\", was auf einem Parameterwert von 50% herausl√§uft.\nDazu geben wir an, f√ºr wie wahrscheinlich wie apriori^[synonym: priori] - also bevor wir irgendwelche Daten erheben - jeden einzelnen Gitterwert halten. Wir machen es uns hier einfach und halten jeden Gitterwert f√ºr gleich wahrscheinlich. Tats√§chlich ist der konkrete Wert hier egal, entscheidend ist das Verh√§ltnis der Apriori-Werte zueinander: Geben wir einigen Gitterwerten den Wert 2, aber anderen den Wert 1, so halten wir Erstere f√ºr (apriori) doppelt so plauibel wie Letztere.\nDer Likelihood wird in diesem Fall mit der Binomialverteilung berechnet. Der Likelihood gibt an, wie wahrscheinlich ein Gitterwert ist gegeben einem bestimmten apriori gew√§hlten Parameterwert.\nDie \"End-Wahrscheinlichkeit\" die \"hinten rauskommt\" ist einfach das Produkt von Priori-Wert und Likelihood.\nAnschaulich gesprochen: Die Priori-Werte werden mit den Likelihoodwerten gewichtet^[synonym: Die Likelihoodwerte werden mit den Apriori-Werten gewichtet.].\nDa wir letztlich eine Wahrscheinlichkeitverteilung bekommen m√∂chten teilen wir jeden Posteriori-Wert durch die Summe aller Posteriori-Werte. Dadurch ist gerantiert, dass sich die Posteriori-Werte zu eins aufaddieren. Damit haben wir dann die Kolmogorov-Anspr√ºche an eine Wahrscheinlichkeitsverteilung erf√ºllt.\n\n\n### Gitterwerte in R berechnen\n\nLegen wir uns eine Tabelle mit Gittewerten an, um deren Posteori-Wahrscheinlichkeit zu berechnen.\n\n\n::: {.cell hash='Globusversuch_cache/html/QM2-Thema2-kleineModelle-28_b90e2507f22fa2dfa4496145fcf8a9a8'}\n\n```{.r .cell-code}\nd <-\n  tibble(\n    # definiere das Gitter: \n    p_Gitter = seq(from = 0, to = 1, length.out = 10),\n    # bestimme den Priori-Wert:       \n    Priori  = 1) %>%  \n    mutate(\n      # berechne Likelihood f√ºr jeden Gitterwert:\n      Likelihood = dbinom(6, size = 9, prob = p_Gitter),\n      # berechen unstand. Posteriori-Werte:\n      unstd_Post = Likelihood * Priori,\n      # berechne stand. Posteriori-Werte (summiert zu 1):\n      Post = unstd_Post / sum(unstd_Post))  \n```\n:::\n\n\nSo sehen unsere \"Gitterdaten\" aus:\n\n\n::: {.cell hash='Globusversuch_cache/html/QM2-Thema2-kleineModelle-29_0d2504b23c20883910cf76306bd693c1'}\n::: {.cell-output-display}\n| p_Gitter| Priori| Likelihood| unstd_Post| Post|\n|--------:|------:|----------:|----------:|----:|\n|     0.00|      1|       0.00|       0.00| 0.00|\n|     0.11|      1|       0.00|       0.00| 0.00|\n|     0.22|      1|       0.00|       0.00| 0.01|\n|     0.33|      1|       0.03|       0.03| 0.04|\n|     0.44|      1|       0.11|       0.11| 0.12|\n|     0.56|      1|       0.22|       0.22| 0.24|\n|     0.67|      1|       0.27|       0.27| 0.30|\n|     0.78|      1|       0.20|       0.20| 0.23|\n|     0.89|      1|       0.06|       0.06| 0.06|\n|     1.00|      1|       0.00|       0.00| 0.00|\n:::\n:::\n\n\nüèãÔ∏è Was wohl mit *Post* passiert, wenn wir *Priori* √§ndern?\n\n### Was sagt die Post?\n\nDie Posteriori-Verteilung (Kurz: \"Post-Verteilung\"), $Pr_{Post}$, zeigt, wie plausibel wir jeden Wert von $p$ halten.\n\n\nAbb. @fig-gitter zeigt die Post-Wahrscheinlichkeit f√ºr 5, 10 und 20 Gitterwerte. Das mittlere Teilbild (10 Gitterwerte) entspricht unserer Tabelle oben.\n\n\n![Je mehr Gittewerte, desto genauer wird die Verteilung wiedergegeben.](img/img242.png){#fig-gitter}\n\n\n:::callout-note\nUnter sonst gleichen Umst√§nden gilt:\n\n- Mehr Gitterwerte gl√§tten die Ann√§herung.\n- Je gr√∂√üer die Stichprobe ($N$), desto zuverl√§ssiger wird unsere Berechnung.\n:::\n\n::: callout-important\nDie Post-Verteilung ist sowas wie das Ziel all Ihrer Tr√§ume (falls Sie es noch nicht gewusst haben):\nAus der Post-Verteilung k√∂nnen Sie ablesen,\nwie wahrscheinlich Ihre Hypothese (Ihr Lieblings-Parameterwert) ist. Und noch einiges mehr, aber das ist Thema des n√§chsten Kapitels.\n:::\n\n\n\n\n## Abschluss\n\n### Zusammenfassung\n\n-   In unserem Modell haben wir Annahmen zu $Pr_{Priori}$ und $L$ getroffen.\n\n-   Auf dieser Basis hat der Golem sein Wissen geupdated zu $Pr_{Post}$.\n\n-   Mit der Gitter-Methode haben wir viele Hypothesen (Parameterwerte) untersucht und jeweils die $Pr_{Post}$ berechnet.\n\n-   Unser Modell bildet die kleine Welt ab; ob es in der gro√üen Welt n√ºtzlich ist, steht auf einem anderen Blatt.\n\nüèãÔ∏è Wenn Sie auf einen Prozentwert f√ºr $W$ tippen m√ºssten, welchen w√ºrden Sie nehmen, laut dem Modell (und gegeben der Daten)?\n\n\n### Vertiefung\n\n\nDas [\"Bayes-Paradox-Video\" von 3b1b](https://youtu.be/lG4VkPoG3ko) pr√§sentiert eine gut verst√§ndliche Darstellung des Bayes-Theorem aus einer zwar nicht gleichen, aber √§hnlichen Darstellung wie in diesem Kapitel.\n\n\n\n\n### Literatur\n\n\n@bourier_2018, Kap. 6.2 und 7.1 erl√§utern einige (grundlegende) theoretische Hintergr√ºnde zu diskreten Zufallsvariablen und Wahrscheinlichkeitsverteilungen. Wichtigstes Exemplar ist dabei die Binomialverteilung.\n@mcelreath_statistical_2020, Kap. 2, stellt das Globusmodell mit mehr Erl√§uterung und etwas mehr theoretischem Hintergrund vor.\n\n\n\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}