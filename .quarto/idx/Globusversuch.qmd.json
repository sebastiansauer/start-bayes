{"title":"Globusversuch","markdown":{"headingText":"Globusversuch","containsRefs":false,"markdown":"\n\n\n\n\n![Bayes:Start](img/Golem_hex.png){width=5%}\n\n\n\n\n## Lernsteuerung\n\n\n### Lernziele\n\nNach Absolvieren des jeweiligen Kapitel sollen folgende Lernziele erreicht sein.\n\nSie k√∂nnen ...\n\n\n- Unterschiede zwischen Modellen und der Realit√§t erl√§utern\n- die Binomialverteilung heranziehen, um geeignete (einfache) Modelle zu erstellen\n- die weite Einsetzbarkeit anhand mehrerer Beispiele exemplifizieren\n- Post-Wahrscheinlichkeiten anhand der Gittermethode berechnen\n\n\n\n\n```{r}\n#| include: false\nlibrary(tidyverse)\nlibrary(patchwork)\nlibrary(easystats)\nlibrary(ggraph)\nlibrary(tidygraph)\ntheme_set(theme_modern())\n```\n\n\n## Von Welten und Golems\n\n### Kleine Welt, gro√üe Welt\n\nBekanntlich segelte Kolumbus 1492 los, und entdeckte Amerika^[wenn auch nicht als Erster]. Das war aber ein gl√ºcklicher Zufall, denn auf seinem Globus existierte Amerika gar nicht. Vielleicht sah sein Globus so aus wie der von Behaim, s. Abb @fig-behaim.\n\n![Behaims Globus: Kein Amerika](img/Behaim.jpg){#fig-behaim}\n\n[Quelle: Ernst Ravenstein, Wikimedia, Public Domain](https://commons.wikimedia.org/wiki/File:RavensteinBehaim.jpg)\n\nDie *kleine Welt des Modells* entsprach hier nicht *der gro√üen Welt, der echten Erdkugel*.\n\nDas ist ein Beispiel, das zeigt, wie Modellieren schiefgehen kann. Es ist aber auch ein Beispiel f√ºr, sagen wir, die Komplexit√§t wissenschaftlicher (und sonstiger) Erkenntnis. Einfach gesagt: Gl√ºck geh√∂rt halt auch dazu.\n\n\n::: callout-note\nBehaims Globus ist nicht gleich der Erde. Die kleine Welt ist nicht die gro√üe Welt.\n:::\n\nWas in der kleinen Welt funktioniert, muss nicht in der gro√üen Welt funktionieren. Modelle zeigen immer nur die kleine Welt: Vorsicht vor schnellen Schl√ºssen und vermeintlicher Gewissheit.\n\nüèã Nennen Sie ein Beispiel, in dem ein Modell nicht (exakt) der Wirklichkeit entspricht!\n\n\n\n\n\n\n### Der Golem von Prag\n\n![Der Golem von Prag](img/170px-Golem_and_Loew.jpg)\n\n[Quelle](https://de.wikipedia.org/wiki/Golem)\n\n[Der Golem von Prag](http://www.prague.net/golem), die Legende einer vom Menschen geschaffene Kreatur mit gewaltiger Kraft, die Befehle w√∂rtlich ausf√ºhrt.\nDie Geschichte besagt, dass ein Rabbi mit Zauberkr√§ften den Golem aus Lehm erschuf, um die j√ºdische Bev√∂lkerung der Stadt zu sch√§tzen.\nBei kluger F√ºhrung kann ein Golem N√ºtzliches vollbringen.\nBei un√ºberlegter Verwendung wird er jedoch gro√üen Schaden anrichten.\n\n### Wissenschaftliche Modelle sind wie Golems\n\n**Golem**\n\n![\"Yeah, ich bin ein Golem!\" - Bildquelle: Klara Schaumann](img/Golem_hex.png){width=25%}\n\n\nEigenschaften des Golems:\n\n-   Besteht aus Lehm\n-   Belebt durch \"Wahrheit\"\n-   M√§chtig\n-   dumm\n-   F√ºhrt Befehle w√∂rtlich aus\n-   Missbrauch leicht m√∂glich\n-   M√§rchen\n\n**Modell**\n\nEigenschaften des Modells:\n\n```{mermaid}\nflowchart LR\nX --> Y\n```\n\n-   Besteht aus ~~Lehm~~Silikon\n-   Belebt durch Wahrheit (?)\n-   Manchmal m√§chtig\n-   simpler als die Realit√§t\n-   F√ºhrt Befehle w√∂rtlich aus\n-   Missbrauch leicht m√∂glich\n-   Nicht einmal falsch\n\n::: callout-note\nWir bauen Golems.\n:::\n\n\nVergleichen wir die kleine Welt unserer Modellen, wie Behaims Globus, mit der Gro√üen Welt, die Kolumbus und wir befahren.\n\n\n\n\n| Kleine Welt                                                | Gro√üe Welt                                 |\n|-----------------------------------------|-------------------------------|\n| Die Welt, wie sie der Golem sieht                          | Die Welt, wie sie in Wirklichkeit ist      |\n| ist das Modell, aber nicht (zwangsl√§ufig) die Wirklichkeit | entspricht nicht (zwangsl√§ufig) dem Modell |\n| Verwenden wir beim Modellieren                             | Ist das, was wir modellieren               |\n\n: Kleine Welt vs. gro√üe Welt\n\n\n\n\n### So denkt unser Bayes-Golem\n\n![So denkt unser Bayes-Golem](img/bayesupdate2.png)\n\nüèã Bayes-Inferenz √§hnelt dem Lernen von Menschen. Geben Sie ein Beispiel von Lernen bei Menschen, das oben dargestelltem Prozess √§hnelt!\n\n## Ein erster Versuch: Wir werfen den Globus\n\n\n\n\n\n### Welcher Anteil der Erdoberfl√§che ist mit Wasser bedeckt?\n\nUnsere Forschungsfrage lautet, mit welchem Anteil die Erde wohl mit Wasser bedeckt ist?\n\n![Die Erde](img/earth.png){width=\"50%\"}\n\n[Quelle](https://pngimg.com/image/25340) CC 4.0 BY-NC\n\nAnalog k√∂nnen wir uns vorstellen, 11 Wissenschaftlis haben jeweils eine andere Hypothese zum Wasseranteil, $\\pi$, der Erde. Die erste Person hat die Hypothese $\\pi_1 = 0$, die zweite Person geht von $\\pi_2 = 0.1$ aus ... die 11. Person von $\\pi_{11} = 1$.\n \nUm die Forschungsfage zu beantworten, werfen Sie einen Globus-Ball in die Luft und fangen in wieder auf. \nSie notieren dann, ob die Stelle unter Ihrem Zeigefinger Wasser zeigt (W) oder Land (L). Den Versuch wiederholen Sie, bis Sie den Globusball insgesamt 9 Mal geworfen haben.\n\nSo sah *mein*^[*Ihr* Ergebnis kann anders aussehen, schlie√ülich ist es ja Zufall.] Ergebnis aus:\n\n$$W \\quad L \\quad W \\quad W \\quad W \\quad L \\quad W \\quad L \\quad W$$\n\nüèãÔ∏èÔ∏è Besorgen Sie sich einen Globus (zur Not eine M√ºnze) und stellen Sie den Versuch nach!\n\n### Wie entstanden die Daten?\n\nDer physikalische Prozess, der zur Entstehung der Daten f√ºhrt, nennt man den  *datengenierenden Prozess*.\n\nIn diesem Fall kann man ihn so beschreiben:\n\n1.  Der wahre Anteil von Wasser, $W$, der Erdoberfl√§che ist $p$ (und $1-p$ ist der Anteil Land, $L$).\n2.  Ein Wurf des Globusballes hat die Wahrscheinlichkeit $p$, eine $W$-Beobachtung zu erzeugen.\n3.  Die W√ºrfe des Globusballes sind unabh√§ngig voneinander.\n4.  Wir haben kein Vorwissen √ºber $p$; jeder Wert ist uns gleich wahrscheinlich.\n\nüèã Welche Annahmen w√ºrden Sie √§ndern? Welche k√∂nnte man wegnehmen? Welche hinzuf√ºgen? Was w√§ren die Konsequenzen?\n\n\n\n\n\n\n### Ein paar Fachbegriffe\n\n-   F√ºr jede Hypothese haben wir ein Vorab-Wissen, das die jeweilige Plausibilit√§t der Hypothese angibt: *Priori-Verteilung*.\n\n-   F√ºr jede Hypothese (d.h. jeden *Parameterwert* $p$) m√∂chten wir wie wahrscheinlich die Daten sind (unter der Annahme, dass die Hypothese richtig ist). Das gibt uns den *Likelihood*.\n\n-   Dann gewichten wir den Likelihood mit dem Vorabwissen, so dass wir die *Posteriori-Verteilung*^[ Anstatt von *Priori* liest man auch *Prior*; anstatt *Posteriori* auch *Posterior*] bekommen.\n\n\n### Bayes-Updates\n\n\nDer Golem denkt eigentlich ganz vern√ºnftig:\nZuerst hat er ein Vorwissen zum Wasseranteil, die dazugeh√∂rige Wahrscheinlichkeitsverteilung nennt man *Priori-Verteilung*.\nIn unserem Beispiel ist das Vorwissen recht bescheiden: Jeder Wasseranteil ist ihm gleich plausibel.\nAls n√§chstes beschaut sich der Golem die Daten und √ºberlegt,\nwie wahrscheinlich die Daten sind, wenn man von einer bestimmten Hypothese ausgeht, z.B. dass der Wasseranteil 10% betr√§gt.\nDie zugeh√∂rige Wahrscheinlichkeit der Daten unter Annahme einer Hypothese nennt man die^[oder den?] *Likelihood.*\nAls letztes bildet sich der Golem eine abschlie√üende Meinung zur Wahrscheinlichkeit jeder Hypothese. Diese Wahrscheinlichkeitsverteilung nennt man *Posteriori-Verteilung*.\nSie berechnet als Gewichtung des Vorwissen mit den neuen Daten.\nAnders gesagt: Das Vorwissen wird anhand der Erkenntnisse (der Daten) aktualisiert oder geupatedet, s. @fig-bayes-update.\n\n\n![Updating mit Bayes](img/bayesupdate.png){#fig-bayes-update}\n\n### Likelihood mit Binomialverteilung\n\nWie wahrscheinlich ist es, einen bestimmten Wasseranteil, z.B. 6 Treffer, (bei 9) W√ºrfen zu bekommen, wenn man eine bestimmte Hypothese (einen bestimmten Wasseranteil, z.B. 70%) annimmt?\nDiese Wahrscheinlichkeit hat einen eigenen Namen, weil sie eine wichtige Sache ist.\nMan mennt sie die *Likelihood*, $L$^[$\\mathfrak{L}$ f√ºr Freunde alter Schriftarten].\n\nGeht man von einer Binomialverteilng aus, ist die Likelihood einfach zu berechnen^[Ein Gl√ºck!].\n\nWenn wir eine Binomialverteilung annehmen, dann gehen wir davon aus,  dass die Daten unabh√§ngig voneinander entstehen und sich der Parameterwert nicht zwischenzeitlich √§ndert^[Die sog. \"iid-Annahme\", *i*ndependently and *i*dentically distributed: Jeder Wurf der Globusballes ist eine Realisation der gleichen Zufallsvariablen. Jeder Wurf ist unabh√§ngig von allen anderen: Das Ergebnis eines Wurfes hat keinen (stochastischen) Einfluss auf ein Ergebnis anderer W√ºrfe. Die Wahrscheinlichkeitsverteilung ist bei jedem Wurf identisch.].\nDer Wasseranteil der Erde bleibt w√§hrend des Versuchs gleich (durchaus plausibel).\n\nLassen Sie uns im Folgenden die Wahrscheinlichkeit ($Pr$), $W$ mal Wasser und $L$ mal Land zu beobachten, wenn die Wahrscheinlichkeit f√ºr Wasser $p$ betr√§gt, so bezeichnen: $(Pr(W,L | p))$.\nDiese Wahrscheinlichkeit, $(Pr(W,L | p))$, kann man mit der *Binomialverteilung* berechnen.\n\nM√∂chte man die Wahrscheinlichkeit ansprechen f√ºr das Ereignis \"5 mal Wasser und 2 mal Land, wenn wir von einem Wasseranteil von 70% ausgehen\", so w√ºrden wir kurz schreiben: $Pr(W=5, L=2 | p=.7)$.\n\nDie Binomialverteilung zeigt die Verteilung der H√§ufigkeit (Wahrscheinlichkeit) der Ereignisse (z.B. 2 Mal Kopf) beim wiederholten M√ºnzwurf (und allen vergleichbaren Zufallsexperimenten): \"M√ºnzwurfverteilung\", s. Kap. @sec-bin-distrib.\n\nDie Formel der Binomialverteilung sieht so aus:\n\n$$Pr(W,L|p) = \\frac{(W+L)!}{W!L!}p^W(1-p)^L = k \\cdot P(A)$${#eq-binomial}\n\n\nFormel @eq-binomial kann wie folgt auf Deutsch √ºbersetzen:\n\n>   Die Wahrscheinlichkeit f√ºr das Ereignis \"W,L\" gegeben p berechnet als Produkt von zwei Termen. Erstens der Quotient von der Fakult√§t von W plus L im Z√§hler und im Nenner das Produkt von erstens der Fakult√§t von W mit zweitens der Fakult√§t von L. Der zweite Term ist das Produkt von p hoch W mal der komplement√§ren Wahrscheinlichkeit von p hoch L.\n\nOder noch k√ºrzer:\n\n>    Die Wahrscheinlichkeit f√ºr das Ereignis \"W,L\" gegeben p berechnet als Produkt von zwei Termen. Erstens der Anzahl der g√ºnstigen Pfade, k und zweitens der Wahrscheinlichkeit f√ºr einen g√ºnstigen Pfad, P(A).\n\n\nPuh, Formeln sind vielleicht doch ganz praktisch, wenn man sich diese lange √úbersetzung der Formel in Prosa duchliest.\nNoch praktischer ist es aber, dass es Rechenmaschinen gibt, die die Formel kennen und f√ºr uns ausrechnen. \nLos, R, mach mal.\n\n### Binomialverteilung mit R\n\n\nWas ist der Anteil der g√ºltigen Pfade in einem Baumdiagramm (Wahrscheinlichkeit), um 2 mal $W$ bei $N=W+L=3$ W√ºrfen zu bekommen, wenn wir von $p=1/2$ ausgehen?^[Allgemeiner spricht man auch von 2 Treffern bei 3 W√ºrfen (d.h. 1 \"Nicht-Treffer\", den wir als \"Niete\" bezeichnen). Treffer werden oft mit `1`  und Nieten mit `0` bezeichnet].\n\n\n```{r QM2-Thema2-kleineModelle-21a, echo = TRUE}\ndbinom(x = 2, size = 3, prob = 1/2)\n```\n\n\nVon den 8 Endkonten bzw. Pfaden sind 3 g√ºnstig. \nDemnach ist die Wahrscheinlichkeit des gesuchten Ereignis (2 Treffer bei 3 W√ºrfen, binomialverteilt) gleich 3 von 8 (alle Pfade sind gleich wahrscheinlich);\n3/8 sind 0.375.\n\n\n\n\n\n\n```{mermaid}\n%%| echo: false\n%%| label: fig-binom1\n%%| fig-cap: Wir werfen den Globus (oder eine M√ºnze) 3 Mal\nflowchart TD\n  A[A - Start] -. 1/2 .-> B[B - 0]\n  A -. 1/2 .-> C[C - 1]\n  B -. 1/2 .-> D[D - 0]\n  B -. 1/2 .-> E[E - 1]\n  C -. 1/2 .-> F[F - 0]\n  C -. 1/2 .-> G[G - 1]\n  D -. 1/2 .-> H[H - 0]\n  D -. 1/2 .-> J[I - 1]\n  E -. 1/2 .-> K[K - 0]\n  E -. 1/2 .-> L[L - 1]\n  F -. 1/2 .-> M[M - 0]\n  F -. 1/2 .-> N[N - 1]\n  G -. 1/2 .-> O[O - 0]\n  G -. 1/2 .-> P[P - 1]\n```\n\n\nAbb. @fig-binom1 stellt einen einfachen Baum f√ºr 3 Globusw√ºrfe mit je zwei m√∂glichen Ereignissen (W vs. L) dar.\nIn der ersten (obersten) Zeile (Knoten A; \"Start\") ist Ausgangspunkt dargestellt: Der Globus ruht wurfbereit in unserer Hand.\nJetzt Achtung: Sie werfen den Globusball hoch.\nDie Pfeile zeigen zu den (zwei) m√∂gliche Ergebnissen.\nDie zweite Zeile (Knoten B und C) stellt die beiden Ergebnisse des Wurfes dar. \nDie Ergebnisse sind hier mit `0` und `1` bezeichnet (das eine eine einfache und weiteinsetzbare Notation).\nDie dritte Zeile (Knoten D bis G) stellt die Ergebnisse des des zweiten Wurfes dar.\nDie vierte Zeile (Knoten H bis P)  stellt die Ergebnisse des des dritten Wurfes dar.\n\n\nF√ºr mehr W√ºrfe w√ºrde das Diagramm irgendwann un√ºbersichtlich werden.\n\n\n\nWas ist der Anteil der g√ºltigen Pfade in einem Baumdiagramm (Wahrscheinlichkeit), um 6 mal $W$ bei $N=W+L=9$ W√ºrfen zu bekommen, wenn wir von $p=1/2$ ausgehen?\n\n```{r QM2-Thema2-kleineModelle-21, echo = TRUE}\ndbinom(x = 6, size = 9, prob = 1/2)\n```\n\n\nAbb @fig-binom2 ist ein vergeblicher Versuch, so einen gro√üen Baum ($n=9$) darzustellen.\n\n:::callout-note\nVisualisierungen wie Baumdiagramme sind eine praktische Hilfe zum Verst√§ndnis,\nkommen aber bei gr√∂√üeren Daten schnell an ihre Grenze.\n:::\n\n\n```{r}\n#| echo: false\n#| label: fig-binom2\n#| fig-cap: Wir werfen den Globus (oder eine M√ºnze) 3 Mal\nmy_tree <- tidygraph::create_tree(511, 2, mode = \"out\")\n\nmy_tree %>%\n  mutate(lab = 1:511) %>% \n  ggraph(circular = TRUE) +\n  geom_edge_link() +\n  geom_node_label(mapping = aes(label = lab)) +\n  coord_flip() +\n  scale_y_reverse() +\n  theme_void()\n\n```\n\n\nJetzt folgen einige Beispiele.\n\n\n::: {#exm-globus2}\n\n## Globus mit 9 Treffern bei 9 W√ºrfen\n\nWas ist die Wahrscheinlichkeit f√ºr $W=9$ bei $N=9$ und $p=1/2$?\n\n```{r QM2-Thema2-kleineModelle-22, echo = TRUE}\ndbinom(x = 9, size = 9, prob = 1/2)\n```\n\nDas ist 1 g√ºnstiger Pfad von 512 Pfaden.\n\n:::\n\n\n\n::: {#exm-globus3}\n\n## Klausur mit 20-Richtig-Falsch-Fragen\n\nEi Professi stellt einen Klausur mit 20 Richtig-Falsch-Fragen. Wie gro√ü ist die Wahrscheinlichkeit, durch blo√ües M√ºnze werfen genau 15 Fragen richtig zu raten?^[Hey, endlich mal was f√ºr echte Leben!].\n\n```{r QM2-Thema2-kleineModelle-23, echo = TRUE}\ndbinom(x = 15, size = 20, prob = .5)\n```\n\nUm *h√∂chstens* 15 Treffer zu erzielen, m√ºssten wir die Wahrscheinlichkeiten von 0 bis 15 Treffern addieren.\n\nPraktischerweise gibt es einen R-Befehl, der das f√ºr uns √ºbernimmt:\n\n```{r}\npbinom(q = 15, size = 20, prob = .5)\n```\n\n\nDie Wahrscheinlichkeit 0, 1, 2, ... oder 15 Treffer zu erzielen, liegt also bei gut 99%.\n\n\n:::\n\n\n\n::: {#exm-globus4}\n\n## 3 M√ºnzw√ºrfe mit 3 Treffern\n\nWas ist die Wahrscheinlichkeit bei 3 M√ºnzw√ºrfen (genau) 3 Treffer (Kopf) zu erzielen?\n\nDas ist eine Frage an die Binomialverteilung;\nin R kann man das mit der Funktion `dbinom` beantworten.\n\n```{r QM2-Thema2-kleineModelle-24, echo = TRUE}\ndbinom(x = 3, size = 3, prob = 1/2)\n```\n\n:::\n\n\n\n`dbinom` gibt uns die Wahrscheinlichkeit von `x` Treffern, bei `size` Versuchen zur√ºck, wobei eine Binomialverteilung angenommen wird mit Trefferwahrscheinlichkeit `prob`.\n\n\n\n### Unser Modell ist geboren\n\nWir fassen das Globusmodell so zusammen:\n\n$$W \\sim \\text{Bin}(N,p),$$\n\nLies: \"W ist *bin*omial verteilt mit den Parametern $N$ und $p$\". $N$ gibt die Anzahl der Globusw√ºrfe an: $N=W+L$.\n\nUnser Vorab-Wissen zu $p$ sei, dass uns alle Werte gleich plausibel erscheinen (\"uniform\"):\n\n$$p \\sim \\text{Unif}(0,1).$$\n\nLies: \"$p$ ist gleich (uniform) verteilt mit der Untergrenze 0 und der Obergrenze 1\".\n\nMan k√∂nnte auch sagen: Wir haben praktisch kein Vorwissen, wir sind erstmal (aprior) indifferent,\njeder Parameterwert erscheint uns erstmal gleich wahrscheinlich.\n\n\n\n\n\n\n\n\n\n\n### Visualisierungen\n\nAbb. @fig-bin zeigt die Binomialverteilung $X \\sim Bin(9, 1/2)$.\n\n```{r QM2-Thema2-kleineModelle-25}\n#| echo: false\n#| label: fig-bin\n#| fig-cap: Ein Beispiel f√ºr eine Binomialverteilung mit Parametern N=9 und p=1/2.\ndbinom(0:9, 9, 1/2) %>% \n  tibble(Wahrscheinlichkeit = .,\n         Treffer = seq(0,9)) %>% \n  ggplot(aes(x = Treffer, y = Wahrscheinlichkeit)) +\n  geom_segment(aes(xend = Treffer, yend = 0)) + \n  geom_point(color = \"red\", size = 5, alpha = .5) +\n  scale_x_continuous(breaks = 0:9)\n\n```\n\n\n\n\n\n```{r QM2-Thema2-kleineModelle-26}\n#| echo: false\n#| fig-cap: Gleichverteilung mit Parametern min=0 und max=1\n#| label: fig-unif\n#| eval: false\n\n\n\nuniform_Plot <- function(a, b){\n  xvals <- data.frame(x = c(a, b)) #Range for x-values\n  \n  ggplot(data.frame(x = xvals), \n         aes(x = x)) + xlim(c(a, b)) + ylim(0, 1/(b - a)) +\n    stat_function(fun = dunif, args = list(min = a, max = b), \n                  geom = \"area\", \n                  fill = \"green\", alpha = 0.35) + \n    stat_function(fun = dunif, args = list(min = a, max = b)) +\n    labs(x = \"X\", y = \"Dichte\")  +\n    geom_vline(xintercept = a, linetype = \"dashed\", colour = \"red\") +\n    geom_vline(xintercept = b, linetype = \"dashed\", colour = \"red\") \n  \n}\nuniform_Plot(0, 1)\n```\n\n\n\nüèãÔ∏èÔ∏è Was f√§llt Ihnen bei der Binomialverteilung auf? Ist sie symmetrisch? Ver√§ndert sich die Wahrscheinlichkeit linear?\n\n## Zur Erinnerung: Bayes Theorem\n\n### Herleitung Bayes' Theorem 1/2: Gemeinsame Wahrscheinlichkeit\n\nDie Wahrscheinlichkeit f√ºr *Regen* und *kalt* ist gleich der Wahrscheinlichkeit von *Regen*, *gegeben kalt* mal der Wahrscheinlichkeit von *kalt*. Entsprechend gilt: Die Wahrscheinlichkeit von $W$, $L$ und $p$ ist das Produkt von $Pr(W,L|p)$ und der Prior-Wahrscheinlichkeit $Pr(p)$:\n\n$$Pr(W,L,p) = Pr(W,L|p) \\cdot Pr(p)$$\n\nGenauso gilt: Die Wahrscheinlichkeit von *Regen* und *kalt* ist gleich der Wahrscheinlichkeit *kalt, wenn's regnet* mal der Wahrscheinlichkeit von *Regen*:\n\n$$Pr(W,L,p) = Pr(p|W,L) \\cdot Pr(W, L)$$\n\n### Herleitung Bayes' Theorem 2/2: Posteriori-Wahrscheinlichkeit\n\nWir setzen die letzten beiden Gleichungen gleich:\n\n$$Pr(W,L|p) \\cdot Pr(p) = Pr(p|W,L) \\cdot (W,L)$$\n\nUnd l√∂sen auf nach der Posteriori-Wahrscheinlichkeit^[k√ºrzen wir mit Post-Wahrscheinlichkeit or $Pr(Post)$ ab], $Pr(p|W,L)$:\n\n$$Pr(p|W,L) = \\frac{Pr(W,L|p) Pr(p)}{Pr(W,L)}$$\n\n$Pr(W,L)$ nennt man die *mittlere Wahrscheinlichkeit der Daten* oder *Evidenz*. Die Evidenz berechnet sich als Mittelwert der Likelihoods √ºber alle Werte von $p$. Die Aufgabe dieser Gr√∂√üe ist nur daf√ºr zu sorgen, dass insgesamt Werte zwischen 0 und 1 herauskommen.\n\n### Bayes' Theorem als Formel\n\n$$Pr(H|D) = \\frac{Pr(D|H) Pr(H)}{Pr(D)} = \\frac{\\text{Likelihood}  \\cdot \\text{Priori}}{\\text{Evidenz}}$$\n\n-   Bestandteile:\n\n    -   Posteriori-Wahrscheinlichkeit: $Pr_{Post} := Pr(H|D)$\n\n    -   Likelihood: $L := Pr(D|H)$\n\n    -   Priori-Wahrscheinlichkeit: $Pr_{Priori} := Pr(H)$\n\n    -   Evidenz: $E := Pr(D)$\n\n-   Bayes' Theorem gibt die $Pr_{Post}$ an, wenn man die Gleichung mit der $Pr_{Priori}$ und dem $L$ f√ºttert.\n\n-   Bayes' Theorem wird h√§ufig verwendet, um die $Pr_{Post}$ zu quantifizieren.\n\n-   Die $Pr_{Post}$ ist proportional zu $L \\times Pr_{Priori}$.\n\n### Posteriori als Produkt von Priori und Likelihood\n\nDie unstandardisierte Post-Wahrscheinlichkeit ist einfach das Produkt von Likelihood und Priori.\n\nDas Standardisieren dient nur dazu, einen Wert zwischen 0 und 1 zu erhalten. \nDies erreichen wir, indem wir durch die Summe aller Post-Wahrscheinlichkeiten dividieren.\nDie Summe der Post-Wahrscheinlichkeiten bezeichnet man (auch) als Evidenz, vgl. Gleichung @eq-post.\n\n\n$$\\text{Posteriori} = \\frac{\\text{Likelihood} \\times \\text{Priori}}{\\text{Evidenz}}$${#eq-post}\n\n\n\nAbb. @fig-post3 visualisiert, dass die Post-Verteilung eine Gewichtung von Priori und Likelihood ist.\nMathematisch gesprochen beruht diese Gewichtung auf einer einfachen Multiplikationen der beiden genannten Terme.\n\n\n\n![Prior mal Likelihood = Post](img/img241.png){#fig-post3}\n\n\n\n\n\n\n\n### Wissen updaten: Wir f√ºttern Daten in das Modell\n\nGolems k√∂nnen lernen?! @fig-lernen-golem zeigt die Post-Verteilung, nach $n=1, 2, ...,n=9$ Datenpunkten, d.h. W√ºrfen mit dem Globusball.\nMan sieht: Am Anfang, apriori, also bevor die Daten haben, vor dem ersten Wurf also, ist jeder Parameterwert gleich wahrscheinlich f√ºr den Golem (das Modell).\nJe nach Ergebnis des Wurfes ver√§ndert sich die Wahrscheinlichkeit der Parameterwerte,\nkurz gesagt, die Post-Verteilung ver√§ndert sich in Abh√§ngigkeit von den Daten.\n\n\n![Unser Golem lernt](img/img221.png){#fig-lernen-golem}\n\n\nInsofern kann man sagen: Unser Golem (das Modell) lernt. Ob das Modell n√ºtzlich ist (pr√§zise Vorhersagen liefert), steht auf einem anderen Blatt.\n\n\n\n## Bayes berechnen mit mit dem Bayes-Gitter\n\nWir erstellen uns eine kleine Tabelle, die man \"Bayes-Gitter\" nennen k√∂nnte.\nDazu gehen wir so vor:\n\n### Idee\n\n1.  Teile den Wertebereich des Parameter in ein \"Gitter\" auf, z.B. $0.1, 0.2, ..., 0.9, 1$ (\"Gitterwerte\").\n2.  W√§hle den Priori-Wert des Parameters f√ºr jeden Gitterwert.\n3.  Berechne den Likelihood f√ºr Gitterwert.\n4.  Berechne den unstandardisierten Posteriori-Wert f√ºr jeden Gitterwert (Produkt von Priori und Likelihood).\n5.  Standardisiere den Posteriori-Wert durch teilen anhand der Summe alle unstand. Posteriori-Werte.\n\n\nF√ºr jeden \"Gitterwert\" berechnen wir eine (Post-)Wahrscheinlichkeit. Ein Gitterwert ist eine m√∂gliche Auspr√§gung des Parameters. \nH√§ufig entspricht eine Hypothese einem Gitterwert, \netwa wenn man sagt: \"Ich glaube, die M√ºnze ist fair\", was auf einem Parameterwert von 50% herausl√§uft.\nDazu geben wir an, f√ºr wie wahrscheinlich wie apriori^[synonym: priori] - also bevor wir irgendwelche Daten erheben - jeden einzelnen Gitterwert halten.\nWir machen es uns hier einfach und halten jeden Gitterwert f√ºr gleich wahrscheinlich. \nTats√§chlich ist der konkrete Wert hier egal, entscheidend ist das Verh√§ltnis der Apriori-Werte zueinander: \nGeben wir einigen Gitterwerten den Wert 2, aber anderen den Wert 1, \nso halten wir Erstere f√ºr (apriori) doppelt so plauibel wie Letztere.\nDer Likelihood wird in diesem Fall mit der Binomialverteilung berechnet. Der Likelihood gibt an, \nwie wahrscheinlich ein Gitterwert ist gegeben einem bestimmten apriori gew√§hlten Parameterwert.\nDie \"End-Wahrscheinlichkeit\", die unstandardisierte Post-Wahrscheinlichkeit, die \"hinten rauskommt\" ist das Produkt von Priori-Wert und Likelihood.\nAnschaulich gesprochen: Die Priori-Werte werden mit den Likelihoodwerten gewichtet^[synonym: Die Likelihoodwerte werden mit den Apriori-Werten gewichtet.].\nDa wir letztlich eine Wahrscheinlichkeitverteilung bekommen m√∂chten teilen wir jeden Posteriori-Wert durch die Summe aller Posteriori-Werte. \nDadurch ist gerantiert, dass sich die Posteriori-Werte zu eins aufaddieren. \nDamit haben wir dann die Kolmogorov-Anspr√ºche an eine Wahrscheinlichkeitsverteilung erf√ºllt.\n\n\n### Bayes-Gitter in R berechnen\n\nLegen wir uns eine Tabelle mit Gitterwerten an, um deren Posteriori-Wahrscheinlichkeit zu berechnen.\n\n```{r QM2-Thema2-kleineModelle-28, echo = TRUE}\nd <-\n  tibble(\n    # definiere die Hypothesen (das \"Gitter\"): \n    p_Gitter = seq(from = 0, to = 1, by = 0.1),\n    # bestimme den Priori-Wert:       \n    Priori  = 1) %>%  \n    mutate(\n      # berechne Likelihood f√ºr jeden Gitterwert:\n      Likelihood = dbinom(6, size = 9, prob = p_Gitter),\n      # berechen unstand. Posteriori-Werte:\n      unstd_Post = Likelihood * Priori,\n      # berechne stand. Posteriori-Werte (summiert zu 1):\n      Post = unstd_Post / sum(unstd_Post))  \n```\n\nDas \"Bayes-Gitter\" (@tbl-globus) zeigt, wie sich die Post-Verteilung berechnet.\n\n```{r QM2-Thema2-kleineModelle-29}\n#| label: tbl-globus\n#| tbl-cap: \"Die Bayes-Box f√ºr den Globusversuch\"\n#| echo: false\nd %>% \n  mutate(id = 1:11) %>% \n  relocate(id, .before = 1) %>% \n  knitr::kable(digits = 2)\n```\n\n\nF√ºr jede Hypothese (Spalte `id`) berechnen wir die unstandardisierte Posteriori-Wahrscheinlichkeit als Produkt von Priori und Likelihood:\n\n$\\text{Post}_{\\text{unstand}} = \\text{Priori} \\cdot \\text{Likelihood}$\n\nUm zur standardisierten Posteriori-Wahrscheinlichkeit zu gelangten,\nteilen wir in jeder Zeile der Gitterbox (also f√ºr jede Hypothese) die unstandardisierte Post-Wahrscheinlichkeit durch die Summe der unstandardisierten Post-Wahrscheinlichkeiten.\n\n\nüèãÔ∏è Was wohl mit *Post* passiert, wenn wir *Priori* √§ndern?\n\n### Was sagt die Post?\n\nDie Posteriori-Verteilung (Kurz: \"Post-Verteilung\"), $Pr_{Post}$, zeigt, wie plausibel wir jeden Wert von $p$ halten.\n\n\nAbb. @fig-gitter zeigt die Post-Wahrscheinlichkeit f√ºr 5, 10 und 20 Gitterwerte. Das mittlere Teilbild (10 Gitterwerte) entspricht unserer Tabelle oben.\n\n\n![Je mehr Gitterwerte, desto genauer wird die Verteilung wiedergegeben.](img/img242.png){#fig-gitter}\n\n\n:::callout-note\nUnter sonst gleichen Umst√§nden gilt:\n\n- Mehr Gitterwerte gl√§tten die Ann√§herung.\n- Je gr√∂√üer die Stichprobe ($N$), desto zuverl√§ssiger wird unsere Berechnung.\n:::\n\n::: callout-important\nDie Post-Verteilung ist sowas wie das Ziel all Ihrer Tr√§ume (falls Sie es noch nicht gewusst haben):\nAus der Post-Verteilung k√∂nnen Sie ablesen,\nwie wahrscheinlich Ihre Hypothese (Ihr Lieblings-Parameterwert) ist. Und noch einiges mehr, aber das ist Thema des n√§chsten Kapitels.\n:::\n\n\n\n## Aufgaben\n\n- [Rethink_2E4](https://datenwerk.netlify.app/posts/rethink_2e4/rethink_2e4)\n- [Rethink_2m1](https://datenwerk.netlify.app/posts/rethink_2m1/rethink_2m1)\n- [Rethink_2m2](https://datenwerk.netlify.app/posts/rethink_2m2/rethink_2m2)\n- [Rethink_2m3](https://datenwerk.netlify.app/posts/rethink_2m3/rethink_2m3)\n- [Rethink_2m4](https://datenwerk.netlify.app/posts/rethink_2m4/rethink_2m4)\n- [Rethink_2m5](https://datenwerk.netlify.app/posts/rethink_2m5/rethink_2m5)\n- [Rethink_2m6](https://datenwerk.netlify.app/posts/rethink_2m6/rethink_2m6)\n- [Rethink_2m7](https://datenwerk.netlify.app/posts/rethink_2m7/rethink_2m7)\n\n\n\n\n\n\n\n## Abschluss\n\n### Zusammenfassung\n\n-   In unserem Modell haben wir Annahmen zu $Pr_{Priori}$ und $L$ getroffen.\n\n-   Auf dieser Basis hat der Golem sein Wissen geupdated zu $Pr_{Post}$.\n\n-   Mit der Gitter-Methode haben wir viele Hypothesen (Parameterwerte) untersucht und jeweils die $Pr_{Post}$ berechnet.\n\n-   Unser Modell bildet die kleine Welt ab; ob es in der gro√üen Welt n√ºtzlich ist, steht auf einem anderen Blatt.\n\nüèãÔ∏è Wenn Sie auf einen Prozentwert f√ºr $W$ tippen m√ºssten, welchen w√ºrden Sie nehmen, laut dem Modell (und gegeben der Daten)?\n\n\n### Vertiefung\n\n\nDas [\"Bayes-Paradox-Video\" von 3b1b](https://youtu.be/lG4VkPoG3ko) pr√§sentiert eine gut verst√§ndliche Darstellung des Bayes-Theorem aus einer zwar nicht gleichen, \naber √§hnlichen Darstellung wie in diesem Kapitel.\n\n\n\n\n### Literatur\n\n\n@bourier_2018, Kap. 6.2 und 7.1 erl√§utern einige (grundlegende) theoretische Hintergr√ºnde zu diskreten Zufallsvariablen und Wahrscheinlichkeitsverteilungen. Wichtigstes Exemplar ist dabei die Binomialverteilung.\n@mcelreath_statistical_2020, Kap. 2, stellt das Globusmodell mit mehr Erl√§uterung und etwas mehr theoretischem Hintergrund vor.\n\n\n\n\n\n\n"},"formats":{"html":{"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"kable","error":false,"eval":true,"cache":true,"freeze":false,"echo":true,"output":true,"warning":false,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"knitr"},"render":{"keep-tex":false,"keep-yaml":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":true,"code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","toc":true,"number-sections":true,"output-file":"Globusversuch.html"},"language":{},"metadata":{"lang":"de","fig-responsive":true,"quarto-version":"1.1.168","bibliography":["references.bib"],"knitr":{"opts_chunk":{"collapse":true,"R.options":{"knitr.graphics.auto_pdf":true}}},"theme":"cosmo"},"extensions":{"book":{"multiFile":true}}}}}