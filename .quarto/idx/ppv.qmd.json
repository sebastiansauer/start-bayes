{"title":"Vorhersage-Verteilung","markdown":{"headingText":"Vorhersage-Verteilung","containsRefs":false,"markdown":"\n\n\n\n![Bayes:Start!](img/Golem_hex.png){width=5%}\n\n```{r}\n#| include: false\nlibrary(tidyverse)\nlibrary(gt)\nlibrary(patchwork)\nlibrary(easystats)\n```\n\n\n\n\n## Lernsteuerung\n\n\n### Lernziele\n\nNach Absolvieren des jeweiligen Kapitels sollen folgende Lernziele erreicht sein.\n\nSie können ...\n\n- erläutern, was eine Posteriori-Prädiktiv-Verteilung (PPV) ist, und inwiefern Sie vor Übergewissheit schützt\n- eine informelle Modellprüfung für das Beispiel aus dem Unterricht anhand der Posteriori-Prädiktiv-Verteilung durchführen\n\n\n\n### Benötigte R-Pakete\n\n\n\n\n\n```{r}\nlibrary(tidyverse)\n```\n\n\n\n\n\n```{r QM2-Thema2-kleineModelle-28}\n#| echo: false\nn <- 10\nn_success <- 6\nn_trials  <- 9\n\nd <-\n  tibble(p_grid = seq(from = 0, to = 1, length.out = n),\n         prior  = 1) %>% \n  mutate(likelihood = dbinom(n_success, \n                             size = n_trials, \n                             prob = p_grid)) %>% \n  mutate(unstand_post = (likelihood * prior),\n         post = unstand_post / sum(unstand_post))\n\nsamples <-\n  d %>%  # nimmt die Tabelle mit Posteriori-Daten,\n  slice_sample(  # Ziehe daraus eine Stichprobe,\n    n = 1e4,  # mit insgesamt n=10000 Elementen,\n    weight_by = post,  # Gewichte nach Spalte mit Post-Wskt.,\n    replace = T)  # Ziehe mit Zurücklegen\n\n```\n\n\n\n\n## Der zwielichte Dozent: Stichproben-Vert. vs. Post-Vert.\n\nIn einer dunklen Gasse fordert Sie ein Statistik-Dozent zu einem Glücksspiel heraus^[Hier bräuchte es ein passendes Meme; Vorschläge bitte an mich.]. Münzwurf; wenn er gewinnt, müssen Sie 10 Euro zahlen. Gewinnen Sie, bekommen Sie 11 Euro. Klingt nach einer guten Partie, nicht war? Natürlich nehmen Sie sofort an. \n\nSie spielen also Münzwurf; der Dozent setzt auf Zahl. Sie spielen 10 Runden. Leider gewinnt der Dozent 9 von 10 Mal^[was er mit lautem Gelächter quittiert].\n\n*Ist die Münze fair oder zieht der mich über den Tisch?*, das ist die Frage, \ndie Ihnen brennend durch den Kopf zieht.\n\n\"Sind 9 von 10 Treffern noch realistisch erwartbar, wenn es mit rechten Dingen zugeht,\noder beweist das Ergebnis, dass die Münze gezinkt ist?\"\n\nWütend (und mit leeren Taschen) ziehen Sie von dannen.\n\n\nZusammengefasst: Daten: 9 von 10 Treffern beim Münzwurf. Forschungsfrage: Ist die Münze fair?\n\n\nSchauen wir uns zunächst einmal an, wie wahrscheinlich 9 von 10 Treffern sind,\n*wenn* die Münze fair ist, s. @fig-stiprovert.\n\n\n```{r QM2-Thema3-Post-befragen-21}\n#| echo: false\n#| fig-cap: \"Stichprobenverteilung einer fairen Münze\"\n#| label: fig-stiprovert\n\ntibble(\n  Trefferzahl = rbinom(n = 1e4, size = 10, prob = 1/2)\n) %>% \n  mutate(signifikant = ifelse(Trefferzahl %in% c(9,10), TRUE, FALSE)) %>% \n  ggplot() +\n  aes(x = Trefferzahl, fill = signifikant) +\n  geom_bar() +\n  scale_x_continuous(breaks = 0:10) +\n  theme(legend.position = c(0.1, 0.8)) +\n  geom_vline(xintercept = 9) +\n  labs(title = \"Stichprobenverteilung für p=0.5\")\n```\n\nDie *Stichprobenverteilung* zeigt, wie wahrscheinlich die empirischen Daten $D$ (z.B. 9 von 10 Treffer) sind, *gegeben* eines Parameterwerts $\\pi$ (z.B. $p=0.5$): $Pr(D|\\pi)$^[Das griechische kleine p wird \"pi\" genannt und $\\pi$ geschrieben. Zur Erinnerung: Parameter- oder Populationskennwerte werden in der Statistik häufig mit griechischen Buchstaben benannt, um sie von Stichprobenkennwerten abzugrenzen.].\n\nAnders gesagt, die Stichprobenverteilung zeigt die Verteilung der Likelihoods eines bestimmten Parameterwerts.\n\n:::callout-note\n*Der p-Wert*\n\nDer p-Wert ist die zentrale Statistik der Inferenzstatistik.\nEr wird genutzt, um über die Ablehnung einer Hypothese zu entscheiden.\nIn diesem Fall entspricht der p-Wert dem türkis markierten Flächenanteil in @fig-stiprovert.\nIst dieser Anteil kleiner als 5% (der Gesamtfläche im Balkendiagramm),\nso wird die Hypothese (hier: faire Münze) verworfen.\nAllgemeiner gesprochne berechnet sich der p-Wert als Summe der Likelihoods, \ndie mindestens so extrem sind wie das beobachtete empirische Ergebnis.\n:::\n\n\nIn der Bayes-Statistik ist die Post-Verteilung Dreh- und Angelpunkt der Entscheidung über eine Hypothese.\nIn @fig-post-zwielicht ist die Posteriori-Verteilung für die Daten zum zwielichten Dozent dargestellt.\n\n\n\n```{r zwielicht-daten}\n#| echo: true\n\n# Post-Verteilung:\nd_zwielicht <-\n  tibble(\n    p_grid = seq( from=0 , to=1 , length.out=100),\n    prior = 1,  # Priori-Gewichte\n    likelihood = dbinom(8, size = 10, prob=p_grid) ,\n    unstandardisierte_posterior = likelihood * prior ,\n    posterior = unstandardisierte_posterior / sum(unstandardisierte_posterior))\n\n# Stichproben ziehen aus der Posteriori-Verteilung:\nsamples_zwielicht <- \n  tibble(\n    gewinnchance_muenze = sample(\n      d_zwielicht$p_grid , \n      prob=d_zwielicht$posterior, \n      size=1e4, \n      replace=TRUE)) %>% \n  mutate(\n    id = row_number())\n```\n\n```{r QM2-Thema3-Post-befragen-22}\n#| echo: false\n#| fig-cap: Post-Verteilung zu den Daten des zwielichten Dozenten (9 von 10 Treffern im wiederholten Münzwurf)\n#| label: fig-post-zwielicht\nsamples_zwielicht %>% \n  ggplot() +\n  aes(x = gewinnchance_muenze) +\n  geom_histogram(fill = \"grey60\", bins = 20) +\n  #geom_vline(xintercept = 0.9) +\n  #geom_label(x = 0.8, y= 0, label = \"Emp. Ergebnis\") +\n  labs(title = \"Posteriori-Verteilung\",\n       subtitle = \"Priori: Gleichverteilung; Daten: 9 von 10 Treffern, binomialverteilt\",\n       caption = \"Das Dreieck zeigt die Wskt. eines Treffers bei einer fairen Münze\",\n       x = \"Gewinnchance der Münze\") +\n  annotate(\"point\", x = .5, y = 0, size = 5, color = \"grey40\", shape = 17)\n```\n\nDie *Posteriori-Verteilung* gibt die Wahrscheinlichkeit jedes Parameterwerts $p$ wider, gegeben der empirischen Daten $D$: $Pr(p|D)$. \n\n\nDie meisten Forschungsfragen lassen sich mit der Post-Verteilung beantworten, nicht mit der Stichprobenverteilung.\n\n\nJetzt können wir wieder die Post-Verteilung auslesen,\num die Hypothese zu beantworten.\nSchauen wir uns einige Beispiel dazu an.\n\n\n:::{#exm-zwielicht1}\n\n### Einigermaßen fair?\n\nWie wahrscheinlich ist es, dass die Münze \"einigermaßen\" fair ist,\nsagen wir, eine Trefferwahrscheinlichkeit $0.45 < \\pi < 0.55$^[zwischen 45% und 55% mit anderen Worten] aufweist?\n\n\n```{r}\nsamples_zwielicht %>% \n  count(gewinnchance_muenze > 0.45 & gewinnchance_muenze < 0.55) %>% \n  mutate(prop = n/sum(n))\n```\n\nDie Wahrscheinlichkeit für eine \"einigermaßen faire\" Münze ist klein, etwa 5%!\n\n:::\n\n\n:::{#exm-zwielicht2}\n\n### Münze gezinkt?\n\nSchauen wir uns an, wie wahrscheinlich es ist - gegeben der Daten und unserem Modell -\ndass die Münze massiv gezinkt ist. \"Massiv\" definieren wir dabei mit \"mindestens 70% Trefferwahrscheinlichkeit\"^[ja, das ist subjektiv], also $\\pi >= .7$^[Führende Nullen bei Anteilen werden oft weggelassen, man schreibt also oft .7 wenn man 0.7 bzw. 70% meint. Das ist nicht nur kürzer, sondern man weiß auch direkt dass es sich um einen Anteil handelt. Behält man die führende Null bei, etwa 0.77, so würde das signalisieren, dass die Zahl auch größer als Null sein könnte.].\n\n```{r}\nsamples_zwielicht %>% \n  count(gewinnchance_muenze > .7) %>% \n  mutate(prop = n / sum(n))\n```\n\nWir finden eine recht hohe Wahrscheinlichkeit für eine \"massive\" Manipulation der Münze.\n\n:::\n\n\n:::callout-important\nIst es nicht einfach und schön, wie wir mit Hilfe des Stichprobenziehens allerlei Forschungsfragen beantworten können?\nEine Post-Verteilung aus Stichproben erlaubt uns, viele Fragen mit einfachen Methoden,\nnämlich schlichtes Zählen, zu beantworten.\n:::\n\nNatürlich könnte (und sollte?) man unser Modell kritisieren.\nIst es wirklich sinnvoll, die Trefferwahrscheinlichkeit apriori als gleichverteilt anzunehmen?\nDas heißt ja, wir glauben, dass eine Trefferwahrscheinlichkeit von 99,99999% genauso wahrscheinlich ist wie 50,55555%.\nAuf der anderen Seite: Der Charme einer Gleichverteilung ist, dass sie objektiv ist,\nin dem Sinne, dass wir keinerlei Information einfließen lassen.\nWir sind indifferent gegenüber dem Parameter $\\pi$, der Trefferwahrscheinlichkeit.\n\n\n\n:::callout-note\nIn einem zweiten Versuch könnten wir jetzt unsere Post-Verteilung als Priori-Verteilung \nnutzen. \nDas Ergebnis des ersten Versuchs wird dann hergenommen als Ausgangspunkt für \neinen zweiten Versuch.\nDamit wird das Wissen der Wissenschaft weitergegeben^[übrigens auf mathematisch gesehen ideale Art und Weise.], so wie es sein sollte.\n:::\n\n\n\n## Mit Stichproben neue Beobachtungen simulieren\n\n\nZur Erinnerung: Der Likelihood (L) zeigt die Wahrscheinlichkeit eine Trefferzahl gegeben eines bestimmten Parameterwerts.\nIn unseren Beispiel könnten wir z.B. die drei Likelihoods für $w=0,1,2$ ausrechnen, gegeben $N=2$ und $p = 0.5$:\n\n```{r QM2-Thema3-Post-befragen-23}\nL <- dbinom(0:2, size = 2, prob = 0.5)\nL\n```\n\n\nAh, die Wahrscheinlichkeit für 0 oder 2 Treffer beträgt 50%, wenn $pi=1/2$; für 1 Treffer beträgt sie entsprechend 50%^[das sollte uns bekannt vorkommen].\n\n\n\n### Wir simulieren die Wasserzahl bei Globuswürfen {#sec-rbinom}\n\nZurück zu unserem Globusversuch!\n\nWir könnten uns jetzt Globusbälle basteln mit verschiedenen Wasseranteilen, \nund diese oft hochwerfen.\nDamit könnten wir herausfinden, welche Trefferzahlen sich bei verschiedenen Wasseranteilen finden lassen würden.\n\nWer gerne bastelt, freut sich darauf. Kritischere Geister^[oder weniger bastelfreundliche] würden den Aufwand bemängeln und die Frage nach dem Zweck der Übung stellen^[bravo!].\n\n:::callout-important\nWenn wir wissen, welche Trefferzahlen laut einem Modell zu erwarten sind,\nkönnen wir die *echten* (beobachteten) Trefferzahlen mit den laut Modell zu erwartenden vergleichen.\nDamit haben wir eine Methode, mit dem wir ein Modell auf Herz und Nieren prüfen können.\nEin schlechtes Modell wird mit seinen Vorhersagen an der Realität scheitern: Erwartung des Modells und beobachtete Daten passen nicht zusammen.\nSagt ein Modell etwa $W=9$ vorher bei $N=9$, aber wir finden $W=0$,\nso wird unser Vertrauen in das Modell erschüttert sein.\nSimulation von Trefferzahlen sind also ein Modell, um die Glaubwürdigkeit unseres Golems zu prüfen. (Nicht nur) bei Golems gilt: Vertrauen ist gut, Kontrolle ist besser.\n:::\n\n\nLos geht's: \nWir simulieren $n=1$ neuen Globusversuch mit $N=2, p=0.7$ und zählen die (Wasser-)Treffer:\n\n```{r QM2-Thema3-Post-befragen-25}\nset.seed(42)  # Zufallszahlen festlegen\nrbinom(n = 1, size = 2, prob = .7)  # 0 Treffer (Wasser)\n```\n\n\nDas geht wie man sieht mit `rbinom`: *r* wie *random* (zufällig) und *binom* wie binomial verteilt,\ndie Münzwurfverteilung.\n\nHier sind die Argumente der Funktion `rbinom` noch etwas näher erklärt:\n\n```{r}\n#| eval: false\n\nrbinom(n = Wie oft soll der Versuch wiederholt werden?,\n       size = Wie viele Globuswürfe pro Versuch (Stichprobengröße),\n       prob = Wie hoch ist die Wahrscheinlichkeit für Wasser (bzw. für einen Treffer))\n```\n\n\nWeiter: Warum nicht $n=10$ neue Globusversuche simulieren?\n\n```{r QM2-Thema3-Post-befragen-27, echo = TRUE}\nrbinom(n = 10, size = 2, prob = 0.7)\n```\n\n\n\"Simulieren\" heißt hier, wir lassen den Computer den Globus werfen,\nganz anschaulich gesprochen.\nNatürlich wirft der Computer nicht in Wirklichkeit einen Globus oder eine Münze,\nsondern er zieht aus der Menge `{0,1}` eine Zahl, und wir geben die Wahrscheinlichkeit für jedes der beiden Elemente vor, z.B. jeweils 50%.^[Übrigens können Computer nicht echten Zufall erzeugen (das kann vermutlich niemand), aber durch gewisse verzwickte Rechnungen sind die Zahlen, die der Computer uns präsentiert, nicht oder kaum vom \"Zufall\" zu unterscheiden, also z.B. gleichverteilt ohne besondere Muster.].\n\n:::callout-important\nSimulationsdaten geben Aufschluss, welche Daten (wie oft Wasser) man bei einem bestimmten Modell, $p,N$, erwarten kann. \nMünzwürfe - und analoge Versuche, wie Globuswürfe - kann man in R mit `rbinom` erstellen (simulieren).\n:::\n\n\n\n### Traue niemals einem Golem (einem Modell)\n\n\n\n\n\n![Never trust a Golem](img/5qmhmo.jpg){width=25%}\n\n\nQuelle: https://imgflip.com/i/5qmhmo\n\n\n\nImmer prüfen und wachsam bleiben:\n\n- (Inwieweit) decken sich die simulierten Daten mit den tatsächlichen Beobachtungen?\n- Wie realistisch sind die Modellannahmen?\n- Kann man das Modell aus verschiedenen Perspektiven prüfen?\n\n\n\n## Mit guten Simulationen kommt man den wahren Werten nahe\n\n\nWarum nicht $n=10^6$ neue Globusversuche simulieren^[Wer R nicht mag, ist eingeladen, diesen Versuch von Hand mit selbstgebastelten Globusbällen zu wiederholen.]:\n\n```{r QM2-Thema3-Post-befragen-28, echo = TRUE}\ndraws <- \n  tibble(\n    draws = rbinom(1e6, size = 2, prob = .7))\n\ndraws %>% \n  count(draws) %>% \n  mutate(prop = n / sum(n))\n```\n\nDiese simulierten Häufigkeiten sind sehr ähnlich zu den theoretisch bestimmten Häufigkeiten mit `dbinom`: Unser Modell liefert plausible Vorhersagen^[Braver Golem!].\n\n```{r QM2-Thema3-Post-befragen-29, echo = TRUE}\ndbinom(0:2, size = 2, prob = .7)\n```\n\n\n## Stichprobenverteilung\n\nWir ziehen viele ($n=10^6$) Stichproben für unseren typischen Globusversuch: $N=9$ Globuswürfe mit $p=0.7$. \n\nWie viele Wasser (W) erhalten wir wohl typischerweise in diesem Versuch?\nDie Verteilung der zu erwartenden Treffer ist in @fig-globus-striprovert dargestellt.\n\n\n```{r QM2-Thema3-Post-befragen-30, echo = TRUE, results = \"hide\", eval = FALSE}\nn_draws <- 1e6\n\ndraws <- \n  tibble(draws = rbinom(n_draws, size = 9, prob = .7))\n\nplot1 <- \n  draws %>% \n  ggplot(aes(x = draws)) +\n  geom_histogram() \n```\n\n\n\n```{r QM2-Thema3-Post-befragen-31}\n#| echo: false\n#| label: fig-globus-striprovert\n#| fig-cap: Anteile (Wahrscheinlichkeit), die man für jede Wasserzahl in unserem Globusversuch erwarten kann\nn_draws <- 1e6\ndraws <- tibble(draws = rbinom(n_draws, \n                               size = 9, \n                               prob = .7))\n\n# the histogram\ndraws %>% \n  ggplot(aes(x = draws)) +\n  geom_histogram(binwidth = 1, center = 0,\n                 color = \"grey92\", size = 1/10) +\n  scale_x_continuous(\"Anzahl Wasser (W) pro Versuch\",\n                     breaks = seq(from = 0, to = 9, by = 1)) +\n  scale_y_continuous(\"Häufigkeit\",\n                     labels = scales::scientific) +\n  coord_cartesian(xlim = c(0, 9)) +\n  theme(panel.grid = element_blank()) +\n  labs(title = \"Stichprobenverteilung für n=9 und p=.7 (binomial verteilt)\")\n```\n\nDie *Stichprobenverteilung* zeigt, welche Stichprobendaten laut unserem Modell (einem bestimmten Parameterwert) zu erwarten sind. Wir können jetzt prüfen, ob die echten Daten zu den Vorhersagen des Modells passen.\n\n:::callout-note\nDie Stichprobenverteilung ist keine empirische Verteilung: Wir führen diese vielen Versuche nicht wirklich durch^[Nur die extremen Bastelfreunde machen das], wir simulieren sie nur am Computer.\n:::\n\n\n\n## Die Posterior-Prädiktiv-Verteilung (PPV)\n\n\n### Was ist die PPV und wozu ist sie gut?\n\n\nUnsere Stichprobenverteilung zeigt, welche Trefferzahlen bei einem bestimmten Parameterwert, z.B. $\\pi=.7$ in welchen Anteilen zu erwarten sind.\nAllerdings sind wir uns ja nicht sicher, dass der Wasseranteil genau 70% beträgt.\nUnser (Un-)Wissen über den Wasseranteil wird ja gerade in der Post-Verteilung gespeichert.\n\nUm eine ehrliche(re) Antwort auf die Frage zu erhalten, wie viele Treffer^[Im Globusversuch ist Wasser der \"Treffer\"; in einem Münzwurf-Versuch könnte \"Kopf\" der Treffer und die Anzahl der geworfenen Köpfe die Trefferzahl sein] zu erhalten ist, müssen wir die Post-Verteilung berücksichtigen.\n\nWir brauche ein Stichprobenverteilung für *jeden* Wert der Post-Verteilung.\nWenn wir dann die resultierenden Strichprobenverteilungen mitteln,\nhaben wir einen ehrlichen Überblick über die zu erwartenden Trefferzahlen.\nDabei sollten wir natürlich wahrscheinliche Parameterwerte höher gewichten als unwahrscheinliche.\nSo sollte etwa der (hoch wahrscheinliche) Wasseranteil von 70% ein hohes Gewicht beim Mitteln der Stichprobenverteilung erhalten;\nder sehr unwahrscheinliche Wasseranteil^[zumindest laut unserer Post-Verteilung] von 1% Wasser, sollte entsprechend weniger gewichtet werden, beim Zusammenfassen (d.h. Mittelwert bilden) der Stichprobenverteilungen.\n\nDie resultierende Verteilung - gemittelte Stichprobenverteilungen über alle Werte der Post-Verteilungen - nennt man *Posterior-Prädiktiv-Verteilung* (PPV).\n\n\n:::callout-important\n\nDie PPV entsteht als gewichteter Mittelwert der Stichprobenverteilungen. Die Gewichte sind die Wahrscheinlichkeiten (bzw. Wahrscheinlichkeitsdichten) der Post-Verteilung.\n:::\n\n\n\n:::{#exm-wozu-ppv}\n\n## Magnus Lagrande braucht die PVV\n\n\nIm Jahre $10^{99}$ wird das Universum von Magnus Lagrande regiert.\nDie Weltraumbehörde, für die Sie arbeiten, ist ihm unterstellt.\nDer Regent findet Ihre Untersuchungen zwar ganz nett,\naber leider versteht er keine Wahrscheinlichkeit.\nIst ihm zu abstrakt, sagt er. \n\"Oder können Sie mir mal so eine Wahrscheinlichkeit in die Hand geben?\nKönnen Sie sagen, Achtung, da hinten rennt eine Wahrscheinlichkeit, fang sie!\"\nMagnus ist also ein Freund für des Konkreten. \nEinige einflussreiche Gruppen an Statistiks [unterstützen diese Haltung](https://www.routledge.com/Predictive-Inference/Geisser/p/book/9780367449919)\n\nJedenfalls hätte Magnus gerne eine Aussage wie \n\"Vermutlich sehen wir beim nächsten Versuch irgendwas zwischen 4 und 7 Treffern\".\n\nNatürlich haben Sie den Anspruch,\neine wissenschaftlich belastbare Aussage zu tätigen.\n\nWas nun? Sie müssen sozusagen die Post-Verteilung in eine Post-Verteilung der Beobachtungen,\nalso der konkreten Werte - in diesem Fall die Anzahl der Wassertreffer - übersetzen.\nGenau das macht die PPV für Sie!\n:::\n\n\n\n\n\n\n### Visualisierung der PPV\n\nDer Prozess des gewichteten Zusammenfassens der Stichprobenverteilungen ist in @fig-ppv dargestellt.\n\n![PPV als gewichtetes Kombinieren der Stichprobenverteilungen](img/ppv.png){#fig-ppv}\n\nQuelle: @mcelreath_statistical_2020\n\n\n\n### PPV berechnen\n\n\n\n<!-- Hier fehlt `posterior_predict` aus rstanarm oder `estimate_prediction` aus modelbased: https://easystats.github.io/modelbased/reference/estimate_expectation.html -->\n\n\n\nDie PPV für unseren Standard-Globusversuch ($N=9$) berechnen wir so:\n\nWir berechnen viele (z.B. $10^4$) Stichprobenverteilungen.\nDabei müssen wir jedes Mal fragen, wie groß die Wahrscheinlichkeit $\\pi$ für Wasser^[d.h. einen Treffer] ist.\nWasseranteile $\\pi$,  die laut Post-Verteilung *wahrscheinlich* sind,\nmüssen wir entsprechend *oft* als Parameterwert ($\\pi$) der Stichprobenverteilung verwenden;\numgekehrt dürfen  wir nur wenige Stichprobenverteilungen für unwahrscheinliche Parameterwerte erstellen.\n\nBeispielsweise würden wir *viele* Stichprobenverteilungen für $\\pi=.7$ erstellen;\nfür $\\pi=0.01$ würden wir *wenige* Stichprobenverteilungen erstellen, s. @fig-ppv.\n\nGlücklicherweise spiegelt unsere Stichproben-Postverteilung `samples` wahrscheinlichere Parameterwerte wieder, indem wahrscheinlichere Parameterwerte häufiger vorkommen.\n\n\n::: callout-note\nWahrscheinliche Parameterwerte kommen in der Stichproben-Postverteilung `samples` häufiger vor.\nDie Häufigkeit der Parameterwerte spiegelt die Wahrscheinlichkeit der jeweiligen Parameterwerte in der (theoretischen) Postverteilung wider.\n:::\n\n\nSchauen Sie sich vielleicht zur Erinnerung noch einmal die Definition von `samples` an,  s. @lst-post-sample. \nTabelle `samples`, die aus Stichproben aus der Post-Verteilung besteht, ist (in Auszügen) in @tbl-postsample1 dargestellt. \nWie die Post-Verteilung auf Basis von Stichproben dann aussieht sieht man in @fig-samples1.\nGlobusversuche kann man mit `rbinom` simulieren, s. @sec-rbinom.\n\n\n\nWir simulieren also viele (z.B $10^4$) Globusversuche, jeweils mit $N=9$ Würfen.\nWahrscheinliche Parameterwerte, etwa $\\pi=7$, sollen häufiger verwendet werden (bei unseren vielen Globusversuchen) als unwahrscheinliche.\n\nPraktischerweise sind die Werte in der Spalte `p_grid` in `samples` so häufig vertreten, wie ihre Wahrscheinlichkeit es erwarten lässt. Hier ist ein Auszug aus `samples`:\n\n```{r}\nsamples %>% \n  select(p_grid) %>% \n  slice_head(n = 10)\n```\n\nWie man sieht, sind wahrscheinliche Parameterwerte häufiger vertreten.^[An dieser Stelle sollten Sie sichd die ganze Spalte `p_grid` anschauen, um sich von dieser Behauptung mit eigenen Augen zu überzeugen.]\n\n\n\n`p_grid` ist also eine Liste^[technisch in R ein *Vektor*] von Parameterwerten, deren Häufigkeit die Wahrscheinlichkeit der Parameterwerte gewichtet.\n\nAuf dieser Basis können wir die PPV erstellen:\n\n\n\n\n\n```{r ppv1, echo = TRUE, eval = TRUE}\nppv <- \n  rbinom(1e4, \n         size = 9, \n         prob = samples$p_grid) %>% \n  as_tibble()\n\nhead(ppv)\n```\n\n\nSchauen wir uns ein Histogramm aller Trefferzahlen an, s. @fig-ppv2.^[Es kann auch dem Verständnis helfen, dass Sie sich alle Werte der Tabelle `ppv` selber in Ruhe anschauen, um sich zu überzeugen, welche Wasserzahlen (Trefferzahlen) häufiger und welche seltener vorkommen.]\n\n```{r}\n#| fig-cap: Die PPV für unseren Standard-Globusversuch (N=9)\n#| label: fig-ppv2\n#| echo: false\nppv_plot2 <-\n  ppv %>% \n  ggplot() +\n  aes(x = value) +\n  geom_bar() +\n  scale_x_continuous(\n    breaks = 0:9)\n\nppv_plot2\n```\n\n\n\n\n\nDie PPV unseres Modells zeigt uns (@fig-ppv2), dass wir in künftigen Versuchen zumeist 6 Treffer zu erwarten haben. \nAber ein relativer breiter Bereich an Treffern ist ebenfalls gut laut unserer PPV erwartbar.\n\n\n:::callout-important\nDie PPV zeigt, welche Beobachtungen laut unserem Modell häufig und welche selten sind.\nDie PPV zeigt keine Parameterwerte, sondern welche Daten (Beobachtungen, Wasserzahlen) wir in künftigen Versuchen wie häufig erwarten können.\n:::\n\n\n:::{#exm-ppv1}\n\n## Der nächste Planet\n\nNur zum Spaß spulen wir kurz die Zeit im Universum vor, sagen wir so $10^{99}$ Jahre.\nSie arbeiten bei einer Raumfahrtbehörde, die nach neuen Planeten sucht.\nNun wurde ein aussichtsreicher Planet gesichtet. \nIhre Behörde hat eine Studie gestartet, im Rahmen derer 9 Sonden zu diesem (weit entfernten) Planeten geschossen sind. Von den 9 Sonden sind 6 im Wasser gelandet,\nwas aus Gründen intergalaktischer Wasserknappheit eine gute Nachricht ist.\n\n\n>   \"Der nächste Planet wird sicher 6 von 9 Wassertreffer erzielen!\"\n\n-- Presse-Chefi der intergalaktischer SpaceY Raumfahrtsbehörde\n\n\n\nJetzt plant Ihre Behörde den Versuch zu wiederholen: Wieder sollen 9 Sonden zu diesem Planeten geschossen werden.\nDis Presse-Chefi^[In der Zeit dieses Beispiels ist es üblich, kein fixes Geschlecht zu haben] tönt vollmundig: \"Ich bin sicher, dass wir wieder 6 von 9 Treffer, also 6 von 9 Mal Wasser, haben werden!\".\n\nKann man diese Aussage mit (hoher) Sicherheit leisten? Perfekte Sicherheit gibt es bekanntlich nur, was Tod und Steuern betrifft, aber kann diese Aussage mit zumindest hoher Sicherheit geleistet werden?\n\nNein, die PPV (@fig-ppv2) zeigt deutlich, dass unser Wissen nicht ausreicht, um präzise Vorhersagen über künftige Ausgänge des Versuchs zu leisten. So sind auch 5 oder 7 Treffer gut möglich. Auch 4 oder 8 Treffer sind nicht so selten. Sogar 9 Treffer sind nicht super selten.\n\nDis Presse-Chefi Ihrer Behörde sollte also den Mund nicht so voll nehmen.\n\n:::\n\n\n\n## Fazit\n\n\n### Vorhersagen sind schwierig\n\n\n... gerade wenn sie die Zukunft betreffen, so ein Sprichwort.\n\nDas zeigt uns die PPV: Der PPV unseres Modells gelingt es zwar, der theoretisch wahrscheinlichste Parameterwert ist auch der häufigste in unseren Stichproben, aber die Vorhersagen haben eine große Streuung, bergen also recht hohe Ungewissheit.\nDie PPV zeigt also, welche Beobachtungen laut unserem Modell künftig zu erwarten sind, s. @fig-ppv2.\n\nWürde man die Vorhersagen nur anhand eines bestimmten Parameterwertes (z.B $p=0.6$) vornehmen, hätten die Vorhersagen *zu wenig Streuung* in den Vorhersagen, würden also die Ungewissheit nicht ausreichend abbilden. Es würde *Übergewissheit* (Overconfidence, Overfitting) resultieren.\n\n[Wir brauchen die PPV](https://imgflip.com/i/6zm1hh). Ohne die PPV können wir nicht seriös abschätzen, wie viel Ungewissheit in unseren Vorhersagen steckt.\n\n\n\n\n\n\n### Zwei Arten von Ungewissheit in Vorhersagen von Modellen\n\n\n1. *Ungewissheit innerhalb des Modells* (\"intrinsische\" Ungewissheit): Auch wenn der (oder die) Modellparameter eines Modells mit Sicherheit bekannt sind, so bleibt Unsicherheit, welche Beobachtung eintreten wird: Auch wenn man sicher weiß, dass $p=1/4$ Murmeln blau sind, so kann man nicht sicher sagen, welche Farbe die nächste Murmel haben wird (Ausnahme: $p=1$ oder $p=0$).\n\n2. *Ungewissheit in den Modellparametern*: Wir sind uns nicht sicher, welchen Wert $p$ (bzw. die Modellparameter) haben. Diese Unsicherheit ist in der Post-Verteilung dargestellt. \n\nUm zu realistischen Vorhersagen zu kommen, möchte man beide Arten von Ungewissheit berücksichtigen: Das macht die *Posteriori-Prädiktiv-Verteilung (PPV)*.\n\n\n\nDie PPV zeigt, welche Daten das Modell vorhersagt (prädiktiv) und mit welcher Häufigkeit, basierend auf der Post-Verteilung.\n\n\n:::callout-note\nDer Unterschied zwischen der Post-Verteilung und der PPV ist erstmal,\ndass die PPV *Ausprägungen* in ihrer Wahrscheinlichkeit bemisst,\nalso z.B. wie wahrscheinlich 4 von 9 Wassertreffern sind.\nDie Post-Verteilung bemisst die Wahrscheinlichkeit von Parameterwerten,\nalso z.B. des Wasseranteils.\n\nEtwas tiefer betrachtet zeigt die PPV zwei Arten von Ungewissheit,\ndie Post-Verteilung nur eine. \nDie PPV zeigt erstens die Ungewissheit zur Verteilung des Parameters (wie die Post-Verteilung),\naber auch noch die intrinsische Ungewissheit.\nDenn auch wenn wir keine Ungewissheit zum Parameter hätten,\nbliebe Ungewissheit, welche Beobachtungen sich manifestieren.\nInsofern ist die PVV \"ehrlicher\",\nsie spiegelt die Ungewissheit zu den Beobachtungen wider.\n:::\n\n\n\n### Vergleich der Verteilungen\n\n@fig-post-pred-ppv-anim stellt die in diesem Kapitel diskutierten Verteilungen gegenüber:\n\n\n\n- Links - *Posterior-Verteilung*: Wahrscheinlichkeiten der Parameterwerte\n- Mitte - *Stichprobenverteilung*: Wahrscheinlichkeiten der Beobachtungen gegeben eines bestimmten Parameterwertes\n- Rechts - *Posterior-Prädiktiv-Verteilung*: Wahrscheinlichkeiten der Beobachtungen unter Berücksichtigung der Unsicherheit der Posteriori-Verteilung\n\n```{r}\n#| echo: false\n#| label: fig-post-pred-ppv-anim\n#| fig-cap: Post- vs. Stichproben- vs. PP-Verteilungen\nif (knitr:::is_html_output()) {\n  knitr::include_graphics(\"img/post-pred-ppv-anim.gif\")\n}\n```\n\n\n\n\n\n\n[Quelle: R. McElreath](https://twitter.com/rlmcelreath/status/1448978045247893505)\n\n\n\n\n\n\n### So viele Verteilungen... \n\n\n\n\n- Die *Posteriori-Verteilung* gibt Aufschluss zur Häufigkeit (Wahrscheinlichkeit) von Parameterwerten:\n    - Wie wahrscheinlich ist es, dass \"in Wirklichkeit\" der Wasseranteil 70% beträgt, also $\\pi=.7$\n    - In der Wissenschaft ist man meist an den Parametern interessiert.\n    \n- Die *PPV* gibt Aufschluss zur Häufigkeit von neuen Beobachtungen:\n    - Welche Beobachtungen (wie viele Wasser/Treffer) sind in Zukunft, bei erneuter Durchführung, zu erwarten.\n    - Für die Praxis kann das eine interessante Frage sein.\n    \n- Der *Likelihood* gibt Aufschluss, wie gut eine bestimmte Hypothese die Datenlage erklärt.\n    - Wie gut passt die Hypothese $\\pi=0.7$ auf die Datenlage 6 von 9 Treffern beim Globusversuch?\n    - Der Likelihood kann aus der Stichprobenverteilung herausgelesen werden. \n\n```{r QM2-Thema3-Post-befragen-33}\n#| echo: false\nn_draws <- 1e5\n\nsimulate_binom <- function(probability) {\n  set.seed(3)\n  rbinom(n_draws, size = 9, prob = probability) \n}\n\nd_small <-\n  tibble(probability = seq(from = .1, to = .9, by = .1)) %>% \n  mutate(draws = purrr::map(probability, simulate_binom)) %>% \n  unnest(draws) %>% \n  mutate(label = str_c(\"p = \", probability))\n```\n\n```{r PP2}\n#| echo: false\nppv2_plot <- \nd_small %>%\n  ggplot(aes(x = draws)) +\n  geom_histogram(binwidth = 1, center = 0,\n                 color = \"grey92\") +\n  scale_x_continuous(\"Wasser\", breaks = seq(from = 0, to = 9, by = 3)) +\n  scale_y_continuous(NULL, breaks = NULL) +\n  labs(subtitle = \"Stichprobenverteilungen\") +\n  coord_cartesian(xlim = c(0, 9)) +\n  theme(panel.grid = element_blank()) +\n  facet_wrap(~ label, ncol = 9) \n```\n\n\n\n## Aufgaben\n\n\n1. [Zwielichter Dozent-Bayes](https://datenwerk.netlify.app/posts/zwielichter-dozent-bayes/zwielichter-dozent-bayes)\n2. [Warum Bayes?](https://datenwerk.netlify.app/posts/warum-bayes/warum-bayes)\n3. [subjektiv-Bayes](https://datenwerk.netlify.app/posts/subjektiv-bayes/subjektiv-bayes)\n4. [Likelihood2](https://datenwerk.netlify.app/posts/likelihood2/likelihood2)\n5. [Anteil-Apple](https://datenwerk.netlify.app/posts/anteil-apple/anteil-apple)\n6. [ReThink3m1](https://datenwerk.netlify.app/posts/rethink3m1/rethink3m1)\n7. [ReThink3m2](https://datenwerk.netlify.app/posts/rethink3m2/rethink3m2)\n8. [ReThink3m3](https://datenwerk.netlify.app/posts/rethink3m3/rethink3m3)\n9. [ReThink3m4](https://datenwerk.netlify.app/posts/rethink3m4/rethink3m4)\n10. [ReThink3m5](https://datenwerk.netlify.app/posts/rethink3m5/rethink3m5)\n11. [Quiz zu Verteilungen](https://datenwerk.netlify.app/#category=Verteilungen-Quiz)\n\n\n\n\n"},"formats":{"html":{"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"kable","error":false,"eval":true,"cache":null,"freeze":"auto","echo":true,"output":true,"warning":false,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"knitr"},"render":{"keep-tex":false,"keep-yaml":false,"keep-source":false,"keep-hidden":false,"prefer-html":true,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":true,"code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","toc":true,"number-sections":true,"output-file":"ppv.html"},"language":{},"metadata":{"lang":"de","fig-responsive":true,"quarto-version":"1.1.168","bibliography":["references.bib"],"editor":"source","knitr":{"opts_chunk":{"collapse":true,"R.options":{"knitr.graphics.auto_pdf":true}}},"theme":"cosmo"},"extensions":{"book":{"multiFile":true}}},"pdf":{"execute":{"fig-width":5.5,"fig-height":3.5,"fig-format":"pdf","fig-dpi":300,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":"auto","echo":true,"output":true,"warning":false,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"knitr"},"render":{"keep-tex":true,"keep-yaml":false,"keep-source":false,"keep-hidden":false,"prefer-html":true,"output-divs":true,"output-ext":"pdf","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":true,"merge-includes":true,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"pdf-engine":"xelatex","standalone":true,"variables":{"graphics":true,"tables":true},"default-image-extension":"pdf","to":"pdf","toc":true,"number-sections":true,"output-file":"ppv.pdf"},"language":{},"metadata":{"block-headings":true,"lang":"de","bibliography":["references.bib"],"editor":"source","knitr":{"opts_chunk":{"collapse":true,"R.options":{"knitr.graphics.auto_pdf":true}}},"colorlinks":true,"papersize":"a4"},"extensions":{"book":{}}}}}