# Globusversuch

![Bayes:Start!](img/Golem_hex.png){width=5%}

## Von Welten und Golems

### Kleine Welt, gro√üe Welt

Bekanntlich segelte Kolumbus 1492 los, und entdeckte Amerika. Das war aber ein gl√ºcklicher Zufall, denn auf seinem Globus existierte Amerika gar nicht. Vielleicht sah sein Globus so aus wie der von Behaim, s. Abb @fig-behaim.

![Behaims Globus: Kein Amerika](img/Behaim.jpg)

[Quelle](https://de.wikipedia.org/wiki/Martin_Behaims_Erdapfel#/media/Datei:RavensteinBehaim.jpg)

Die *kleine Welt des Modells* entsprach hier nicht *der gro√üen Welt, der echten Erdkugel*.

Das ist ein Beispiel, das zeigt, wie Modellieren schiefgehen kann. Es ist aber auch ein Beispiel f√ºr, sagen wir, die Komplexit√§t wissenschaftlicher (und sonstiger) Erkenntnis. Einfach gesagt: Gl√ºck geh√∂rt halt auch dazu.

| Kleine Welt                                                | Gro√üe Welt                                 |
|-----------------------------------------|-------------------------------|
| Die Welt, wie sie der Golem sieht                          | Die Welt, wie sie in Wirklichkeit ist      |
| ist das Modell, aber nicht (zwangsl√§ufig) die Wirklichkeit | entspricht nicht (zwangsl√§ufig) dem Modell |
| Verwenden wir beim Modellieren                             | Ist das, was wir modellieren               |

: Kleine Welt vs. gro√üe Welt

::: callout-note
Behaims Globus ist nicht gleich der Erde. Die kleine Welt ist nicht die gro√üe Welt.
:::

Was in der kleinen Welt funktioniert, muss nicht in der gro√üen Welt funktionieren. Modelle zeigen immer nur die kleine Welt: Vorsicht vor schnellen Schl√ºssen und vermeintlicher Gewissheit.

üèã Nennen Sie ein Beispiel, in dem ein Modell nicht (exakt) der Wirklichkeit entspricht!





### Der Golem von Prag

![Der Golem von Prag](img/170px-Golem_and_Loew.jpg)

[Quelle](https://de.wikipedia.org/wiki/Golem)

Der Golem von Prag, eine vom Menschen geschaffene Kreatur gewaltiger Kraft, die Befehle w√∂rtlich ausf√ºhrt.

Bei kluger F√ºhrung kann ein Golem N√ºtzliches vollbringen.

Bei un√ºberlegter Verwendung wird er jedoch gro√üen Schaden anrichten.

### Wissenschaftliche Modelle sind wie Golems

**Golem**

![](img/Golem_hex.png)]{width=25%}

-   Besteht aus Lehm
-   Belebt durch "Wahrheit"
-   M√§chtig
-   dumm
-   F√ºhrt Befehle w√∂rtlich aus
-   Missbrauch leicht m√∂glich
-   M√§rchen

**Modell**

```{mermaid}
flowchart LR
X --> Y
```

-   Besteht aus ~~Lehm~~Silikon
-   Belebt durch Wahrheit (?)
-   Manchmal m√§chtig
-   simpler als die Realit√§t
-   F√ºhrt Befehle w√∂rtlich aus
-   Missbrauch leicht m√∂glich
-   Nicht einmal falsch

::: callout-important
Wir bauen Golems.
:::

### So denkt unser Bayes-Golem

![So denkt unser Bayes-Golem](img/bayesupdate2.png)

üèã Bayes-Inferenz √§hnelt dem Lernen von Menschen. Geben Sie ein Beispiel von Lernen bei Menschen, das oben dargestelltem Prozess √§hnelt!

## Ein erster Versuch: Wir werfen den Globus

### Welcher Anteil der Erdoberfl√§che ist mit Wasser bedeckt?

Unsere Hypothese bzw. unsere Forschungsfrage lautet, mit welchem Anteil die Erde wohl mit Wasser bedeckt ist?

![Der Erdball](img/earth.png){width="50%"}

[Quelle](https://pngimg.com/image/25340) CC 4.0 BY-NC

Sie werden einen Globus-Ball in die Luft und fangen in wieder auf. Sie notieren dann, ob die Stelle unter Ihrem Zeigefinger Wasser zeigt (W) oder Land (L). Den Versuch wiederholen Sie 9 Mal.

So sah mein Ergebnis aus:

$$W \quad L \quad W \quad W \quad W \quad L \quad W \quad L \quad W$$

üèãÔ∏èÔ∏è Besorgen Sie sich einen Globus (zur Not eine M√ºnze) und stellen Sie den Versuch nach!



### Wie entstanden die Daten?

Der physikalische Prozess, der zur Entstehung der Daten f√ºhrt, nennt man den *den datengenierende Prozess*.

In diesem Fall kann man ihn so beschreiben:

1.  Der wahre Anteil von Wasser, $W$, der Erdoberfl√§che ist $p$ (und $1-p$ ist der Anteil Land, $L$).
2.  Ein Wurf des Globusballes hat die Wahrscheinlichkeit $p$, eine $W$-Beobachtung zu erzeugen.
3.  Die W√ºrfe des Globusballes sind unabh√§ngig voneinander.
4.  Wir haben kein Vorwissen √ºber $p$; jeder Wert ist uns gleich wahrscheinlich.

üèã Welche Annahmen w√ºrden Sie √§ndern? Welche k√∂nnte man wegnehmen? Welche hinzuf√ºgen? Was w√§ren die Konsequenzen?



### Wissen updaten: Wir f√ºttern Daten in das Modell

```{r QM2-Thema2-kleineModelle-19}
#| echo: false
#| out.width="100%"
source("R-Code/img221.R")
```

Unser Golem (das Modell) lernt. Ob das Modell n√ºtzlich ist (pr√§zise Vorhersagen liefert), steht auf einem anderen Blatt.

### Ein paar Fachbegriffe

-   F√ºr jede Hypothese haben wir ein Vorab-Wissen, das die jeweilige Plausibilit√§t der Hypothese angibt: *Priori-Verteilung*.

-   F√ºr jede Hypothese (d.h. jeden *Parameterwert* $p$) m√∂chten wir wie wahrscheinlich die Daten sind (unter der Annahme, dass die Hypothese richtig ist). Das gibt uns den *Likelihood*.

-   Dann gewichten wir den Likelihood mit dem Vorabwissen, so dass wir die *Posteriori-Verteilung*[^globusversuch-1] bekommen.

[^globusversuch-1]: Anstatt von *Priori* liest man auch *Prior*; anstatt *Posteriori* auch *Posterior*

![Updating mit Bayes](img/bayesupdate.png)

### WDie Binomialverteilung

Wir nehmen an, dass die Daten unabh√§ngig voneinander entstehen und sich der Parameterwert nicht zwischenzeitlich √§ndert[^globusversuch-2].

[^globusversuch-2]: Die sog. "iid-Annahme", wie *i*ndependently and *i*dentically distributed: Jeder Wurf der Globusballes ist eine Realisation der gleichen Zufallsvariablen

Dann kann man die Wahrscheinlichkeit ($Pr$), $W$ mal Wasser und $L$ mal Land zu beobachten, wenn die Wahrscheinlichkeit f√ºr Wasser $p$ betr√§gt, mit der *Binomialverteilung* berechnen.

Die Binomialverteilung zeigt die Verteilung der H√§ufigkeit (Wahrscheinlichkeit) der Ereignisse (z.B. 2 Mal Kopf) beim wiederholten M√ºnzwurf (und allen vergleichbaren Zufallsexperimenten)[^globusversuch-3].

[^globusversuch-3]: "M√ºnzwurfverteilung"

$$Pr(W,L|p) = \frac{(W+L)!}{W!L!}p^W(1-p)^L$$

### Binomialverteilung mit R

Was ist der Anteil der g√ºltigen Pfade (Wahrscheinlichkeit), um 6 mal $W$ bei $N=W+L=9$ W√ºrfen zu bekommen, wenn wir von $p=1/2$ ausgehen?

```{r QM2-Thema2-kleineModelle-21, echo = TRUE}
dbinom(x = 6, size = 9, prob = 1/2)
```

Was ist die Wahrscheinlichkeit f√ºr $W=9$ bei $N=9$ und $p=1/2$?

```{r QM2-Thema2-kleineModelle-22, echo = TRUE}
dbinom(x = 9, size = 9, prob = 1/2)
```

### Beispiele zur Berechnung einer binomial verteilten Wahrscheinlichkeit

Ei Professi stellt einen Klausur mit 20 Richtig-Falsch-Fragen. Wie gro√ü ist die Wahrscheinlichkeit, durch blo√ües M√ºnze werfen genau 15 Fragen richtig zu raten?[^globusversuch-4]

[^globusversuch-4]: Hey, endlich mal was f√ºr echte Leben!

```{r QM2-Thema2-kleineModelle-23, echo = TRUE}
dbinom(x = 15, size = 20, prob = .5)
```

Was ist die Wahrscheinlichkeit bei 3 M√ºnzw√ºrfen (genau) 3 Treffer (Kopf) zu erzielen?

```{r QM2-Thema2-kleineModelle-24, echo = TRUE}
dbinom(3, 3, 1/2)
```

### Unser Modell ist geboren

Wir fassen das Globusmodell so zusammen:

$$W \sim \text{Bin}(N,p),$$

Lies: "W ist *bin*omial verteilt mit den Parametern $N$ und $p$". $N$ gibt die Anzahl der Globusw√ºrfe an: $N=W+L$.

Unser Vorab-Wissen zu $p$ sei, dass uns alle Werte gleich plausibel erscheinen ("uniform"):

$$p \sim \text{Unif}(0,1).$$

Lies: "$p$ ist gleich (uniform) verteilt mit der Untergrenze 0 und der Obergrenze 1".

### So sehen die Verteilungen aus

Abb. @fig-bin zeigt die Binomialverteilung.

```{r QM2-Thema2-kleineModelle-25}
#| echo: false
#| label: fig-bin
#| fig-cap: Ein Beispiel f√ºr eine Binomialverteilung
dbinom(0:9, 9, 1/2) %>% 
  tibble(Wahrscheinlichkeit = .,
         Treffer = seq(0,9)) %>% 
  ggplot(aes(x = Treffer, y = Wahrscheinlichkeit)) +
  geom_segment(aes(xend = Treffer, yend = 0)) + 
  geom_point(color = "red", size = 5, alpha = .5) +
  scale_x_continuous(breaks = 0:9)

```

$N=9, p = 1/2$

Abb. @fig-unif zeigt ein Beispiel f√ºr eine Gleichverteilung (uniform distribution).

```{r QM2-Thema2-kleineModelle-26}
#| echo: false
#| fig-cap: Gleichverteilung
#| label: fig-unif



uniform_Plot <- function(a, b){
  xvals <- data.frame(x = c(a, b)) #Range for x-values
  
  ggplot(data.frame(x = xvals), 
         aes(x = x)) + xlim(c(a, b)) + ylim(0, 1/(b - a)) +
    stat_function(fun = dunif, args = list(min = a, max = b), 
                  geom = "area", 
                  fill = "green", alpha = 0.35) + 
    stat_function(fun = dunif, args = list(min = a, max = b)) +
    labs(x = "X", y = "Dichte")  +
    geom_vline(xintercept = a, linetype = "dashed", colour = "red") +
    geom_vline(xintercept = b, linetype = "dashed", colour = "red") 
  
}
uniform_Plot(0, 1)
```

$Min = 0, Max = 1$

üèãÔ∏èÔ∏è Was f√§llt Ihnen bei der Binomialverteilung auf? Ist sie symmetrisch? Ver√§ndert sich die Wahrscheinlichkeit linear? Was f√§llt Ihnen bei der Gleichverteilung auf?

## Zur Erinnerung: Bayes Theorem

### Herleitung Bayes' Theorem 1/2: Gemeinsame Wahrscheinlichkeit

Die Wahrscheinlichkeit f√ºr *Regen* und *kalt* ist gleich der Wahrscheinlihckeit von *Regen*, *gegeben kalt* mal der Wahrscheinlicht von *kalt*. Entsprechend gilt: Die Wahrscheinlichkeit von $W$, $L$ und $p$ ist das Produkt von $Pr(W,L|p)$ und der Prior-Wahrscheinlichkeit $Pr(p)$:

$$Pr(W,L,p) = Pr(W,L|p) \cdot Pr(p)$$

Genauso gilt: Die Wahrscheinlichkeit von *Regen* und *kalt* ist gleich der Wahrscheinlichkeit *kalt, wenn's regnet* mal der Wahrscheinlichkeit von *Regen*:

$$Pr(W,L,p) = Pr(p|W,L) \cdot Pr(W, L)$$

### Herleitung Bayes' Theorem 2/2: Posteriori-Wahrscheinlichkeit

Wir setzen die letzten beiden Gleichungen gleich:

$$Pr(W,L|p) \cdot Pr(p) = Pr(p|W,L) \cdot (W,L)$$

Und l√∂sen auf nach der Posteriori-Wahrscheinlichkeit, $Pr(p|W,L)$:

$$Pr(p|W,L) = \frac{Pr(W,L|p) Pr(p)}{Pr(W,L)}$$

$Pr(W,L)$ nennt man die *mittlere Wahrscheinlichkeit der Daten* oder *Evidenz*. Die Evidenz berechnet sich als Mittelwert der Likelihoods √ºber alle Werte von $p$. Die Aufgabe dieser Gr√∂√üe ist nur daf√ºr zu sorgen, dass insgesamt Werte zwischen 0 und 1 herauskommen.

### Bayes' Theorem als Formel

$$Pr(H|D) = \frac{Pr(D|H) Pr(H)}{Pr(D)}$$

-   Bestandteile:

    -   Posteriori-Wahrscheinlichkeit: $Pr_{Post} := Pr(H|D)$

    -   Likelihood: $L := Pr(D|H)$

    -   Priori-Wahrscheinlichkeit: $Pr_{Priori} := Pr(H)$

    -   Evidenz: $E := Pr(D)$

-   Bayes' Theorem gibt die $Pr_{Post}$ an, wenn man die Gleichung mit der $Pr_{Priori}$ und dem $L$ f√ºttert.

-   Bayes' Theorem wird h√§ufig verwendet, um die $Pr_{Post}$ zu quantifizieren.

-   Die $Pr_{Post}$ ist proportional zu $L \times Pr_{Priori}$.

### Posteriori als Produkt von Priori und Likelihood

$$\text{Posteriori} = \frac{\text{Likelihood} \times \text{Priori}}{\text{Evidenz}}$$

```{r QM2-Thema2-kleineModelle-27}
source("R-Code/img241.R")
```

## Bayes berechnen mit mit der Gitter-Methode

Die Methode *Gitter-Ann√§herung* nennt man auch Grid Approximation\*.

### Idee

1.  Teile den Wertebereich des Parameter in ein "Gitter" auf, z.B. $0.1, 0.2, ..., 0.9, 1$ ("Gitterwerte").
2.  Bestimme den Priori-Wert des Parameters f√ºr jeden Gitterwert.
3.  Berechne den Likelihood f√ºr Gitterwert.
4.  Berechne den unstandardisierten Posteriori-Wert f√ºr jeden Gitterwert (Produkt von Priori und Likelihood).
5.  Standardisiere den Posteriori-Wert durch teilen anhand der Summe alle unstand. Posteriori-Werte.

### Gitterwerte in R berechnen

```{r QM2-Thema2-kleineModelle-28, echo = TRUE}
d <-
  tibble(
    # definiere das Gitter: 
    p_Gitter = seq(from = 0, to = 1, length.out = 10),
    # bestimme den Priori-Wert:       
    Priori  = 1) %>%  
    mutate(
      # berechne Likelihood f√ºr jeden Gitterwert:
      Likelihood = dbinom(6, size = 9, prob = p_Gitter),
      # berechen unstand. Posteriori-Werte:
      unstd_Post = Likelihood * Priori,
      # berechne stand. Posteriori-Werte (summiert zu 1):
      Post = unstd_Post / sum(unstd_Post))  
```

So sehen unsere "Gitterdaten" aus:

```{r QM2-Thema2-kleineModelle-29}
d %>% 
  kbl(digits = 2)
```

üèãÔ∏è Was wohl mit *Post* passiert, wenn wir *Priori* √§ndern?

### Was sagt die Post?

Die Posteriori-Verteilung (Kurz: "Post-Verteilung"), $Pr_{Post}$, zeigt, wie plausibel wir jeden Wert von $p$ halten.

```{r QM2-Thema2-kleineModelle-30}
source("R-Code/img242.R")
```

Mehr Gitterwerte gl√§tten die Ann√§herung.

Je gr√∂√üer die Stichprobe ($N$), desto zuverl√§ssiger wird unsere Berechnung.

```{r QM2-Thema2-kleineModelle-32, out.width="100%"}
source("R-Code/img243.R")
```

### Zusammenfassung

-   In unserem Modell haben wir Annahmen zu $Pr_{Priori}$ und $L$ getroffen.

-   Auf dieser Basis hat der Golem sein Wissen geupdated zu $Pr_{Post}$.

-   Mit der Gitter-Methode haben wir viele Hypothesen (Parameterwerte) untersucht und jeweils die $Pr_{Post}$ berechnet.

-   Unser Modell bildet die kleine Welt ab; ob es in der gro√üen Welt n√ºtzlich ist, steht auf einem anderen Blatt.

\includegraphics[width=1em]{../img/weight.pdf} Wenn Sie auf einen Prozentwert f√ºr $W$ tippen m√ºssten, welchen w√ºrden Sie nehmen, laut dem Modell (und gegeben der Daten)?
