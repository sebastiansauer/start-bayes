# Globusversuch





![Bayes:Start](img/Golem_hex.png){width=5%}




## Lernsteuerung


### Lernziele

Nach Absolvieren des jeweiligen Kapitel sollen folgende Lernziele erreicht sein.

Sie k√∂nnen ...


- Unterschiede zwischen Modellen und der Realit√§t erl√§utern
- die Binomialverteilung heranziehen, um geeignete (einfache) Modelle zu erstellen
- die weite Einsetzbarkeit anhand mehrerer Beispiele exemplifizieren
- Post-Wahrscheinlichkeiten anhand der Gittermethode berechnen




```{r}
#| include: false
library(tidyverse)
library(patchwork)
library(easystats)
library(ggraph)
library(tidygraph)
theme_set(theme_modern())
```


## Von Welten und Golems

### Kleine Welt, gro√üe Welt

Bekanntlich segelte Kolumbus 1492 los, und entdeckte Amerika. Das war aber ein gl√ºcklicher Zufall, denn auf seinem Globus existierte Amerika gar nicht. Vielleicht sah sein Globus so aus wie der von Behaim, s. Abb @fig-behaim.

![Behaims Globus: Kein Amerika](img/Behaim.jpg){#fig-behaim}

[Quelle: Ernst Ravenstein, Wikimedia, Public Domain](https://commons.wikimedia.org/wiki/File:RavensteinBehaim.jpg)

Die *kleine Welt des Modells* entsprach hier nicht *der gro√üen Welt, der echten Erdkugel*.

Das ist ein Beispiel, das zeigt, wie Modellieren schiefgehen kann. Es ist aber auch ein Beispiel f√ºr, sagen wir, die Komplexit√§t wissenschaftlicher (und sonstiger) Erkenntnis. Einfach gesagt: Gl√ºck geh√∂rt halt auch dazu.
::: callout-note
Behaims Globus ist nicht gleich der Erde. Die kleine Welt ist nicht die gro√üe Welt.
:::

Was in der kleinen Welt funktioniert, muss nicht in der gro√üen Welt funktionieren. Modelle zeigen immer nur die kleine Welt: Vorsicht vor schnellen Schl√ºssen und vermeintlicher Gewissheit.

üèã Nennen Sie ein Beispiel, in dem ein Modell nicht (exakt) der Wirklichkeit entspricht!






### Der Golem von Prag

![Der Golem von Prag](img/170px-Golem_and_Loew.jpg)

[Quelle](https://de.wikipedia.org/wiki/Golem)

Der Golem von Prag, eine vom Menschen geschaffene Kreatur gewaltiger Kraft, die Befehle w√∂rtlich ausf√ºhrt.
Bei kluger F√ºhrung kann ein Golem N√ºtzliches vollbringen.
Bei un√ºberlegter Verwendung wird er jedoch gro√üen Schaden anrichten.

### Wissenschaftliche Modelle sind wie Golems

**Golem**

!["Yeah, ich bin ein Golem!" - Bildquelle: Klara Schaumann](img/Golem_hex.png){width=25%}


Eigenschaften des Golems:

-   Besteht aus Lehm
-   Belebt durch "Wahrheit"
-   M√§chtig
-   dumm
-   F√ºhrt Befehle w√∂rtlich aus
-   Missbrauch leicht m√∂glich
-   M√§rchen

**Modell**

Eigenschaften des Modells:

```{mermaid}
flowchart LR
X --> Y
```

-   Besteht aus ~~Lehm~~Silikon
-   Belebt durch Wahrheit (?)
-   Manchmal m√§chtig
-   simpler als die Realit√§t
-   F√ºhrt Befehle w√∂rtlich aus
-   Missbrauch leicht m√∂glich
-   Nicht einmal falsch

::: callout-important
Wir bauen Golems.
:::


Vergleichen wir die kleine Welt unserer Modellen, wie Behaims Globus, mit der Gro√üen Welt, die Kolumbus und wir befahren.




| Kleine Welt                                                | Gro√üe Welt                                 |
|-----------------------------------------|-------------------------------|
| Die Welt, wie sie der Golem sieht                          | Die Welt, wie sie in Wirklichkeit ist      |
| ist das Modell, aber nicht (zwangsl√§ufig) die Wirklichkeit | entspricht nicht (zwangsl√§ufig) dem Modell |
| Verwenden wir beim Modellieren                             | Ist das, was wir modellieren               |

: Kleine Welt vs. gro√üe Welt




### So denkt unser Bayes-Golem

![So denkt unser Bayes-Golem](img/bayesupdate2.png)

üèã Bayes-Inferenz √§hnelt dem Lernen von Menschen. Geben Sie ein Beispiel von Lernen bei Menschen, das oben dargestelltem Prozess √§hnelt!

## Ein erster Versuch: Wir werfen den Globus





### Welcher Anteil der Erdoberfl√§che ist mit Wasser bedeckt?

Unsere Hypothese bzw. unsere Forschungsfrage lautet, mit welchem Anteil die Erde wohl mit Wasser bedeckt ist?

![Der Erdball](img/earth.png){width="50%"}

[Quelle](https://pngimg.com/image/25340) CC 4.0 BY-NC

Sie werden einen Globus-Ball in die Luft und fangen in wieder auf. Sie notieren dann, ob die Stelle unter Ihrem Zeigefinger Wasser zeigt (W) oder Land (L). Den Versuch wiederholen Sie 9 Mal.

So sah mein Ergebnis aus:

$$W \quad L \quad W \quad W \quad W \quad L \quad W \quad L \quad W$$

üèãÔ∏èÔ∏è Besorgen Sie sich einen Globus (zur Not eine M√ºnze) und stellen Sie den Versuch nach!

### Wie entstanden die Daten?

Der physikalische Prozess, der zur Entstehung der Daten f√ºhrt, nennt man den *den datengenierende Prozess*.

In diesem Fall kann man ihn so beschreiben:

1.  Der wahre Anteil von Wasser, $W$, der Erdoberfl√§che ist $p$ (und $1-p$ ist der Anteil Land, $L$).
2.  Ein Wurf des Globusballes hat die Wahrscheinlichkeit $p$, eine $W$-Beobachtung zu erzeugen.
3.  Die W√ºrfe des Globusballes sind unabh√§ngig voneinander.
4.  Wir haben kein Vorwissen √ºber $p$; jeder Wert ist uns gleich wahrscheinlich.

üèã Welche Annahmen w√ºrden Sie √§ndern? Welche k√∂nnte man wegnehmen? Welche hinzuf√ºgen? Was w√§ren die Konsequenzen?






### Ein paar Fachbegriffe

-   F√ºr jede Hypothese haben wir ein Vorab-Wissen, das die jeweilige Plausibilit√§t der Hypothese angibt: *Priori-Verteilung*.

-   F√ºr jede Hypothese (d.h. jeden *Parameterwert* $p$) m√∂chten wir wie wahrscheinlich die Daten sind (unter der Annahme, dass die Hypothese richtig ist). Das gibt uns den *Likelihood*.

-   Dann gewichten wir den Likelihood mit dem Vorabwissen, so dass wir die *Posteriori-Verteilung*^[ Anstatt von *Priori* liest man auch *Prior*; anstatt *Posteriori* auch *Posterior*] bekommen.

![Updating mit Bayes](img/bayesupdate.png)

### Die Binomialverteilung

Wir nehmen an, dass die Daten unabh√§ngig voneinander entstehen und sich der Parameterwert nicht zwischenzeitlich √§ndert^[Die sog. "iid-Annahme", *i*ndependently and *i*dentically distributed: Jeder Wurf der Globusballes ist eine Realisation der gleichen Zufallsvariablen. Jeder Wurf ist unabh√§ngig von allen anderen: Das Ergebnis eines Wurfes hat keinen (stochastischen) Einfluss auf ein Ergebnis anderer W√ºrfe. Die Wahrscheinlichkeitsverteilung ist bei jedem Wurf identisch.].

Lassen Sie uns im Folgenden die Wahrscheinlichkeit ($Pr$), $W$ mal Wasser und $L$ mal Land zu beobachten, wenn die Wahrscheinlichkeit f√ºr Wasser $p$ betr√§gt, so bezeichnen: $(Pr(W,L | p))$.
Diese Wahrscheinlichkeit - $(Pr(W,L | p))$ - kann man mit der *Binomialverteilung* berechnen.

M√∂chte man die Wahrscheinlichkeit ansprechen f√ºr das Ereignis "5 mal Wasser und 2 mal Land, wenn wir von einem Wasseranteil von 70% ausgehen", so w√ºrden wir kurz schreiben: $Pr(W=5, L=2 | p=.7)$.

Die Binomialverteilung zeigt die Verteilung der H√§ufigkeit (Wahrscheinlichkeit) der Ereignisse (z.B. 2 Mal Kopf) beim wiederholten M√ºnzwurf (und allen vergleichbaren Zufallsexperimenten): "M√ºnzwurfverteilung"

$$Pr(W,L|p) = \frac{(W+L)!}{W!L!}p^W(1-p)^L$${#eq-binomial}


Formel @eq-binomial kann wie folgt auf Deutsch √ºbersetzen:

>   Die Wahrscheinlichkeit f√ºr das Ereignis "W,L" gegeben p berechnet als Produkt von zwei Termen. Erstens der Quotient von der Fakult√§t von W plus L im Z√§hler und im Nenner das Produkt von erstens der Fakult√§t von W mit zweitens der Fakult√§t von L. Der zweite Term ist das Produkt von p hoch W mal der komplement√§ren Wahrscheinlichkeit von p hoch L.


Puh, Formeln sind vielleicht doch ganz praktisch. 
Noch praktischer ist es aber, dass es Rechenmaschinen gibt, die die Formel kennen und f√ºr uns ausrechnen. 
Los, R, mach mal.

### Binomialverteilung mit R


Was ist der Anteil der g√ºltigen Pfade in einem Baumdiagramm (Wahrscheinlichkeit), um 2 mal $W$ bei $N=W+L=3$ W√ºrfen zu bekommen, wenn wir von $p=1/2$ ausgehen?^[Allgemeiner spricht man auch von 2 Treffern bei 3 W√ºrfen (d.h. 1 "Nicht-Treffer", den wir als "Niete" bezeichnen). Treffer werden oft mit `1`  und Niten mit `0` bezeichnet].


```{r QM2-Thema2-kleineModelle-21a, echo = TRUE}
dbinom(x = 2, size = 3, prob = 1/2)
```


Von den 8 Endkonten bzw. Pfaden sind 3 g√ºnstig. 
Demnach ist die Wahrscheinlichkeit des gesuchten Ereignis (2 Treffer bei 3 W√ºrfen, binomialverteilt) gleich 3 von 8.





```{mermaid}
%%| echo: false
%%| ref-label: fig-binom1
%%| fig-cap: Wir werfen den Globus (oder eine M√ºnze) 3 Mal
flowchart TD
  A[A - Start] -. 1/2 .-> B[B - 0]
  A -. 1/2 .-> C[C - 1]
  B -. 1/2 .-> D[D - 0]
  B -. 1/2 .-> E[E - 1]
  C -. 1/2 .-> F[F - 0]
  C -. 1/2 .-> G[G - 1]
  D -. 1/2 .-> H[H - 0]
  D -. 1/2 .-> J[I - 1]
  E -. 1/2 .-> K[K - 0]
  E -. 1/2 .-> L[L - 1]
  F -. 1/2 .-> M[M - 0]
  F -. 1/2 .-> N[N - 1]
  G -. 1/2 .-> O[O - 0]
  G -. 1/2 .-> P[P - 1]
```


Abb. @fig-binom1 stellt einen einfachen Baum f√ºr 3 Globusw√ºrfe mit je zwei m√∂glichen Ereignissen (W vs. L) dar.
In der ersten (obersten) Zeile (Knoten A; "Start") ist Ausgangspunkt dargestellt: Der Globus ruht wurfbereit in unserer Hand.
Jezt Achtung: Sie werfen den Globusball hoch.
Die Pfeile zeigen zu den (zwei) m√∂gliche Ergebnissen.
Die zweite Zeile (Knoten B und C) stellt die beiden Ergebnisse des Wurfes dar. 
Die Ergebnisse sind hier mit `0` und `1` bezeichnet (das eine eine einfache und weiteinsetzbare Notation).
Die dritte Zeile (Knoten D bis G) stellt die Ergebnisse des des zweiten Wurfes dar.
Die vierte Zeile (Knoten H bis P)  stellt die Ergebnisse des des dritten Wurfes dar.


F√ºr mehr W√ºrfe w√ºrde das Diagramm irgendwann un√ºbersichtlich werden.



Was ist der Anteil der g√ºltigen Pfade in einem Baumdiagramm (Wahrscheinlichkeit), um 6 mal $W$ bei $N=W+L=9$ W√ºrfen zu bekommen, wenn wir von $p=1/2$ ausgehen?

```{r QM2-Thema2-kleineModelle-21, echo = TRUE}
dbinom(x = 6, size = 9, prob = 1/2)
```


Abb @fig-binom2 ist ein vergeblicher Versuch, so einen gro√üen Baum darzustellen.

:::callout-note
Visualisierungen wie Baumdiagramme sind eine praktische Hilfe zum Verst√§ndnis,
kommen aber bei gr√∂√üeren Daten schnell an ihre Grenze.
:::


```{r}
#| echo: false
#| ref-label: fig-binom2
#| fig-cap: Wir werfen den Globus (oder eine M√ºnze) 3 Mal
my_tree <- tidygraph::create_tree(511, 2, mode = "out")

my_tree %>%
  mutate(lab = 1:511) %>% 
  ggraph(circular = TRUE) +
  geom_edge_link() +
  geom_node_label(mapping = aes(label = lab)) +
  coord_flip() +
  scale_y_reverse() +
  theme_void()
```


Geb

Was ist die Wahrscheinlichkeit f√ºr $W=9$ bei $N=9$ und $p=1/2$?

```{r QM2-Thema2-kleineModelle-22, echo = TRUE}
dbinom(x = 9, size = 9, prob = 1/2)
```

### Beispiele zur Berechnung einer binomial verteilten Wahrscheinlichkeit

Ei Professi stellt einen Klausur mit 20 Richtig-Falsch-Fragen. Wie gro√ü ist die Wahrscheinlichkeit, durch blo√ües M√ºnze werfen genau 15 Fragen richtig zu raten?^[Hey, endlich mal was f√ºr echte Leben!].

```{r QM2-Thema2-kleineModelle-23, echo = TRUE}
dbinom(x = 15, size = 20, prob = .5)
```

Was ist die Wahrscheinlichkeit bei 3 M√ºnzw√ºrfen (genau) 3 Treffer (Kopf) zu erzielen?

```{r QM2-Thema2-kleineModelle-24, echo = TRUE}
dbinom(3, 3, 1/2)
```

### Unser Modell ist geboren

Wir fassen das Globusmodell so zusammen:

$$W \sim \text{Bin}(N,p),$$

Lies: "W ist *bin*omial verteilt mit den Parametern $N$ und $p$". $N$ gibt die Anzahl der Globusw√ºrfe an: $N=W+L$.

Unser Vorab-Wissen zu $p$ sei, dass uns alle Werte gleich plausibel erscheinen ("uniform"):

$$p \sim \text{Unif}(0,1).$$

Lies: "$p$ ist gleich (uniform) verteilt mit der Untergrenze 0 und der Obergrenze 1".










### So sehen die Verteilungen aus

Abb. @fig-bin zeigt die Binomialverteilung.

```{r QM2-Thema2-kleineModelle-25}
#| echo: false
#| label: fig-bin
#| fig-cap: Ein Beispiel f√ºr eine Binomialverteilung
dbinom(0:9, 9, 1/2) %>% 
  tibble(Wahrscheinlichkeit = .,
         Treffer = seq(0,9)) %>% 
  ggplot(aes(x = Treffer, y = Wahrscheinlichkeit)) +
  geom_segment(aes(xend = Treffer, yend = 0)) + 
  geom_point(color = "red", size = 5, alpha = .5) +
  scale_x_continuous(breaks = 0:9)

```

$N=9, p = 1/2$

Abb. @fig-unif zeigt ein Beispiel f√ºr eine Gleichverteilung (uniform distribution).

```{r QM2-Thema2-kleineModelle-26}
#| echo: false
#| fig-cap: Gleichverteilung
#| label: fig-unif



uniform_Plot <- function(a, b){
  xvals <- data.frame(x = c(a, b)) #Range for x-values
  
  ggplot(data.frame(x = xvals), 
         aes(x = x)) + xlim(c(a, b)) + ylim(0, 1/(b - a)) +
    stat_function(fun = dunif, args = list(min = a, max = b), 
                  geom = "area", 
                  fill = "green", alpha = 0.35) + 
    stat_function(fun = dunif, args = list(min = a, max = b)) +
    labs(x = "X", y = "Dichte")  +
    geom_vline(xintercept = a, linetype = "dashed", colour = "red") +
    geom_vline(xintercept = b, linetype = "dashed", colour = "red") 
  
}
uniform_Plot(0, 1)
```

$Min = 0, Max = 1$

üèãÔ∏èÔ∏è Was f√§llt Ihnen bei der Binomialverteilung auf? Ist sie symmetrisch? Ver√§ndert sich die Wahrscheinlichkeit linear? Was f√§llt Ihnen bei der Gleichverteilung auf?

## Zur Erinnerung: Bayes Theorem

### Herleitung Bayes' Theorem 1/2: Gemeinsame Wahrscheinlichkeit

Die Wahrscheinlichkeit f√ºr *Regen* und *kalt* ist gleich der Wahrscheinlihckeit von *Regen*, *gegeben kalt* mal der Wahrscheinlicht von *kalt*. Entsprechend gilt: Die Wahrscheinlichkeit von $W$, $L$ und $p$ ist das Produkt von $Pr(W,L|p)$ und der Prior-Wahrscheinlichkeit $Pr(p)$:

$$Pr(W,L,p) = Pr(W,L|p) \cdot Pr(p)$$

Genauso gilt: Die Wahrscheinlichkeit von *Regen* und *kalt* ist gleich der Wahrscheinlichkeit *kalt, wenn's regnet* mal der Wahrscheinlichkeit von *Regen*:

$$Pr(W,L,p) = Pr(p|W,L) \cdot Pr(W, L)$$

### Herleitung Bayes' Theorem 2/2: Posteriori-Wahrscheinlichkeit

Wir setzen die letzten beiden Gleichungen gleich:

$$Pr(W,L|p) \cdot Pr(p) = Pr(p|W,L) \cdot (W,L)$$

Und l√∂sen auf nach der Posteriori-Wahrscheinlichkeit^[k√ºrzen wir mit Post-Wahrscheinlichkeit or $Pr(Post)$ ab], $Pr(p|W,L)$:

$$Pr(p|W,L) = \frac{Pr(W,L|p) Pr(p)}{Pr(W,L)}$$

$Pr(W,L)$ nennt man die *mittlere Wahrscheinlichkeit der Daten* oder *Evidenz*. Die Evidenz berechnet sich als Mittelwert der Likelihoods √ºber alle Werte von $p$. Die Aufgabe dieser Gr√∂√üe ist nur daf√ºr zu sorgen, dass insgesamt Werte zwischen 0 und 1 herauskommen.

### Bayes' Theorem als Formel

$$Pr(H|D) = \frac{Pr(D|H) Pr(H)}{Pr(D)}$$

-   Bestandteile:

    -   Posteriori-Wahrscheinlichkeit: $Pr_{Post} := Pr(H|D)$

    -   Likelihood: $L := Pr(D|H)$

    -   Priori-Wahrscheinlichkeit: $Pr_{Priori} := Pr(H)$

    -   Evidenz: $E := Pr(D)$

-   Bayes' Theorem gibt die $Pr_{Post}$ an, wenn man die Gleichung mit der $Pr_{Priori}$ und dem $L$ f√ºttert.

-   Bayes' Theorem wird h√§ufig verwendet, um die $Pr_{Post}$ zu quantifizieren.

-   Die $Pr_{Post}$ ist proportional zu $L \times Pr_{Priori}$.

### Posteriori als Produkt von Priori und Likelihood

Die unstandardisierte Post-Wahrscheinlichkeit ist einfach das Produkt von Likelihood und Priori.

Das Standardisieren dient nur dazu, einen Wert zwischen 0 und 1 zu erhalten. 
Dies erreichen wir, indem wir durch die Summe aller Post-Wahrscheinlichkeiten dividieren.
Die Summe der Post-Wahrscheinlichkeiten bezeichnet man (auch) als Evidenz, vgl. Gleichung @eq-post.


$$\text{Posteriori} = \frac{\text{Likelihood} \times \text{Priori}}{\text{Evidenz}}$${#eq-post}



Abb. @fig-post3 visualisiert, dass die Post-Verteilung eine Gewichtung von Priori und Likelihood ist.
Mathematisch gesprochen beruht diese Gewichtung auf einer einfachen Multiplikationen der beiden genannten Terme.



![Prior mal Likelihood = Post](img/img241.png){#fig-post3}







### Wissen updaten: Wir f√ºttern Daten in das Modell

Golemns k√∂nnen lernen?! Abb. @fig-lernen-golem zeigt die Post-Verteilung, nach $n=1, 2, ...,n=9$ Datenpunkten, d.h. W√ºrfen mit dem Globusball.
Man sieht: Am Anfang, apriori, also bevor die Daten haben, vor dem ersten Wurf also, ist jeder Parameterwert gleich wahrscheinlich f√ºr den Golem (das Modell).
Je nach Ergebnis des Wurfes ver√§ndert sich die Wahrscheinlichkeit der Parameterwerte,
kurz gesagt, die Post-Verteilung ver√§ndert sich in Abh√§ngigkeit von den Daten.


![Unser Golem lernt](img/img221.png){#fig-lernen-golem}


Insofern kann man sagen: Unser Golem (das Modell) lernt. Ob das Modell n√ºtzlich ist (pr√§zise Vorhersagen liefert), steht auf einem anderen Blatt.



## Bayes berechnen mit mit der Gitter-Methode

Die Methode *Gitter-Ann√§herung* nennt man auch Grid Approximation\*.

### Idee

1.  Teile den Wertebereich des Parameter in ein "Gitter" auf, z.B. $0.1, 0.2, ..., 0.9, 1$ ("Gitterwerte").
2.  Bestimme den Priori-Wert des Parameters f√ºr jeden Gitterwert.
3.  Berechne den Likelihood f√ºr Gitterwert.
4.  Berechne den unstandardisierten Posteriori-Wert f√ºr jeden Gitterwert (Produkt von Priori und Likelihood).
5.  Standardisiere den Posteriori-Wert durch teilen anhand der Summe alle unstand. Posteriori-Werte.


F√ºr jeden "Gitterwert" berechnen wir eine (Post-)Wahrscheinlichkeit. Ein Gitterwert ist eine m√∂gliche Auspr√§gung des Parameters. H√§ufig entspricht eine Hypothese einem Gitterwert, etwa wenn man sagt: "Ich glaube, die M√ºnze ist fair", was auf einem Parameterwert von 50% herausl√§uft.
Dazu geben wir an, f√ºr wie wahrscheinlich wie apriori^[synonym: priori] - also bevor wir irgendwelche Daten erheben - jeden einzelnen Gitterwert halten. Wir machen es uns hier einfach und halten jeden Gitterwert f√ºr gleich wahrscheinlich. Tats√§chlich ist der konkrete Wert hier egal, entscheidend ist das Verh√§ltnis der Apriori-Werte zueinander: Geben wir einigen Gitterwerten den Wert 2, aber anderen den Wert 1, so halten wir Erstere f√ºr (apriori) doppelt so plauibel wie Letztere.
Der Likelihood wird in diesem Fall mit der Binomialverteilung berechnet. Der Likelihood gibt an, wie wahrscheinlich ein Gitterwert ist gegeben einem bestimmten apriori gew√§hlten Parameterwert.
Die "End-Wahrscheinlichkeit" die "hinten rauskommt" ist einfach das Produkt von Priori-Wert und Likelihood.
Anschaulich gesprochen: Die Priori-Werte werden mit den Likelihoodwerten gewichtet^[synonym: Die Likelihoodwerte werden mit den Apriori-Werten gewichtet.].
Da wir letztlich eine Wahrscheinlichkeitverteilung bekommen m√∂chten teilen wir jeden Posteriori-Wert durch die Summe aller Posteriori-Werte. Dadurch ist gerantiert, dass sich die Posteriori-Werte zu eins aufaddieren. Damit haben wir dann die Kolmogorov-Anspr√ºche an eine Wahrscheinlichkeitsverteilung erf√ºllt.


### Gitterwerte in R berechnen

Legen wir uns eine Tabelle mit Gittewerten an, um deren Posteori-Wahrscheinlichkeit zu berechnen.

```{r QM2-Thema2-kleineModelle-28, echo = TRUE}
d <-
  tibble(
    # definiere das Gitter: 
    p_Gitter = seq(from = 0, to = 1, length.out = 10),
    # bestimme den Priori-Wert:       
    Priori  = 1) %>%  
    mutate(
      # berechne Likelihood f√ºr jeden Gitterwert:
      Likelihood = dbinom(6, size = 9, prob = p_Gitter),
      # berechen unstand. Posteriori-Werte:
      unstd_Post = Likelihood * Priori,
      # berechne stand. Posteriori-Werte (summiert zu 1):
      Post = unstd_Post / sum(unstd_Post))  
```

So sehen unsere "Gitterdaten" aus:

```{r QM2-Thema2-kleineModelle-29}
#| echo: false
d %>% 
  knitr::kable(digits = 2)
```

üèãÔ∏è Was wohl mit *Post* passiert, wenn wir *Priori* √§ndern?

### Was sagt die Post?

Die Posteriori-Verteilung (Kurz: "Post-Verteilung"), $Pr_{Post}$, zeigt, wie plausibel wir jeden Wert von $p$ halten.


Abb. @fig-gitter zeigt die Post-Wahrscheinlichkeit f√ºr 5, 10 und 20 Gitterwerte. Das mittlere Teilbild (10 Gitterwerte) entspricht unserer Tabelle oben.


![Je mehr Gittewerte, desto genauer wird die Verteilung wiedergegeben.](img/img242.png){#fig-gitter}


:::callout-note
Unter sonst gleichen Umst√§nden gilt:

- Mehr Gitterwerte gl√§tten die Ann√§herung.
- Je gr√∂√üer die Stichprobe ($N$), desto zuverl√§ssiger wird unsere Berechnung.
:::

::: callout-important
Die Post-Verteilung ist sowas wie das Ziel all Ihrer Tr√§ume (falls Sie es noch nicht gewusst haben):
Aus der Post-Verteilung k√∂nnen Sie ablesen,
wie wahrscheinlich Ihre Hypothese (Ihr Lieblings-Parameterwert) ist. Und noch einiges mehr, aber das ist Thema des n√§chsten Kapitels.
:::




## Abschluss

### Zusammenfassung

-   In unserem Modell haben wir Annahmen zu $Pr_{Priori}$ und $L$ getroffen.

-   Auf dieser Basis hat der Golem sein Wissen geupdated zu $Pr_{Post}$.

-   Mit der Gitter-Methode haben wir viele Hypothesen (Parameterwerte) untersucht und jeweils die $Pr_{Post}$ berechnet.

-   Unser Modell bildet die kleine Welt ab; ob es in der gro√üen Welt n√ºtzlich ist, steht auf einem anderen Blatt.

üèãÔ∏è Wenn Sie auf einen Prozentwert f√ºr $W$ tippen m√ºssten, welchen w√ºrden Sie nehmen, laut dem Modell (und gegeben der Daten)?


### Vertiefung


Das ["Bayes-Paradox-Video" von 3b1b](https://youtu.be/lG4VkPoG3ko) pr√§sentiert eine gut verst√§ndliche Darstellung des Bayes-Theorem aus einer zwar nicht gleichen, aber √§hnlichen Darstellung wie in diesem Kapitel.




### Literatur


@bourier_2018, Kap. 6.2 und 7.1 erl√§utern einige (grundlegende) theoretische Hintergr√ºnde zu diskreten Zufallsvariablen und Wahrscheinlichkeitsverteilungen. Wichtigstes Exemplar ist dabei die Binomialverteilung.
@mcelreath_statistical_2020, Kap. 2, stellt das Globusmodell mit mehr Erl√§uterung und etwas mehr theoretischem Hintergrund vor.




