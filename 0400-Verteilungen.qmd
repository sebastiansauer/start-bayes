# Verteilungen




## Lernsteuerung


### Position im Modulverlauf

@fig-modulverlauf gibt einen √úberblick zum aktuellen Standort im Modulverlauf.


### Lernziele

Nach Absolvieren des jeweiligen Kapitels sollen folgende Lernziele erreicht sein.

Sie k√∂nnen ...


- den Begriff der Zufallsvariablen erl√§utern
- die Begriffe von Wahrscheinlichkeitsdichte und Verteilungsfunktion erl√§utern 
- den Begriff einer Gleichverteilung erl√§utern 
- die Parameter einer Normalverteilung nennen und erl√§utern
- zentrale Konzepte in R umsetzen



### Vorbereitung im Eigenstudium



Lesen Sie selbst√§ndig, zus√§tzlich  zum Stoff dieses Kapitels noch @bourier_2018; dort folgende Abschnitte:

- Kap. 6.1 (Zum Begriff Zufallsvariable)
- Kap. 6.3 (Stetige Zufallsvariablen)
- Kap. 7.1.1 (Binomialverteilung)
- Kap. 7.2.1 (Gleichverteilung)
- Kap. 7.2.3 (Normalverteilung)

L√∂sen Sie auch die √úbungsaufgaben dazu.


Weitere √úbungsaufgaben finden Sie im dazugeh√∂rigen √úbungsbuch, @bourier_statistik-ubungen_2022.


### Pr√ºfungsrelevanter Stoff

Beachten Sie, dass neben den Inhalten des Kapitels auch stets der vorzubereitende Stoff pr√ºfungsrelevant ist.


### Ben√∂tigte R-Pakete

```{r}
#| message: false
library(tidyverse)
```


```{r}
#| include: false
library(gt)
library(patchwork)
library(faux)
library(openintro)
library(easystats)
library(ggraph)
library(knitr)

source("funs/uniformplot.R")
source("funs/binomial_plot.R")
```




```{r r-setup}
#| echo: false
#| message: false
theme_set(theme_minimal())
#scale_color_okabeito()
scale_colour_discrete <- function(...) 
  scale_color_okabeito()
```





### Zentrale Begriffe




#### Eigenschaften von Zufallsvariablen

- Zufallsvariable (random variable)
- Diskret vs. stetig
- Wahrscheinlichkeitsdichte (Dichte, (probability) density, f)
- Wahrscheinlichkeitsfunktion (kumulierte Wahrscheinlichkeit, Wahrscheinlichkeitsmasse)


#### Verteilungen

- Gleichverteilung
- Normalverteilung
- Standardnormalverteilung

### Begleitvideos


- [Video 1 zum Thema Verteilungen](https://youtu.be/7GqIE4sKDs4)
- [Video 2 zum Thema Verteilungen](https://youtu.be/HKWwondYsW8)



## Zufallsvariable


:::{#exm-thesis}
Schorsch sucht eine Betreuerin f√ºr seine Abschlussarbeit.
An die ideale Betreuerin setzt er 4 Kriterien an: a) klare, schriftliche fixierte Rahmenbedingungen, b) viel Erfahrung, c) guten Ruf und d) interessante Forschungsinteressen.
Je mehr dieser 4 Kriterien erf√ºllt sind, desto besser. 
Schorsch geht davon aus, dass die 4 Kriterien voneinander unabh√§ngig sind (ob eines erf√ºllt ist oder nicht, √§ndert nichts an der Wahrscheinlichkeit eines anderen Kriteriums).
Schorsch interessiert sich also f√ºr die *Anzahl* der erf√ºllten Kriterien, also eine Zahl von 0 bis 4.
Er sch√§tzt die Wahrscheinlichkeit f√ºr einen "Treffer" in jedem seiner 4 Kriterien auf 50%.
Viel Gl√ºck, Schorsch!
Sein Zufallsexperiment hat 16 Ausg√§nge, s. @fig-4muenzen und @tbl-schorsch-zufall. Ganz sch√∂n komplex.
Eigentlich w√ºrden ihm ja eine Darstellung mit 5 Ergebnissen, also der "Gutachter-Score" von 0 bis 4 ja reichen. 
Wie k√∂nnen wir es √ºbersichtlicher f√ºr Schorsch?$\square$
:::


 

```{r}
#| echo: false
#| label: fig-4muenzen
#| fig-cap: "Ein Baumdiagramm mit 16 Ausg√§ngen, analog zur 4 M√ºnzw√ºrfen. Jede M√ºnze ist in einer anderen Farbe dargestellt. Der Knoten '1' ist der Start, da ist noch keine M√ºnze geworfen."
my_tree <- tidygraph::create_tree(31, 2, mode = "out")

my_tree %>%
  mutate(lab = 1:31) %>%
  mutate(muenze = case_when(
    lab == 1 ~ 0,
    lab <= 3 ~ 1,
    lab <= 7 ~ 2,
    lab <= 15 ~ 3,
    TRUE ~ 4
  )) %>% 
  mutate(muenze = factor(muenze)) %>% 
  ggraph(circular = FALSE) +
  geom_edge_link() +
   geom_node_label(mapping = aes(label = lab, fill = muenze)) +
  coord_flip() +
  scale_y_reverse() +
  # scale_fill_manual(values = c("0" = "red", "1" = "blue", "2" = "green", "3" = "purple", "4" = "orange"),
  #                   name = "Muenze") +
  theme_void() +
  theme(text = element_text(size = 12))
```



```{r}
#| echo: false
#| tbl-cap: Schorschs Zufallsexperiment, Auszug der Elementarereignisse
#| label: tbl-schorsch-zufall
d <- tibble::tribble(
   ~i, ~Elementarereignis, ~`Pr(EE)`, ~Trefferzahl, ~`Pr(Trefferzahl)`,
  "1",             "NNNN",    "1/16",          "0",             "1/16",
  "2",             "NNNT",    "1/16",          "1",              "1/4",
  "3",             "NNTN",    "1/16",          "1",              "1/4",
  "4",             "NTNN",    "1/16",          "1",              "1/4",
  "5",             "TNNN",    "1/16",          "1",              "1/4",
  "6",             "NNTT",    "1/16",          "2",                "‚Ä¶",
  "‚Ä¶",                "‚Ä¶",       "‚Ä¶",          "‚Ä¶",                "‚Ä¶"
  )


kable(d, booktabs = TRUE)
```


Schorsch braucht also eine √ºbersichtlichere Darstellung;
die Zahl der Treffer und ihre Wahrscheinlichkeit w√ºrde ihm ganz reichen.
In vielen Situationen ist man an der *Anzahl der Treffer* interessiert.
Die Wahrscheinlichkeit f√ºr eine bestimmte Trefferanzahl bekommt man einfach durch Addieren der Wahrscheinlichkeiten der zugeh√∂rigen Elementarereignisse, s. @tbl-schorsch-zufall.
Hier kommt die *Zufallsvariable* ins Spiel.
Wir nutzen sie, um die Anzahl der Treffer in einem Zufallsexperiment zu z√§hlen.


:::{#def-zufallsvariable}
### Zufallsvariable
Die Zuordnung der Elementarereignisse eines Zufallsexperiments zu genau einer Zahl $\in \mathbb{R}$ nennt man Zufallsvariable.$\square$
:::

Die den Elementarereignissen zugewiesenen Zahlen nennt man *Realisationen* oder Auspr√§gungen der Zufallsvariablen.

:::{#exm-lotto}
### Lotto
Ein Lottospiel hat ca. 14 Millionen Elementarereignisse. Die Zufallsvariable "Anzahl der Treffer" hat nur 7 Realisationen: 0,1,...,6.$\square$
:::

Es hat sich eingeb√ºrgert, Zufallszahlen mit $X$ zu bezeichnen (oder anderen Buchstaben weit hinten aus dem Alphabet).

Man schreibt kurz: $X: \Omega \rightarrow \mathbb{R}$. Um die Vorschrift der Zuordnung genauer zu bestimmen, kann man folgende Kurzschreibweise nutzen:


${\displaystyle X(\omega )={\begin{cases}1,&{\text{wenn }}\omega ={\text{Kopf}},\\[6pt]0,&{\text{wenn }}\omega ={\text{Zahl}}.\end{cases}}}$


@fig-zv stellt diese Abbildung dar.



```{mermaid}
%%| fig-cap: Eine Zufallsvariable ist eine Abbildung vom Ereignisraum zu den Realisationen der Zufallsvariable. Au√üerdem sieht man, wie Zufallsvariablen genutzt werden, um Wahrsscheinlichkeiten zu bestimmen.
%%| label: fig-zv
flowchart LR
  subgraph A[Ereignisse<br> im Ereignisraum]
    Kopf
    Zahl
  end
  subgraph B[Realisationen der <br>Zufallsvariable]
    null[0]
    eins[1]
  end
  subgraph C[Wahrscheinlichkeit]
    half[50%]
  end
  
  Kopf --> null
  Zahl --> eins
  null --> half
  eins --> half
```



Zufallsverteilungen kann im zwei Artein einteilen:

1. diskrete Zufallsvariablen
2. stetige Zufallsvariablen



### Diskrete Zufallsvariable

#### Grundlagen

Eine diskrete Zufallsvariable ist dadurch gekennzeichnet, dass nur bestimmte Realisationen m√∂glich sind, zumeist nat√ºrliche Zahlen, wie 0, 1, 2,..., .
@fig-zuv-disk versinnbildlicht die Zufallsvariable des "Gutachter-Scores", s. @exm-thesis.


```{r}
#| echo: false
#| fig-asp: 0.2
#| label: fig-zuv-disk
#| fig-cap: Sinnbild einer diskreten Zufallsvariablen X f√ºr Schorschs Suche nach einer Betreuerin seiner Abschlussarbeit. X gibt den Score der Gutachterin wider.
ggplot(data.frame(x=c(0:4), y = 0), aes(x,y)) +
  geom_point(size = 5, alpha = .7) +
  scale_y_continuous(limits = c(-1,1), breaks = NULL) +
  scale_x_continuous(breaks = 0:4, labels = 0:4) +
  theme_minimal() +
  labs(y = "", x = "")

```



:::{#exm-zv-disk}
### Diskrete Zufallsvariablen

- Anzahl der Bewerbungen bis zum ersten Job-Interview
- Anzahl Anl√§ufe bis zum Bestehen der Statistik-Klausur
- Anzahl der Absolventen an der HS Ansbach pro Jahr
- Anzahl Treffer beim Kauf von Losen
- Anzahl Betriebsunf√§lle
- Anzahl der Produkte in der Produktpalette$\square$
:::



:::{#exm-zweiwuerfel}
Der zweifache W√ºrfelwurf ist ein typisches Lehrbuchbeispiel f√ºr eine diskrete Zufallsvariable.^[da einfach und deutlich]
Hier ist $S$^[S wie Summe] die Augen*s*umme des zweifachen W√ºrfelwurfs und $S$ ist eine Zahl zwischen 2 und 12.
F√ºr jede Realisation $X=x$ kann die Wahrscheinlichkeit berechnen, @fig-zweiwuerfel-vert versinnbildlicht die Wahrscheinlichkeit f√ºr jede Realisation von $X$.$\square$
:::

![Augensumme des zweifachen W√ºrfelwurfs; f√ºr jede Realisation von S ist die zugeh√∂rige Wahrscheinlichkeit dargestellt. Bildquelle: Tim Stellmach, Wikipedia, PD](img/Dice_Distribution_(bar).svg.png){#fig-zweiwuerfel-vert width=50%}



*Wahrscheinlichkeitsverteilungen* dienen dazu, den Realisationen einer Zufallsvariablen eine Wahrscheinlichkeit zuzuordnen.








:::{#def-wvert-disk}
### Diskrete Wahrscheinlichkeitsverteilung
Eine *diskrete* Wahrscheinlichkeitsverteilung der (diskreten) Zufallsvariablen $X$ ordnet jeder der $k$ Auspr√§gungen $X=x$ eine Wahrscheinlichkeit $p$ zu.$\square$
:::

:::{#exm-babies}
### Wahrscheinlichkeit des Geschlechts bei der Geburt
So hat die Variable *Geschlecht eines Babies* die beiden Auspr√§gungen *M√§dchen* und *Junge* mit den Wahrscheinlichkeiten $p_M = 51.2\%$ bzw. $p_J = 48.8\%$, laut einer Studie [@gelman_regression_2021].$\square$
:::


Zwischen der deskriptiven Statistik und der Wahrscheinlichkeitstheorie bestehen enge Parallelen, @tbl-wkeit-desk stellt einige zentrale Konzepte gegen√ºber.

```{r}
#| echo: false
#| label: tbl-wkeit-desk
#| tbl-cap: "Gegen√ºberstellung von Wahrscheinlichkeitstheorie und deskriptiver Statistik"
d <- tibble::tribble(
         ~Wahrscheinlichkeitstheorie,                      ~Desktiptive.Statistik,
                   "Zufallsvariable",                                   "Merkmal",
                "Wahrscheinlichkeit",               "relative H√§ufigkeit, Anteil",
       "Wahrscheinlichkeitsfunktion",   "einfache relative H√§ufigkeitsverteilung",
               "Verteilungsfunktion", "kumulierte relative H√§ufigkeitsverteilung",
                    "Erwartungswert",                                "Mittelwert",
                           "Varianz",                                   "Varianz"
       )
gt::gt(d)
```




```{r}
#| echo: false
#| message: false
#| warning: false
dice_outcomes <- expand.grid(Die1 = 1:6, Die2 = 1:6)

# Calculate the sum of the two dice for each outcome
dice_outcomes$Sum <- dice_outcomes$Die1 + dice_outcomes$Die2

# Calculate the probability of each sum using the table function
sum_counts <- table(dice_outcomes$Sum)
total_outcomes <- sum(sum_counts)
probabilities <- sum_counts / total_outcomes

twodice <- tibble(
  Augensumme = 2:12,
  p = probabilities) |> 
  mutate(p_cum = cumsum(p))

p_twodice <- 
  ggplot(twodice, aes(x = Augensumme, y = p)) + 
  geom_col() +
  geom_label(aes(y = p, label = round(p, 2), nudge_y = .1)) +
  scale_x_continuous(breaks = 1:12)
```




```{r}
#| echo: false
#| message: false
#| warning: false
#| 
num_trials <- 1000  # You can change this to the desired number of trials

# Simulate repeated throws of two dice
results <- replicate(num_trials, {
  die1 <- sample(1:6, 1, replace = TRUE)  # Simulate the first die
  die2 <- sample(1:6, 1, replace = TRUE)  # Simulate the second die
  c(Die1 = die1, Die2 = die2)  # Return the results as a vector
}) |> 
  t() |> 
  as_tibble() |> 
  mutate(Augensumme  = Die1 + Die2)

# Display

results_count <-
  results |> 
  count(Augensumme) |> 
  mutate(prop = n/num_trials) |> 
  mutate(n_cum = cumsum(n),
         prop_cum = cumsum(prop))

p_sim2dice <-
  ggplot(results_count) +
  aes(x = Augensumme, y = n) +
  geom_col() +
  geom_label(aes(y = n, label = round(prop, 2))) +
  scale_x_continuous(breaks = 1:12)
```





Eine *Verteilung* zeigt, welche Auspr√§gungen eine Variable aufweist und wie h√§ufig bzw. wahrscheinlich diese sind. 
Einfach gesprochen veranschaulicht eine Balken- oder Histogramm eine Verteilung. Man unterscheidet H√§ufigkeitsverteilungen (s. Abb. @fig-2dice-sim) von Wahrscheinlichkeitsverteilungen (Abb. @fig-2dice-prob).




:::: {.columns}

::: {.column width="50%"}
```{r}
#| echo: false
#| fig-cap: Wahrscheinlichkeitsverteilung der Zufallsvariable "Augenzahl im zweifachen W√ºrfelwurf"
#| label: fig-2dice-prob
p_twodice
```

:::

::: {.column width="50%"}
```{r}
#| echo: false
#| fig-cap: (relative und absolute) H√§ufigkeiten des zweifachen W√ºrfelwurfs, 1000 Mal wiederholt
#| label: fig-2dice-sim
p_sim2dice
```
:::

::::



:::{#exm-wert-wuerfel}
### Wahrscheinlichkeitsfunktion eines W√ºrfels
@fig-w-wuerfel zeigt die Wahrscheinlichkeitsfunktion eines einfachen W√ºrfelwurfs.$\square$
:::

![Wahrscheinlichkeitsfunktion eines einfachen W√ºrfelwurfs, Bildrechte: Olex Alexandrov, Wikipedia, PD](img/220px-Fair_dice_probability_distribution.svg.png){#fig-w-wuerfel width=33%}




:::{#exm-mtcars)
<!-- ### H√§ufigkeitsverteilung bei `mtcars` -->

Die H√§ufigkeitsverteilung eines *diskreten* Merkmals $X$ mit $k$ Auspr√§gungen zeigt (vgl. @tbl-hauef-tab),
wie h√§ufig die einzelnen Auspr√§gungen sind.
So hat die Variable *Zylinder* (in einem Datensatz) etwa die Auspr√§gungen 4,6 und 8.$\square$
:::


:::: {.columns}

::: {.column width="70%"}


```{r}
#| echo: false
#| fig-cap: "H√§ufigkeitsverteilung von `cyl` und `hp` (diskretisiert in 10 K√∂rbe oder Gruppen)"
#| label: fig-mtcars-freq
p1 <- 
  mtcars %>% 
  ggplot(aes(x = cyl)) +
  geom_bar()


p2 <- mtcars %>% 
  ggplot(aes(x = hp)) +
  geom_histogram(bins=10)

plots(p1, p2, n_rows = 1)
```
  
:::

::: {.column width="30%"}
```{r }
#| eval: true
#| echo: false
#| tbl-cap: Eine diskrete H√§ufigkeitsverteilung, dargestellt in einer H√§ufigkeitstabelle
#| label: tbl-hauef-tab
data(mtcars)
  mtcars %>% 
    count(cyl)
```
:::

::::



  
  
Abb. @fig-mtcars-freq, links, visualisiert die H√§ufigkeitsverteilung von `cyl`.
Ein *stetiges* Merkmal, wie `hp` (PS-Zahl), l√§sst sich durch Klassenbildung in ein diskretes umwandeln (diskretisieren), s. Abb. @fig-mtcars-freq, rechts.

#### Wahrscheinlichkeitsfunktion

:::{#def-w-fun}
### Wahrscheinlichkeitsfunktion

Die Funktion $f$, die den m√∂glichen Realisationen $x_i$ der diskreten Zufallsvariablen $X$ die Eintrittswahrscheinlichkeiten zuordnet, hei√üt Wahrscheinlichkeitsfunktion.$\square$
:::


:::{#exm-w-fun}
Die Wahrscheinlichkeitsfunktion f√ºr $X$ "Augensumme im zweifachen W√ºrfelwurf" ist in @fig-2dice-prob visualisiert.$\square$ 
:::


:::{#exm-muenz}
Die Wahrscheinlichkeitsfunktion f√ºr $X$ "Treffer im einfachen M√ºnzwurf, mit Zahl ist Treffer" ist $Pr(X=1)=1/2.$, vgl. @fig-zv.$\square$
:::

üí° Einfach gesprochen gibt die Wahrscheinlichkeitsfunktion die Wahrscheinlichkeit einer bestimmten Realisation einer Zufallsvariable an.

#### Verteilungsfunktion



:::{#def-vert-fun}
### Verteilungsfunktion
Die Verteilungsfunktion $F$ gibt die Wahrscheinlichkeit an, dass die diskrete Zufallsvariable $X$ eine Realisation annimmt, die kleiner oder gleich $x$ ist.$\square$
:::



Die Berechnung von $F(x)$ erfolgt, indem die Wahrscheinlichkeiten aller m√∂glichen Realisationen $x_i$, die kleiner oder gleich dem vorgegebenen Realisationswert $x$ sind, addiert werden:

$F(x) = \sum_{x_ \le x} Pr(X=x_i).$


```{r}
#| echo: false
p_F <- 
  ggplot(twodice, aes(x = Augensumme, y = p_cum)) + 
  geom_col() +
  geom_line() +
  geom_label(aes(label = round(p_cum, 2))) + 
  scale_x_continuous(breaks = 1:12) +
  labs(y = "Verteilungsfunktion F")


y_lab <- "empirische Verteilungsfunktion F emp."

p_F_emp <-
  ggplot(results_count) +
  aes(x = Augensumme, y = prop_cum) +
  geom_col() +
  geom_line() +
  geom_label(aes(y = prop_cum, label = round(prop_cum, 2))) +
  labs(y = y_lab) +
  scale_x_continuous(breaks = 2:12)
```


Die Verteilungsfunktion ist das Pendant zur *kumulierten H√§ufigkeitsverteilung*, vgl. @fig-kum-h-vert und @fig-kum-h-vert-emp:
Was die kumulierte H√§ufigkeitsverteilung f√ºr H√§ufigkeiten ist, ist die Verteilungsfunktion f√ºr Wahrscheinlichkeiten.


:::: {.columns}

::: {.column width="50%"}
```{r}
#| echo: false
#| fig-cap: Verteilungsfunktion $F(X \le x_i)$ f√ºr die Zufallsvariable "Augenzahl im zweifachen W√ºrfelwurf"
#| label: fig-kum-h-vert
p_F
```

:::

::: {.column width="50%"}
```{r}
#| echo: false
#| fig-cap: Empirische Verteilungsfunktion (kumulierte H√§ufigkeitsverteilung) $F(X \le x_i)$ von 1000 zweifachen M√ºnzw√ºrfen
#| label: fig-kum-h-vert-emp
p_F_emp 
```

:::

::::


### Stetige Zufallsvariablen

@fig-zv-stetig-groesse versinnbildlicht die stetige Zufallsvariable "K√∂rpergr√∂√üe", die (theoretisch, in Ann√§herung) jeden beliebigen Wert zwischen 0 und (vielleicht) 3m annehmen kann.

```{r echo = FALSE}
#| fig-cap: Sinnbild f√ºr eine stetige Zufallsvariable X "K√∂rpergr√∂√üe"
#| label: fig-zv-stetig-groesse
#| fig-asp: 0.2
 
ggplot(data.frame(x=0, y = 0), aes(x,y)) +
  #geom_point(size = 5, alpha = .7) +
  scale_y_continuous(limits = c(-1,1), breaks = NULL) +
  scale_x_continuous(breaks = c(0, 50, 100, 150, 200)) +
  annotate("segment", x = 0, xend = 200, y = 0, yend = 0, color = "red")  +
  theme_minimal() +
  annotate("label", x = 200, y = 0, label = "...") +
  labs(y = "", x = "")
```


:::{#def-zv-stetig}
### Stetige Zufallsvariable
Eine stetige Zufallsvariable gleicht einer diskreten, nur dass alle Werte im Intervall erlaubt sind.$\square$
:::


:::{#exm-zu-stetig}
- Spritverbrauch
- K√∂rpergewicht von Professoren
- Schnabell√§ngen von Pinguinen
- Geschwindigkeit beim Geblitztwerden$\square$
:::


:::{#exr-bus-42}
### Warten auf den Bus, 42 Sekunden
Sie stehen an der Bushaltestellen und warten auf den Bus.
Langweilig.
Da kommt Ihnen ein Gedanken in den Sinn: 
Wie hoch ist wohl die Wahrscheinlichkeit, dass Sie *exakt* 42 Sekunden auf den Bus warten m√ºssen, s. @fig-p42?
Weiterhin √ºberlegen Sie, dass davon auszugehen ist, dass jede Wartezeit zwischen 0 und 10 Minuten gleich wahrscheinlich ist.
Sp√§testens nach 10 Minuten kommt der Bus, so ist die Taktung (extrem zuverl√§ssig).
Exakt hei√üt *exakt*, also nicht 42.1s, nicht 42.01s, nicht 42.001s, etc. bis zur x-ten Dezimale.$\square$
:::


Nicht so einfach (?). Hingegen ist die Frage, wie hoch die Wahrscheinlichkeit ist, zwischen 0 und 5 Minuten auf den Bus zu warten ($0<x<5$), einfach: Sie betr√§gt 50%, wie man in @fig-p05 gut sehen kann.




:::: {.columns}

::: {.column width="50%"}

![Wie gro√ü ist die Wahrscheinlichkeit, zwischen 0 und 5 Minuten auf den Bus zu warten? 50 Prozent!](img/p_bus2.png){#fig-p05}


:::

::: {.column width="50%"}
```{r}
#| echo: false
#| fig-cap: "Wie gro√ü ist die Wahrscheinlichkeit, genau 42 Sekunden auf den Bus zu warten? Hm."
#| label: fig-p42
#| warning: false
p_bus1 <- 
  uniform_Plot(0, 10) + 
  geom_vline(xintercept = .42, color = "#56B4E9FF", size = 1) +
  annotate("label", x = .42, y = .05, hjust = 0, label = "Pr(X=0.42)=?", color="#56B4E9FF") +
  annotate("point", x = .42, y = 0, size = 5, color = "#56B4E9FF", alpha = .7) +
  annotate("label", x= 5, y = 0.1, label = "f(x) = 1/10", color = "#009E73FF", size = 10)

p_bus1
```
:::

::::

Vergleicht man @fig-p42 und @fig-p05 kommt man (vielleicht) zu dem Schluss, dass die Wahrscheinlichkeit exakt 42s auf den Bus zu warten, praktisch Null ist.
Der Grund ist, dass die Fl√§che des Intervalls gegen Null geht, wenn das Intervall immer schm√§ler wird.
Aus diesem Grund kann man bei stetigen Zufallszahlen nicht von einer Wahrscheinlichkeit eines bestimmten Punktes $X=x$ sprechen.
F√ºr einen bestimmten Punkt $X=x$ kann man aber die *Dichte* der Wahrscheinlichkeit angeben.

Was  gleich ist in beiden Situationen ($Pr(X=.42)$ und $Pr(0<x<0.5)$) ist die *Wahrscheinlichkeitsdichte*, $f$.
In @fig-p42 und @fig-p05 ist die Wahrscheinlichkeitsdichte gleich, $f=1/10=0.1$.

:::{#def-wdichte}
### Wahrscheinlichkeitsdichte
Die Wahrscheinlichkeitsdichte $f(x)$ gibt an, wie viel Wahrscheinlichkeitsmasse pro Einheit von $X$ an an der Stelle $x$ ist.$\square$
:::


Die Wahrscheinlichkeitsdichte zeigt an, an welchen Stellen $x$ die Wahrscheinlichkeit besonders "geballt" oder "dicht" sind, s. @fig-wdichte-sinnbild.

![Die Wahrscheinlichkeit, dass eine Zufallsvariable einen Wert zwischen und annimmt, entspricht dem Inhalt der Fl√§che unter dem Graph der Wahrscheinlichkeitsdichtefunktion. Bildrechte: 4C, Wikipedia, CC-BY-SA .](img/260px-Integral_as_region_under_curve.svg.png){#fig-wdichte-sinnbild width=33%}


#### Verteilungsfunktion


:::: {.columns}

::: {.column width="50%"}
:::{#def-vert-fun-stetig}
Die Verteilungsfunktion einer stetigen Zufallsvariablen gibt wie im diskreten Fall an, wie gro√ü die Wahrscheinlichkeit f√ºr eine Realisation kleiner oder gleich einem vorgegebenen Realisationswert $x$ ist.$\square$

Die Verteilungsfunktion $F(x)$ ist analog zur kumulierten H√§ufigkeitsverteilung zu verstehen, vgl. @fig-F-Bus.

:::
:::

::: {.column width="50%"}



```{r}
#| echo: false
#| fig-cap: 'Verteilungsfunktion F f√ºr X="Wartezeit auf den Bus"'
#| label: fig-F-Bus
#| fig-asp: 0.3
d <- 
  tibble(x=1:10,
         y= 1:10/10) 

ggplot(d, aes(x,y)) +
  geom_point(alpha = .5) +
  geom_line() +
  scale_x_continuous(breaks = 1:10) +
  theme_minimal()
```




:::

::::






:::{#def-wvert-stetig}
### Stetige Wahrscheinlichkeitsverteilung
Bei *stetigen* Zufallsvariablen $X$ geht man von unendlich vielen Auspr√§gungen aus; die Wahrscheinlichkeit einer bestimmten Auspr√§gung ist (praktisch) Null: $Pr(X=x_j)=0, \quad j=1,...,+\infty \square$.
:::


:::{#exm-groesse}
### Wahrscheinlichkeitsverteilung f√ºr die K√∂rpergr√∂√üe
So ist die Wahrscheinlichkeit, dass eine Person exakt 166,66666666... cm gro√ü ist, (praktisch) Null.
Man gibt stattdessen die *Dichte* der Wahrscheinlichkeit an: Das ist die Wahrscheinlichkeit(smasse) pro  Einheit von $X$.$\square$
:::


F√ºr praktische Fragen berechnet man zumeist die Wahrscheinlichkeit von Intervallen, s. @fig-wdichte-sinnbild.

## Wichtige Verteilngen


### Gleichverteilung

#### Indifferenz als Grundlage

Eine Gleichverteilung nimmt an, dass jeder Wert im Ergebnisraum der zugeh√∂rigen Zufallsvariable *gleichwahrscheinlich* ist.
Wenn man keinen hinreichenden Grund hat, eine Realisation einer Zufallsvariablen f√ºr plausibler als einen anderen zu halten,
ist eine Gleichverteilung eine passende Verteilung.
Gleichverteilungen gibt es im diskreten und im stetigen Fall.

Abb. @fig-uniform zeigt ein Beispiel f√ºr eine (stetige) Gleichverteilung.


```{r Normalverteilung-4, fig.asp = 0.5}
#| echo: false
#| fig-cap: "Stetige Gleichverteilung; man beachte jeweils die Y-Achse"
#| fig-subcap: 
#|   - "Beispiel a: Gleichverteilung min=-1, max=1. Dichte: 1/2"
#|   - "Beispiel b: Gleichverteilung min=0, max=3. Dichte: 1/3"
#| label: fig-uniform
#| layout-ncol: 2
#
#source: https://dk81.github.io/dkmathstats_site/rmath-uniform-plots.html

uniform_Plot(-1, 1)
uniform_Plot(0, 3)
```




@fig-uniform, links: Bei $X=0$ hat eine Einheit von $X$ die Wahrscheinlichkeitsmasse von 50%, da der Bereich $[-0.5, +0.5]$ die H√§lfte (50%) der Wahrscheinlichkeitsmasse der Verteilung beinhaltet. Bei jedem anderen Punkt $x$ ist die Dichte gleich.
@fig-uniform, rechts: Bei $X=0$ hat eine Einheit von $X$ die Wahrscheinlichkeitsmasse von ca. 33%, da der Bereich $[-0.5, +0.5]$ ein Drittel der Wahrscheinlichkeitsmasse der Verteilung beinhaltet. Bei jedem anderen Punkt $x$ ist die Dichte gleich.
Definierendes Kennzeichen einer Gleichverteilung ist die *konstante Dichte*.



#### Simulation


M√∂chte man die Verteilungsfunktion einer stetigen Zufallsvariablen berechnen,
kann die Mathe ganz sch√∂n kompliziert werden, schlie√ülich muss man Integrale l√∂sen.
Aber es gibt einen Trick, wie man die Sache stark vereinfachen kann: 
man simuliert die Verteilung. Was bedeutet das?



Angenommen, die Wartezeit auf einen Bus ist gleichverteilt (engl. *uniform distribution*); 
der Bus kommt regelm√§√üig und p√ºnktlich alle 10 Minuten. 
Die minimale Wartezeit betr√§gt also 0 Minuten und die maximale 10 Minuten.
Nennen wir die zugeh√∂rige Zufallsvariable $X$, das ist sch√∂n kurz zu schreiben.

Eine gleichverteilte Zufallsvariable $X$ mit Min $m_0$ und Maximum $m_1$ schreibt man auch wie folgt in Kurzschreibweise:

$$X \sim Unif(m_0,m_1).$$



Ja, das sieht fancy aus, ist aber daf√ºr sch√∂n kurz, aber wo ist der versprochene Trick zum Vereinfachen?
Kommt gleich, Moment.

Eine Frage k√∂nnte nun lauten, wie gro√ü ist die Wahrscheinlichkeit, dass man zwischen 3 und 5 Minuten auf den Bus warten muss?
Achtung: Hier ist der Trick. N√§mlich, dass wir Integralrechnung gegen stumpfes Z√§hlen eintauschen.

Computer (und damit R) haben eingebaute Funktionen, die eine beliebige Zufallszahl ziehen k√∂nnen,
zum Beispiel gleichverteilte.
Auf Errisch hei√üt das Zauberwort `runif()`:

```{r}
#| eval: false
runif(n = 1, min = 0, max = 10)
```


```{r}
#| echo: false
set.seed(42)
runif(n = 1, min = 0, max = 10)
```

Auf Deutsch hei√üt das: 

> üë®‚Äçüè´   "Hey R, ich h√§tte gerne eine (daher `n = 1`) Zufallszahl (*r* wie *random*),
die gleichverteilt ist (*uniform*) mit `min = 0` und `max = 10`.

>   ü§ñ Jawohl, oh herrliches Leberwesen



(Zu) anschaulich gesprochen: R hat den Bus kommen lassen und es hat gut 9.1 Minuten gedauert,
bis er da war.
Achtung, jetzt kommt's: Jetzt lassen wir R mal $10^5$ (`1e5` auf Computersprech) Busse vorfahren. 
R soll jedes Mal notieren, wie lange man auf den Bus warten musste.^[Machen Sie das mal ohne Computer, wenn Sie ein Wochenende lang Langeweile haben.]




```{r}
#| eval: false
x_simu <- runif(n = 1e5, min = 0, max = 10)
```

```{r}
#| echo: false
n <- 1e5
set.seed(42)
x_simu <- runif(n = n, min = 0, max = 10)  # gibt Vektor zur√ºck

x_simu_df <-
  tibble(id = 1:n,
         x = x_simu)
```



Schauen wir uns die Verteilung an, s. @fig-simu-gleichvert.^[Alternativ kann man z.B. auch `ggplot` verwenden: `ggplot(x_simu_df, aes(x = x_simu)) +  geom_histogram(bins = 50)`.]

```{r}
#| eval: false

library(ggpubr)
gghistogram(x_simu_df, x = "x_simu", fill = "grey20")
```




```{r}
#| label: fig-simu-gleichvert
#| fig-cap: "Simulation einer gleichverteiluten Zufallsvariablen"
#| eval: true
#| echo: false

library(ggpubr)
gghistogram(x_simu_df, x = "x_simu", fill = "grey20")
```






Okay, unsere Verteilung sieht nicht *exakt* gleichverteilt, aber einigerma√üen. 
Gut genug f√ºr unsere Zwecke!

So, und jetzt kommt das Ernten.
Wir k√∂nnen jetzt n√§mlich einfach z√§hlen (`count()`), um die Antwort auf unsere Frage (der Wartezeit 3-5 Min.) zu erhalten, s. @tbl-count-simu-bus.


:::: {.columns}

::: {.column width="50%"}
```{r}
#| eval: false
x_simu_df %>% 
  count(Schnittmenge = x > 3 & x < 5)
```

:::

::: {.column width="50%"}
```{r}
#| echo: false
#| tbl-cap: H√§ufigkiten auslesen anstelle von Integralen berechnen
#| label: tbl-count-simu-bus
x_simu_df %>% 
  count(Schnittmenge = x > 3 & x < 5)
```

:::

::::




Das Zeichen `&` ist das logische UND, also die Schnittmenge der zwei Mengen $A := \{x|x>3\}$ und $B := \{x|x<5\}$, 
also $A \cap B$.

Wie man sieht, fallen ca. 20% der Stichproben in den entsprechenden Bereich. 


Da viele Probleme, wenn sie komplexer werden, kaum noch "analytisch" (d.h. wie Ausrechnen von Integralen) l√∂sbar sind,
greift man in der modernen (Analyse-)Welt oft lieber auf Simulationsverfahren zur√ºck - Dank sei den schnellen Rechnern.
F√ºr uns Menschen ist damit die Aufgabe des Integrierens auf schn√∂des Z√§hlen zur√ºckgef√ºhrt.







### Binomialverteilung {#sec-bin-distrib}

#### Grundlagen

:::{#def-binvert}
### Binomialverteilung
Die Binomialverteilung dient zur Darstellung der Wahrscheinlichkeit der Ergebnisse eines $n$-fach wiederholten binomialen Zufallexperiments,
eines Zufallsexperiments mit *zwei*^[von lat. *bis* "zweimal"] Ergebnissen bzw. Elementarereignissen also. 
Dabei interessiert die Anzahl der $x$ Treffer.
Typisches Beispiel ist ein (wiederholter) M√ºnzwurf.
Bei jeder Wiederholung des Zufallexperiments bleibt die Wahrscheinlichkeit der Ergebnisse gleich: Die M√ºnze ver√§ndert sich nicht durch die W√ºrfe (*Ziehen mit Zur√ºcklegen*, ZmZ). 
Au√üerdem hat ein bestimmtes Ergebnis im ersten Wurf keinen Einfluss auf die Wahrscheinlichkeit eines bestimmten Ergebnisses im zweiten Wurf, etc.$\square$ 
:::

F√ºr eine binomialverteilte Zufallsvariable $X$ schreibt man kurz (s. @eq-binvert): 

$$X \sim \text{Bin}(n, x)$${#eq-binvert}


#### Veranschaulichung

Stellen wir uns eine Kistchen^[In den Lehrb√ºchern h√§ufig als Urne bezeichnet, was den b√∂sen Spott von "Friedhofstatistik" nach sich zog.] mit 5 Losen vor, darunter 2 *T*reffer (Gewinn) und 3 *N*ieten, s. Abb. @fig-urne.
Der Versuch l√§uft so ab: Wir ziehen ein Los, schauen ob es ein Treffer ist oder nicht, legen es zur√ºck und ziehen erneut.



```{r echo = FALSE}
#| fig-cap: Ein K√§stchen mit 5 Losen, darunter 2 Treffer und 3 Nieten.
#| label: fig-urne
#| fig-asp: 0.2
d <- 
  tibble(id = 1:5,
         event = c("T", "T", "N", "N", "N"))


ggplot(d) +
  aes(x = id) +
  theme_minimal() +
  annotate("rect", xmin = 0, xmax = 6, ymin = 0, ymax = 2, fill = "grey80", alpha = .8) +
  geom_point(size = 10, y = 1, aes(color = event) )+
  geom_text(aes(label = event), y = 1) +
  theme(axis.text = element_blank()) +
  labs(x = "", fill = "")
```





:::callout-important

Um die Wahrscheinlichkeitsverteilung einer binomialverteilte Zufallsvariable ausrechnen zu k√∂nnen, muss man zwei Dinge wissen: Erstens die Anzahl der Z√ºge, $n$ (Stichprobengr√∂√üe) und zweitens die Trefferwahrscheinlichkeit, $p$. 

:::


:::{#exr-bin1}
### Vier Lose, zwei Treffer
Wie gro√ü ist die Wahrscheinlichkeit von $A^{\prime}$, d.h. bei $n=4$ Z√ºgen $x=2$ Treffer zu erzielen (und $n-x=2$ Nieten), gegeben dass die Trefferwahrscheinlichkeit bei $p=2/4$ liegt?
Wir ziehen dabei *ohne* Zur√ºcklegen (ZoZ). Au√üerdem sind die Lose nicht zu unterscheiden (abgesehen davon, ob es Treffer oder Nieten sind).$\square$
:::


Wir k√∂nnten jetzt ein Baumdiagramm zeichnen und pro Pfad die Wahrscheinlichkeit ausrechnen  (Multiplikationssatz, @eq-multtheorem), vgl. @fig-dreimuenzen.
Die Summe der Wahrscheinlichkeiten der Pfade ist dann die gesuchte Wahrscheinlichkeit, $W$ (Additionssatz).
Das ist einfach, dauert aber.
In diesem Fall ist die Wahrscheinlichkeit eines (g√ºnstigen) Pfades, $A$:

$Pr(A) = Pr(T)^2 \cdot Pr(N)^2 = \left( \frac{2}{5} \right)^2 \cdot \left( \frac{3}{5} \right) ^2$.

```{r}
p_a = (2/5)^2 * (3/5)^2
p_a
```

Etwas m√ºhevolles Z√§hlen der Pfade w√ºrde uns zeigen, dass es $k=6$ Pfade gibt, die alle die gleiche Wahrscheinlichkeit, $Pr(A)$, aufweisen.
Damit betr√§gt die Wahrscheinlichkeit des gesuchten Ereignisses $A^{\prime}$ (2 Treffer bei 4 Z√ºgen):

$Pr(A^{\prime}) = 6 \cdot Pr(A)$.

```{r}
p_a_strich = 6 * p_a
p_a_strich
```


Mithilfe der Formel der Binomialverteilung l√§sst sich das Ergebnis, die Wahrscheinlichkeit von $A^{\prime}$  schneller ausrechnen. 
Einfach gesprochen sieht sie so aus:



$$Pr(A^{\prime}) = k \cdot Pr(A)$$
Dabei steht $k$ f√ºr die Anzahl der g√ºnstigen Pfade und $Pr(A)$ f√ºr die Wahrscheinlichkeit eines g√ºnstigen Pfades (d.h. 2 Treffer und 2 Nieten) und alle Pfade haben die gleiche Wahrscheinlichkeit.

Die Anzahl der Pfade kann man mit dem *Binomialkoeffizient* ausrechnen, den man so darstellt, s. @eq-binkoeff.^[wobei gelten muss $n \ge k$]

:::{#def-binkoeff}
### Binomialkoeffizient
Der Binomialkoeffizient gibt an, auf wie vielen verschiedenen Arten man aus einer Menge von $n$ verschiedenen Objekten $k$ Objekte ziehen kann (ohne Zur√ºcklegen und ohne Beachtung der Reihenfolge).

$$k = \tbinom{n}{k}= \frac{n!}{k!(n-k)!} $${#eq-binkoeff}


Lies: "W√§hle aus $n$ m√∂glichen Ereignissen (Pfade im Baum) $k$ g√ºnstige Ereignisse (g√ºnstige Pfade) oder k√ºrzer "k aus n".\square
:::

:::{#exm-binkoeff}
### Lotto

Wie viele Zahlenkombinationen gibt es im Lotto f√ºr 6 Richtige?
Der Binomialkoeffizient verr√§t es uns:
$\tbinom{49}{6}= 13\,983\,816\square$
:::

Auf Errisch geht das so:


:::: {.columns}

::: {.column width="50%"}
>    üë®‚Äçüè´ Hey R, Wie viele M√∂glichkeiten gibt es, aus $n=4$ Pfaden $k=2$ auszuw√§hlen?


:::

::: {.column width="50%"}
>    ü§ñ √Ñh, Moment, oh herzliches Leberwesen
```{r}
choose(4,2)
```
:::

::::

Hier ist ein √úberblick der m√∂glichen 6 g√ºnstigen Ereigniise des Experiments (2 Treffer bei 4 Versuchen):
1. TTNN, 2. TNTN, 3. TNNT, 4. NTTN ,5. NTNT, 6. NNTT.

```{r}
#| echo: false
# Define the values of n and k
n <- 4  # Total number of items
k <- 2  # Number of items to choose

# Generate all combinations
combinations <- combn(n, k) |> t() |> as_tibble()
```



:::{#exm-bef√∂rdern}
### Bef√∂rderung
Aus einem Team mit 25 Personen sollen 11 Personen bef√∂rdert werden. Wie viele m√∂gliche Kombinationen (von bef√∂rderten Personen) k√∂nnen gebildet werden?

$\tbinom{25}{11} = \frac{25!}{11!\cdot(25-11)!} = 4\,457\,400$

In Errisch:

```{r}
choose(n = 25, k = 11)
```

Es gibt 4457400 Kombinationen von Teams; dabei ist die Reihenfolge der Ziehung nicht ber√ºcksichtigt.$\square$
:::


#### Formel der Binomialverteilung


@eq-binomial zeigt die mathematische Definition der Binomialverteilung.
Dabei liegt immer ein Zufallsversuch mit $n$ Durchg√§ngen und $k$ Treffern zugrunde. Jeder Durchgang hat die Trefferwahrscheinlichkeit $p$ und jeder Durchgang ist unabh√§ngig von allen anderen.



$$Pr(X=k|p,n) = \frac{n}{k!(n-k)!}p^k(1-p)^{n-k}$${#eq-binomial}


 @eq-binomial kann wie folgt auf Deutsch √ºbersetzen:

>   Die Wahrscheinlichkeit f√ºr das Ereignis "W,L" gegeben p berechnet als Produkt von zwei Termen. Erstens der Quotient von der Fakult√§t von W plus L im Z√§hler und im Nenner das Produkt von erstens der Fakult√§t von W mit zweitens der Fakult√§t von L. Der zweite Term ist das Produkt von p hoch W mal der komplement√§ren Wahrscheinlichkeit von p hoch L.

Oder noch k√ºrzer:

>    Die Wahrscheinlichkeit f√ºr das Ereignis "W,L" gegeben p berechnet als Produkt von zwei Termen. Erstens der Anzahl der g√ºnstigen Pfade, k und zweitens der Wahrscheinlichkeit f√ºr einen g√ºnstigen Pfad, P(A).


Puh, Formeln sind vielleicht doch ganz praktisch, wenn man sich diese lange √úbersetzung der Formel in Prosa duchliest.
Noch praktischer ist es aber, dass es Rechenmaschinen gibt, die die Formel kennen und f√ºr uns ausrechnen. 
Los, R, mach mal.



#### Rechnen mit R


Die Binomialverteilung ist in R eingebaut; man kann sich leicht entsprechende Wahrscheinlichkeiten ausrechnen lassen.

Die Wahrscheinlichkeit, bei 4 Z√ºgen 2 Treffer zu erzielen mit $p=2/5$ unter der Annahme einer Binomialverteilung l√§sst sich so mit R berechnen:

```{r}
dbinom(x = 2, size = 4, prob = 2/5)
```



::: {#exm-binom}
### Pumpstation-Beispiel zur Binomialverteilung

In einer Pumpstation arbeiten 7 Motoren, die wir als identisch annehmen. Mit einer Wahrscheinlichkeit von 5% f√§llt ein Motor aus und ist f√ºr den Rest des Tages nicht einsatzbereit. Der Betrieb kann aufrecht erhalten werden, solange mindestens 5 Motoren arbeiten. Wie gro√ü ist die Wahrscheinlichkeit, dass die Pumpstation aus dem Betrieb f√§llt?

$Pr(X=k)$ (oder kurz: $Pr(k)$) gibt die Wahrscheinlichkeit (Wahrscheinlichkeitsfunktion) an f√ºr das Ereignis, dass *k* Motoren arbeiten.

Lassen wir R mal $Pr(X=5)$ ausrechnen.


```{r}
dbinom(x = 5, size = 7, prob = .95)
```


Es gilt also $Pr(X=5) \approx .04$. Die Wahrscheinlichkeit, dass (nur) 5 Motoren laufen an einem beliebigen Tag ist relativ gering^[wobei "gering" subjektiv ist, die Betreiberfirma findet diese Wahrscheinlichkeit, dass 2 Pumpen ausfallen, wohl viel zu hoch.].
Die Wahrscheinlichkeit, dass $k=0 \ldots 7$ Motoren laufen, ist in @fig-motoren dargestellt.


`dbinom()` steht f√ºr die Wahrscheinlichkeits*d*ichte (im diskreten Fall, wie hier, Wahrscheinlichkeitsfunktion genannt) und `binom` f√ºr die Binomialverteilung. `x` gibt die Anzahl der Treffer an (das gesuchte Ereignis, hier 5 Motoren arbeiten); `size` gibt die Stichprobengr√∂√üe an (hier 7 Motoren).

Damit gilt:

$Pr(X\ge 5) = Pr(X=5) + Pr(X=6) + Pr(X=7)$

Berechnen wir zun√§chst die Wahrscheinlichkeit, dass 5,6 oder 7 Motoren laufen:

```{r}
p_5 <- dbinom(x = 5, size = 7, prob = .95)
p_6 <- dbinom(x = 6, size = 7, prob = .95)
p_7 <- dbinom(x = 7, size = 7, prob = .95)

p_5
p_6
p_7
```

Die gesuchte Wahrscheinlichkeit, `p_mind_5`, ist die Summe der drei Einzelwahrscheinlichkeiten:

```{r}
p_mind_5 <- p_5 + p_6 + p_7

p_mind_5
```


Die Wahrscheinlichkeit, dass mind. 5 Motoren arbeiten betr√§gt also `r round(p_mind_5, 4)`.


Das Komplement zu diesem Ereignis ist, dass *nicht* mind. 5 Motoren arbeiten, also *h√∂chstens 4* und es daher zu einem Ausfall kommt.

Nat√ºrlich gilt $Pr(\bar{X}) = 1- Pr(X)$.

```{r}
p_weniger_als_4 <- 1 - p_mind_5
p_weniger_als_4
```



```{r}
#| echo: false
#| fig-cap: "Wahrscheinlichkeit, dass genau k = 0..7 Motoren laufen"
#| fig-subcap: 
#|   - "In 'normaler' Wahrscheinlichkeit, 0<p<1"
#|   - "In Log-Einheiten (Basis 2), 'Halbierungen'"
#| label: fig-motoren
#| layout-ncol: 2
#| 

font_size <- 6

ps <- dbinom(x = 0:7, size = 7, prob = .95)

ps_df <-
  tibble(Motorenzahl = 0:7,
         Pr = ps,
         Pr_log = log(ps, base = 2))

ps_df |> 
  ggplot(aes(x = Motorenzahl, y = Pr)) +
  geom_col() +
  scale_x_continuous(breaks = 0:7) +
  geom_label(aes(label = round(Pr, 3)), size = font_size)

ps_df |> 
  ggplot(aes(x = Motorenzahl, y = Pr_log)) +
  geom_col() +
  scale_x_continuous(breaks = 0:7) +
  geom_label(aes(label = round(Pr_log, 0)), size = font_size)
```


Alternativ kann man mit der Verteilungsfunktion rechnen: $Pr(X \le 4)$.




In R kann man die Funktion `pbinom()` nutzen (p f√ºr (kumulierte) Wahrscheinlichkeit), um die Verteilungsfunktion der Binomialverteilung zu berechnen:

```{r}
pbinom(q = 4, size = 7, prob = .95)
```

`q = 4` steht f√ºr $X \le 4$, also f√ºr h√∂chstens 4 Treffer (arbeitende Motoren); `size = 7` meint die Stichprobengr√∂√üe, hier 7 Motoren.$\square$

:::


:::callout-important

Die Funktion, die die Wahrscheinlichkeit daf√ºr angibt, dass die diskrete Zufallsvariable $X$ eine Realisation annimmt, die *kleiner oder gleich* (h√∂chstens) einem Wert $X=x$ ist, hei√üt *Verteilungsfunktion*.

$F(X=x) = Pr(X \le x)$

:::


:::{.callout-note}
### Logarithmus
Der Logarithmus zur Basis 2^[als "Logarithmus Dualis", ld, bezeichnet] gibt die "Verdopplungen" bzw. "Halbierungen" der Wahrscheinlichkeit an, wobei $ld(1/2) = -1.\square$
:::


:::{#exm-log2-1}
$ld(1/2) = -1:$
```{r}
log(.5, base = 2)
```

1/2 ist genau "minus 1 Verdopplung" von 1 entfernt, d.h. eine Halbierung.

$ld(1/4) = -2:$
```{r}
log(1/4, base = 2)
```
1/2 ist genau "minus 2 Verdopplungen" von 1 entfernt, d.h. zwei Halbierungen.

$ld(1/8) = -3:$
```{r}
log(1/8, base = 2)
```

1/8 (0.125) ist 3 Halbierungen von 1 entfernt.$\square$
:::





::: {#exm-klausur20}

## Klausur mit 20-Richtig-Falsch-Fragen

Ei Professi stellt einen Klausur mit 20 Richtig-Falsch-Fragen. Wie gro√ü ist die Wahrscheinlichkeit, durch blo√ües M√ºnze werfen genau 15 Fragen richtig zu raten?^[Hey, endlich mal was f√ºr echte Leben!]

```{r QM2-Thema2-kleineModelle-23, echo = TRUE}
dbinom(x = 15, size = 20, prob = .5)
```

Um *h√∂chstens* 15 Treffer zu erzielen, m√ºssten wir die Wahrscheinlichkeiten von 0 bis 15 Treffern addieren.

Praktischerweise gibt es einen R-Befehl, der das f√ºr uns √ºbernimmt:

```{r}
pbinom(q = 15, size = 20, prob = .5)
```


Die Wahrscheinlichkeit 0, 1, 2, ... oder 15 Treffer zu erzielen, liegt also bei gut 99%.


:::



::: {#exm-globus4}

## 3 M√ºnzw√ºrfe mit 3 Treffern

Was ist die Wahrscheinlichkeit bei 3 M√ºnzw√ºrfen (genau) 3 Treffer (Kopf) zu erzielen?

Das ist eine Frage an die Binomialverteilung;
in R kann man das mit der Funktion `dbinom` beantworten.

```{r QM2-Thema2-kleineModelle-24, echo = TRUE}
dbinom(x = 3, size = 3, prob = 1/2)
```

Die L√∂sung lautet also $p=1/8 = .125.\qquad \square$

:::


<!-- Todo: XXX -->

```{r QM2-Thema2-kleineModelle-25}
#| echo: false
#| label: p-bin-exms
#| fig-cap: "Verschiedene Binomialverteilungen"
#| layout-ncol: 2
#| fig-subcap: 
#|   - "n=3, p=1/2"
#|   - "n=9, p=.7"

binomial_plot(3, 1/2)

binomial_plot(9, .7)
```




:::{#exr-binom}
üèãÔ∏èÔ∏è Was f√§llt Ihnen bei der Binomialverteilung auf? Ist sie symmetrisch? Ver√§ndert sich die Wahrscheinlichkeit linear?
:::


#### Simulieren wir eine Binomialverteilung


Die Binomialverteilung l√§sst sich gut als "M√ºnzwurf-Verteilung" auffassen.

Werfen wir eine M√ºnze und sehen wir, was passiert.


```{r}
sample(x = c(0, 1), size = 1)
```


Mit `sample()` ziehen wir eine Stichprobe aus dem Ereignisraum `x`, hier 0 und 1. 
Dabei vereinbaren wir (willk√ºrlich), dass 0 f√ºr "Kopf" steht und 1 f√ºr "Zahl".
`size = 1` bedeutet, wir werfen die M√ºnze ein Mal (d.h. Stichprobengr√∂√üe *size* ist 1).

Okay, noch an Bord? Dann werfen wir die M√ºnze 10 Mal:


```{r}
sample(x = c(0, 1), size = 10, replace = TRUE)
```

`replace = TRUE` hei√üt, wir legen die M√ºnze wieder zur√ºck auf den Tisch, wenn wir sie geworfen haben.
Oder anders ausgedr√ºckt: *Ziehen mit Zur√ºcklegen*.



R, mach dich bereit, wirf die M√ºnze 1000 ($n=10^3$ oder `1e3`) Mal^[R meckert nicht bei langweiligen Aufgaben.]:


```{r}
n <- 1e3

muenze_oft <- 
  sample(x = c(0, 1), size = n, replace = TRUE) 


muenze_oft %>% 
  sum()
```


Mit `sum()` nach dem Pfeifensymbol `%>%` haben wir aus dem Vektor `muenze_oft`, der aus der ersten Zeile resultiert,
die Summe ausgerechnet. 

Jetzt wissen wir, wie oft die M√ºnze "Zahl" gezeigt hat, n√§mlich `r sum(muenze_oft)` Mal.



::: callout-note
Wenn Sie einen Zufallsversuch wiederholen, muss nicht jedes Mal das gleiche Ergebnis resultieren. Entsprechend wird bei wiederholten Ausf√ºhrung der Funktion `sample()` nicht immer das gleiche Ergebnis resultieren. Wundern Sie sich also nicht, wenn bei Ihrem Computer eine √§hnliche, aber nicht gleiche, Zahl herauskommt.
:::


Visualisieren wir mal unsere M√ºnzw√ºrfe. Dazu erstellen wir zuerst eine geeignete Tabelle, @tbl-muenz.


```{r}
muenz_tab <-
  tibble(
    id = 1:n,
    x = muenze_oft,
    x_cumsum = cumsum(x) / id  # gibt Anteil von "Zahl" wieder
  )
```


```{r}
#| echo: false
#| label: tbl-muenz
#| tbl-cap: "Die kumulierte Summe beim M√ºnzwurf (nur die ersten paar Zeilen)"
head(muenz_tab)
```


Und hier der Anteil von "Zahl" im Verlauf unserer M√ºnzw√ºrfe, s. @fig-lln.^[`library(ggpubr); ggline(muenz_tab, x = "id", y = "x_cumsum")`]


```{r}
#| label: fig-lln
#| echo: false
#| fig-cap: "Das Gesetz der gro√üen Zahl am Beispiel der Stabilisierung des Trefferanteils beim wiederholten M√ºnzwurf"
#| fig-asp: 0.3

muenz_tab %>% 
  slice_head(n = 1e3) %>% 
  ggplot() +
  aes(x = id, y = x_cumsum) +
  geom_line() +
  theme_minimal()
```



Grob gesagt scheint sich ein M√ºnzwurf nach, naja, vielleicht 500 W√ºrfen "einigerma√üen" zu stabilisieren.^[Was "einigerma√üen" bedeuten soll, ist kein statistischer Begriff, sondern einer, der im echten Leben von den Menschen beantwortet werden muss, die eine Entscheidung zu treffen haben.]



::: callout-important
### Das Gesetz der gro√üen Zahl

Zieht man (zuf√§llig) immer mehr Werte aus einer Verteilung (mit endlichem Mittelwert), n√§hert sich der Mittelwert der Stichprobe immer mehr mit dem Mittelwert (oft als *Erwartungswert* bezeichnet) der Verteilung an.
:::



```{r lln, out.width = "100%", fig.align="center", fig.asp = .5}
#| eval: false
#| echo: false
source(paste0(here::here(),"/R-Code/img15.R"))
```



<!-- 3b1b hat ein [nettes Video zu diesem Thema](https://youtu.be/8idr1WZ1A7Q), das sich als Vertiefung eignet. -->




### Normalverteilung


:::{#def-nv}
### Normalverteilung
Normalverteilungen haben eine charakteristische Glockenform;
sie sind symmetrisch^[d.h. die Schiefe (`skewness`) ist 0].
Normalverteilungen k√∂nnen sich unterscheiden in ihrem  Mittelwert $\mu$ und ihrer Streuung, $\sigma$.
Diese beiden Gr√∂√üen ("Parameter") determinieren den Graphen einer bestimmten Normalverteilungsfunktion, s. @fig-norms.
Sind diese beiden Parameter bekannt, so ist die Dichte jedes beliebigen Datenpunkts (aus dieser Normalverteilung) bestimmt.$\square$
:::

Eine normalverteilte Zufallsvariable $X$ mit einem bestimmten Mittelwert und einer bestimmten Streuung schreibt man kurz so:

$$X \sim \mathcal{N}(\mu, \sigma)$$

::: {#def-parameter}
### Parameter

Ein Parameter (einer Verteilung) legt die "Varianten" einer Verteilung fest. Durch die Wahl der Parameterwerte nimmt eine Verteilung eine genaue Form an.$\square$

:::

![Beispiele von Normalverteilungen mit verschiedenen Mittelwerten und Streuungen, Quelle: Wikipedia](img/normals.png){#fig-norms width=50% fig-align="center"}


Beispiel: Wie gro√ü sind Studentis ([Quelle des Datensatzes](https://rdrr.io/cran/openintro/man/speed_gender_height.html))? 

Das Quantil von z.B. 25% zeigt die K√∂rpergr√∂√üe der 25% kleinsten Studentis an, analog f√ºr 50%, 75%, vgl. @tbl-quantiles.

```{r QM2-Thema3-Teil1-5, echo = FALSE}
#| tbl-cap: Quantile der K√∂rpergr√∂√üen von Studentis
#| label: tbl-quantiles
data(speed_gender_height, package = "openintro")

height_summary <- 
  speed_gender_height %>% 
  drop_na(height) %>% 
  mutate(height = height * 2.54) %>% 
  summarise(q25 = quantile(height, prob = .25),
            q50 = quantile(height, prob = .5),
            q75 = quantile(height, prob = .75))

height_summary %>% 
  gt()
```

@fig-quantiles zeigt eine Visualisierung der Quantile.

```{r QM2-Thema3-Teil1-6}
#| echo: false
#| results: hold
#| fig-width: 12
#| fig-asp: 0.618
#| fig-cap: "Quantile verschieden visualisiert"
#| label: fig-quantiles

speed_gender_height <-
  speed_gender_height %>% 
  mutate(height_cm = height * 2.54)

p1 <- 
  speed_gender_height %>% 
  ggplot() +
  aes(x = 1, y = height_cm) +
  geom_boxplot() +
  labs(x = "",
       y = "Gr√∂√üe in cm",
       title = "Die Box zeigt das 25%-, 50%- und 75%-Quantil")

height_summary_long <- 
  height_summary %>% 
  pivot_longer(everything(),
               names_to = "q",
               values_to = "height_cm") 

p2 <- 
  speed_gender_height %>% 
  ggplot() +
  aes(x = height_cm) +
  geom_histogram() +
  geom_vline(data = height_summary_long,
             aes(xintercept = height_cm)) +
  geom_text(data = height_summary_long,
             aes(x = height_cm+1,
                 y = 0,
                 label = paste0(q, ": ",height_cm)),
             angle = 90,
            hjust = 0,
            color = "white"
             ) +
  labs(title = "Die vertikalen Striche zeigen die Quantile",
       y = "H√§ufigkeit")  +
  theme_minimal()
 
plots(p1, p2)
```


::: callout-note
Das 25%-Quantil nennt man *1. Quartil*, das 50%-Quantil auch *2. Quartil*, das 75%-Quantil das *3. Quartil*, und das 100%-Quantil (Maximalwert) das *4. Quartil*.
:::








#### Normal auf dem Fu√üballfeld

Sie und 100 Ihrer besten Freunde stehen auf der Mittellinie eines Fu√üballfelds. Auf Kommando werfen alle jeweils eine M√ºnze; bei Kopf geht man einen Schritt nach links, bei Zahl nach rechts. Das wird 16 Mal wiederholt. Wie wird die Verteilung der Positionen wohl aussehen?


```{r Normalverteilung-6, fig.asp = .45, fig.width=7}
#| echo: false
source(paste0(here::here(),"/R-Code/img13.R"))
```

[@mcelreath_statistical_2020]







#### Normal durch Addieren

Die Summe vieler (gleich starker) Zufallswerte (aus der gleichen Verteilung) erzeugt eine Normalverteilung; egal aus welcher Verteilung die Zufallswerte kommen (Zentraler Grenzwertsatz), vgl. @fig-fussball.


```{r Normalverteilung-7, out.width="100%", fig.asp = 0.5, fig.align="center", fig.width=7}
#| echo: false
#| fig-cap: "Entstehen einer Normalverteilung durch Addition vieler unabhg√§ngiger Ereignisse"
#| label: fig-fussball
source(paste0(here::here(),"/R-Code/img14.R"))
```



Verwechseln Sie die Normalverteilung nicht mit der Paranormalverteilung, s. @fig-paranormal.

![Die Paranormalverteilung](img/ch33910f1.jpg){#fig-paranormal width=30%}







#### Normalverteilung vs. randlastige Verteilungen



Bei randlastigen Verteilungen ("fat tails") kommen Extremereignisse viel h√§ufiger vor als bei Normalverteilungen. Deshalb ist es wichtig sein, zu wissen, ob eine Normalverteilung oder eine randlastige Verteilung vorliegt. Viele statistische Methoden sind nicht zuverl√§ssig bei (stark) randlastigen Methoden. @fig-fat grenzt eine Normalverteilung von einer "Fat-Tail-Verteilung" ab.


```{r Normalverteilung-9, fig.asp=0.5}
#| echo: false
#| fig-cap: Normalverteilung vs. randlastige Verteilungen
#| label: fig-fat
ggplot(data = data.frame(x = c(-3, 3)), aes(x)) +
  stat_function(fun = dnorm, n = 101, args = list(mean = 0, sd = 1), color = "#E69F00FF") +
  labs(y = "Dichte", x = "Merkmal, X") +
  stat_function(fun = dt, n = 101, args = list(df = 1, ncp =0), color = "#56B4E9FF") +
  labs(caption = "orange: Normalverteilung\n blau: randlastige Verteilung (t-Verteilung mit df=1)")
```


:::::{#exm-normal-rand}
### Beispiele f√ºr Normal- und randlastige Verteilungen


:::: {.columns}

::: {.column width="50%"}
Normal verteilt:

- Gr√∂√üe 
- M√ºnzw√ºrfe
- Gewicht
- IQ
- Blutdruck
- Ausschuss einer Maschine
:::

::: {.column width="50%"}
Randlastig verteilt:

- Verm√∂gen
- Verkaufte B√ºcher (Anzahl)
- Ruhm (z.B. Anzahl Follower auf Instagram)
- Aktienkurse (Kurswert)
- Erdbeben (St√§rke)
- Anzahl von Todesopfern in Pandemien
- Anzahl von Todesopfern in Kriege
- Erfolg auf Tinder (Anzahl erfolgreicher Matches)
- Meteroritengr√∂√üe (Volumen)
- Stadtgr√∂√üen (Einwohnerzahl)

:::

::::

:::::





#### Formel der Normalverteilung

Vereinfacht ausgedr√ºckt l√§sst die Normalverteilung $\mathcal{N}$ durch Exponenzieren einer Quadratfunktion beschreiben:

$$\mathcal{N} \propto e^{-x^2}$$

mit $e=2.71...$, der Eulerschen Zahl.^[Das Zeichen $y \propto x$ bedeutet "x ist proportional zu y", also $y = mx$.]


Wie man sieht (@fig-normal1) ergibt sich eine Normalverteilung. 


```{r norm, eval = FALSE, echo = TRUE}
d <-
  tibble(
    x = seq(-3, 3, 
            length.out = 100),
    y = exp(-x^2)
  )

ggline(d, x = "x",y = "y")  # aus {ggpubr}
```



```{r Normalverteilung-10, echo = FALSE}
#| fig-asp: 0.5
#| label: fig-normal1
#| fig-cap: "Wir basteln uns eine Normalverteilung"
d <-
  tibble(
    x = seq(-3, 3, 
            length.out = 100),
    y = exp(-x^2)
  )

d %>% 
  ggplot() +
  aes(x = x, y = y) +
  geom_line()
```


Eine Normalverteilung mit $\mu=0$ und $\sigma=1$ nennt man auch *Standardnormalverteilung* und 
man schreibt:


$$IQ \sim \mathcal{N}(0,1)$$

Die Normalverteilung wird auch *[Gauss](https://de.wikipedia.org/wiki/Carl_Friedrich_Gau%C3%9F)-Verteilung* oder *Glockenkurve* genannt.





#### Simulation einer Normalverteilung


R hat eine Funktion eingebaut zur Erzeugung von Zufallszahlen (Zufallszahlengenerator), z.B. normalverteilte.
Man √ºbergibt dieser Funktion den gew√ºnschten Mittelwert und die gew√ºnschte Streuung und die Funktion zieht dann zuf√§llig Werte aus dieser Verteilung.

Diesen Zufallszahlengenerator kann man mit einem Duschkopf vergleichen, s. @fig-shower.
An diesem Duschkopf kann man einen Schwenker einstellen, der den Duschkopf
ausrichtet, also steuert, ob die Wassertropfen weit in die eine oder die andere
Richtugn fallen.
Zweitens hat unser Duschkopf noch einen Streuregler,
der den Wasserstrahl entweder eng b√ºndelt^[Massagedusche, behauptet der Hersteller] oder weit auseinanderf√§chert. Im ersten Fall f√§llt der Wasserstrahl eng und schmal aus. Im zweiten Fall f√§llt der Wasserstrahl breit aus.

![Zufallszahlengenerator als Duschkopf](img/shower-data.png){#fig-shower width="50%"}

[Quelle](https://jkkweb.sitehost.iu.edu/KruschkeFreqAndBayesAppTutorial.html#data_are_described_by_mathematical_models): John Kruschke.



Eine Zufallszahl (*r*andom number), die *norm*alverteilt ist, mit $\mu=0$ und $\sigma=1$ kann man in R so erzeugen:


```{r}
rnorm(n = 1, mean = 0, sd = 1)
```



Ein Fallbeispiel: Der Inhalt einer T√ºte mit Zucker, $X$, sei normalverteilt mit $\mu = 10002$ g und $\sigma=1.5$ g. Aus vertragsrechtlichen Gr√ºnden darf das F√ºllgewicht von 1000g nicht unterschritten werden, sonst drohen Konventionalstrafen.

Wie gro√ü ist die Wahrscheinlichkeit, dass 1000g unterschritten werden?


Simulieren wir uns 1e4 Zuckert√ºten!

```{r}
n <- 1e4
d <- 
  tibble(
    id = 1:n,
    x = rnorm(n = n, mean = 1002, sd = 1.5)
  )

head(d)
```


Z√§hlen wir, viele der Zuckert√ºten ein Gewicht von weniger als 1000g aufweisen:


```{r}
d %>% 
  count(x < 1000)
```


Ein ziemlich^["Ziemlich" ist nat√ºrlich subjektiv; je nach Situation kann es zu viel oder nicht zu viel sein.] kleiner Anteil. Rechnen wir uns noch die Anteile (*prop*ortion) aus:

```{r}
d %>% 
  count(x < 1000) %>% 
  mutate(prop = n/1e4)
```







### IQ-Verteilung

Die Verteilung der Zufallsvariablen IQ ist normalverteilt mit einem Mittelwert von 100 und einer Streuung von 15, s. @fig-norm-100-15:

$IQ \sim \mathcal{N}(100,15)$

:::{#exr-iq}
### Wie schlau muss man (nicht) sein?
- Wie schlau muss man sein, um zu den unteren 75%, 50%, 25%, 5%, 1% zu geh√∂ren?
- Anders gesagt: Welcher IQ-Wert wird von 75%, 50%, ... der Leute nicht √ºberschritten?$\square$
:::


![Visualisierung der theoretischen IQ-Verteilung](img/norm-100-15.png){#fig-norm-100-15 width="50%"}

[Quelle:](https://jkkweb.sitehost.iu.edu/KruschkeFreqAndBayesAppTutorial.html#data_are_described_by_mathematical_models): John Kruschke.


:::: {.columns}

::: {.column width="70%"}

Ziehen wir zuf√§llig $1e4$ Stichproben aus $\mathcal{N}(100,15)$ und berechnen die Quantile, s. @tbl-quantiles2.

```{r echo = TRUE}
d <-
  tibble(
  iq = rnorm(n = 1e4, 
             mean = 100, 
             sd = 15))

probs <- c(0.75,.5,.25,.05,.01)

d_summary <- d %>% 
  summarise(p = probs,
            q = quantile(iq, probs))
```

:::

::: {.column width="25%"}


```{r}
#| echo: false
#| tbl-cap: Quantile der IQ-Verteilung
#| label: tbl-quantiles2
d_summary %>% 
  gt() %>% 
  fmt_number(p, decimals = 2) %>% 
  fmt_number(q, decimals = 0)
```

:::

::::


Das *Quantil* $q$ zur kumulierten Wahrscheinlichkeit $p=75$ ist 110, etc. 






Umgekehrt k√∂nnen wir uns auch fragen: Gegeben einer Realisation der Zufallsvariablen (z.B. IQ), was ist die zugeh√∂rige Wahrscheinlichkeit (Wert der Verteilungsfunktion?)


:::{#exr-schlau2}
### Wie schlau muss man (nicht) sein, Teil 2
- Welcher Anteil der Fl√§che unter der Kurve $p$ geh√∂rt zu den IQ-Werten 75, 100, 115, 130?
- Anders gesagt: Welcher Anteil der Wahrscheinlichkeitsmasse der Verteilung liegt unter IQ=75, IQ=100, etc.?$\square$
:::


Ziehen wir Stichproben aus $\mathcal{N}(100,15)$. 
Was ist die Wahrscheinlichkeit f√ºr eine `iq < 100`?

```{r echo = TRUE, eval = FALSE}
d <-
  tibble(
    iq = rnorm(1e4, 
               mean = 100, 
               sd = 15)) %>% 
  mutate(iq = round(iq))

qs <- c(75,100,115,130)

d %>% 
  count(p_100 = iq < 100) %>% 
  mutate(prop = n / sum(n)) 
```


@tbl-iq100 zeigt uns die Antwort. 

:::{.callout-note}
Wir sch√§tzen die wahre, "theoretische" Wahrscheinlichkeit durch einfaches Ausprobieren: Wir f√ºhren das Zufallsexperiment einfach h√§ufig durch.
Dann z√§hlen wir den Anteil der Treffer. Nennt man auch "Simulieren"; klingt cooler als "Ausprobieren".ü§ì$\square$
:::


```{r echo = FALSE, eval = TRUE}
#| tbl-cap: Wahrscheinlichkeit f√ºr iq < 100
#| label: tbl-iq100
d <-
  tibble(
    iq = rnorm(1e4, 
               mean = 100, 
               sd = 15)) %>% 
  mutate(iq = round(iq))

qs <- c(75,100,115,130)

d %>% 
  count(p_100 = iq < 100) %>% 
  mutate(prop = n / sum(n)) %>% 
  gt() %>% 
  fmt_number(columns = 3)
```

Anstelle von `iq < 100` kann man `iq < 115` einsetzen, etc.
 
```{r eval= FALSE}
#| echo: false
d %>% 
  mutate(prop = percent_rank(iq)) %>% 
  filter(iq %in% qs) %>% 
  distinct(iq, .keep_all = TRUE)
```

Die *Verteilungsfunktion* (der Anteil der Wahrscheinlichkeitsmasse), `p`, f√ºr IQ-Werte nicht gr√∂√üer als 100,  $IQ\le100$, ist 50%, etc. 



#### Quantile der Normalverteilung 




üí° Zur Erinnerung: *Quantile* teilen eine Verteilung so ein, dass ein Anteil $p$ kleiner oder gleich und der andere Teil $1-p$ gr√∂√üer  dem Quantil $q$ ist.


:::{#exm-quantil} 
"50%-Quantil = 100" meint, dass 50% der Elemente der Verteilung einen Wert kleiner oder gleich als 100 haben.
Man schreibt auch: `q(.5) = 100`.
:::


üí°  Zur Erinnerung: Die *Verteilungsfunktion F* (f√ºr einen Wert $x$ der Zufallsvariable $X$) gibt die Wahrscheinlichkeit an, dass $X$ einen Wert h√∂chstens so gro√ü wie $x$ annimmt. Sie zeigt also die kumulierte Wahrscheinlichkeit $[-\infty, q)$.


:::{#exm-f}
"F(100) = 50%" meint: Die Wahrscheinlichkeit f√ºr eine Auspr√§gung von h√∂chstens als 100 betr√§gt 50%.$\square$
:::


Schauen wir uns die Quantile der Normalverteilung einmal n√§her an.
Wir gehen von einer Normalverteilung aus, wie sie zur Beschreibung von Intelligenz (IQ) verwendet wird, s. @fig-nv-quants.


```{r }
#| echo: false
#| label: fig-nv-quants
#| fig-cap: Quantile der Normalverteiltung

  

p1 <- 
  ggplot(NULL, aes(c(60,145))) +
  geom_area(stat = "function", fun = dnorm, fill = "grey", args = list(mean=100,sd = 15), xlim= c(60, 100)) +
  geom_line(stat = "function", fun = dnorm, args = list(mean=100,sd = 15)) +
  labs(x = "X", y = "Dichte",
       title = "50%-Quantil: 100;\nVerteilungsfunktion von 100:50%") +
  scale_y_continuous(breaks = NULL)

p2 <-
    ggplot(NULL, aes(c(60,145))) +
  geom_area(stat = "function", fun = dnorm, fill = "grey", args = list(mean=100,sd = 15), xlim= c(60, 125)) +
  geom_line(stat = "function", fun = dnorm, args = list(mean=100,sd = 15)) +
  labs(x = "X", y = "Dichte",
       title = "95%-Quantil: 125;\nVerteilungsfunktion von 125:95%") +
  scale_y_continuous(breaks = NULL)


plots(p1, p2)
```

$$IQ \sim \mathcal{N}(100, 15)$$
Mit R kann man sich die beiden Gr√∂√üen komfortabel berechnen lassen:

```{r echo = TRUE, eval = FALSE}
qnorm(.50, mean = 100, sd = 15)  # 50%-Quantil
pnorm(100, mean = 100, sd = 15)  # Verteilungsfunktion f√ºr IQ=100
```


Betrachten wir einige wichtigen Quantile, s. @fig-nv-quants2.

```{r}
#| echo: false
q_p50 <-
  ggplot(NULL, aes(c(60,145))) +
  geom_area(stat = "function", fun = dnorm, fill = "grey", args = list(mean=100,sd = 15), xlim= c(60, 100)) +
  geom_line(stat = "function", fun = dnorm, args = list(mean=100,sd = 15)) +
  labs(x = "IQ", y = "Dichte",
       title = "50%-Quantil: 100") +
  scale_y_continuous(breaks = NULL)
```


```{r}
#| echo: false
q_inv <- .25
q_p <- qnorm(q_inv, mean = 100, sd= 15)
p_q <- pnorm(q_p, mean = 100, sd= 15)

q_p25 <-
  ggplot(NULL, aes(c(60,145))) +
  geom_area(stat = "function", fun = dnorm, fill = "grey", args = list(mean=100,sd = 15), xlim= c(60, q_p)) +
  geom_line(stat = "function", fun = dnorm, args = list(mean=100,sd = 15)) +
  labs(x = "IQ", y = "Dichte",
       title = paste0(q_inv,"-Quantil: ",round(q_p)),
       caption = "MW-0.68sd") +
  scale_y_continuous(breaks = NULL)
```


```{r}
#| echo: false
q_inv <- .95
q_p <- qnorm(q_inv, mean = 100, sd= 15)

q_p95 <-
  ggplot(NULL, aes(c(60,145))) +
  geom_area(stat = "function", fun = dnorm, fill = "grey", args = list(mean=100,sd = 15), xlim= c(60, q_p)) +
  geom_line(stat = "function", fun = dnorm, args = list(mean=100,sd = 15)) +
  labs(x = "IQ", y = "Dichte",
       title = paste0(q_inv,"-Quantil: ",round(q_p)),
       caption = "MW+1.64sd") +
  scale_y_continuous(breaks = NULL)
```



```{r}
#| echo: false
q_inv <- .975
q_p <- qnorm(q_inv, mean = 100, sd= 15)
#pnorm(115, mean= 100, sd = 15)

q_p975 <-
  ggplot(NULL, aes(c(60,145))) +
  geom_area(stat = "function", fun = dnorm, fill = "grey", args = list(mean=100,sd = 15), xlim= c(60, q_p)) +
  geom_line(stat = "function", fun = dnorm, args = list(mean=100,sd = 15)) +
  labs(x = "IQ", y = "Dichte",
       title = paste0(q_inv,"-Quantil: ",round(q_p)),
       caption = "MW+2SD") +
  scale_y_continuous(breaks = NULL)
```





```{r}
#| echo: false
q_inv <- .84
q_p <- qnorm(q_inv, mean = 100, sd= 15)
#pnorm(115, mean= 100, sd = 15)

q_p84 <-
  ggplot(NULL, aes(c(60,145))) +
  geom_area(stat = "function", fun = dnorm, fill = "grey", args = list(mean=100,sd = 15), xlim= c(60, q_p)) +
  geom_line(stat = "function", fun = dnorm, args = list(mean=100,sd = 15)) +
  labs(x = "IQ", y = "Dichte",
       title = paste0(q_inv,"-Quantil: ",round(q_p)),
       caption = "MW+1sd") +
  scale_y_continuous(breaks = NULL)
```

```{r}
#| echo: false
q_inv <- .69
q_p <- qnorm(q_inv, mean = 100, sd= 15)  # halbe SD
#pnorm(107.5, mean= 100, sd = 15)

q_p69 <-
  ggplot(NULL, aes(c(60,145))) +
  geom_area(stat = "function", fun = dnorm, fill = "grey", args = list(mean=100,sd = 15), xlim= c(60, q_p)) +
  geom_line(stat = "function", fun = dnorm, args = list(mean=100,sd = 15)) +
  labs(x = "IQ", y = "Dichte",
       title = paste0(q_inv,"-Quantil: ",round(q_p)),
       caption = "MW+0.5sd") +
  scale_y_continuous(breaks = NULL)
```



```{r fig-nv-quants2}
#| echo: false
#| label: fig-nv-quants2
#| fig-width: 10
#| fig-cap: Verschiedene Quantil der Normalverteilung

(q_p50 + q_p25 + q_p69) / (q_p95 + q_p975 + q_p84)
```


#### Standardnormalverteilung


:::: {.columns}

::: {.column width="50%"}
```{r Normalverteilung-3}
#| echo: false
#| fig-cap: "Normalverteilung mit Mittelwert 0 und SD 1, auch *Standard-Normalverteilung* genannt"
#| label: fig-snv
ggplot(data = data.frame(x = c(-3, 3)), aes(x)) +
  stat_function(fun = dnorm, n = 101, args = list(mean = 0, sd = 1)) +
  labs(y = "Dichte", x = "Merkmal, X") +
  theme_modern()
```

:::

::: {.column width="50%"}
Bei $X=0$ einer *Standard-Normalverteilung* (s. @fig-snv) gilt:

- hat eine Einheit von $X$ die Wahrscheinlichkeitsmasse von 40% (Wahrscheinlichkeitsdichte)
- sind 50% der Wahrscheinlichkeitsmasse (Fl√§che unter der Kurve) kleiner als dieser Wert (Verteilungsfunktion).

In Summe liegen 100% der Wahrscheinlichkeitsmasse unter der Kurve.
:::

::::






#### Normalverteilung als konservative Wahl

Dem Mathematiker [Carl Friedrich Gauss](https://de.wikipedia.org/wiki/Carl_Friedrich_Gau%C3%9F) (s. @fig-gauss) wird die Ehre zuerkannt,
die Normalverteilung eingef√ºhrt zu haben.

```{r Normalverteilung-1-bis, echo = FALSE }
#| fig-align: "center"
#| label: fig-gauss
#| fig-cap: "Zehn-Mark-Geldschein mit Gauss und Normalverteilung"
knitr::include_graphics("img/10_Deutsche_Mark_-_detail.png")
```

Quelle: Uni Greifswald, Public domain, via Wikimedia Commons


::: callout-note
*Ontologische Begr√ºndung*

- Wirken viele, gleichstarke Einfl√ºsse additiv zusammen, entsteht eine Normalverteilung  [@mcelreath_statistical_2020], Kap. 4.1.4.

*Epistemologische Begr√ºndung*

- Wenn wir nur wissen, dass eine Variable √ºber einen endlichen Mittelwert und eine endliche Varianz verf√ºgt und wir keine weiteren Annahmen treffen bzw. √ºber kein weiteres Vorwissen verf√ºgen, dann ist die Normalverteilung die plausibelste Verteilung (maximale Entropie) [@mcelreath_statistical_2020], Kap. 7 und 10.
:::


<!-- ### Zweidimensionale Normalverteilung, unkorreliert -->


<!-- ```{r Normalverteilung-11, out.width="70%", fig.align="center"} -->
<!-- #| echo: false -->
<!-- knitr::include_graphics("https://github.com/sebastiansauer/QM2-Folien/raw/main/img/mult-norm.png") -->
<!-- ``` -->


<!-- [Quelle](https://tex.stackexchange.com/questions/31708/draw-a-bivariate-normal-distribution-in-tikz) -->

<!-- [Vgl. auch dieses Diagramm](http://ballistipedia.com/index.php?title=File:Bivariate.png)] -->




<!-- ### 2D-Normalverteilung mit R, unkorreliert -->

<!-- $r(X,Y) = 0$ -->




<!-- ```{r norm-plot1} -->
<!-- #| echo: false -->
<!-- #| eval: false -->
<!-- #| fig-asp: 1 -->
<!-- #| out-width: "50%" -->
<!-- d1 <-  -->
<!--   tibble(  -->
<!--     x=rnorm(1e4),  -->
<!--     y=rnorm(1e4) -->
<!--   ) -->

<!-- ggplot(d1) + -->
<!--   aes(x, y) + -->
<!--   geom_hex()  -->

<!-- ggplot(d1) + -->
<!--   aes(x, y) + -->
<!--   geom_density2d() -->
<!-- ``` -->

<!-- [[ggplot-Referenz](https://ggplot2.tidyverse.org/reference/geom_density_2d.html), [Quellcode](https://www.r-graph-gallery.com/2d-density-plot-with-ggplot2.html)] -->

<!-- Mit `scale_fill_continuous(type = "viridis")`kann man die Farbpalette der F√ºllfarbe √§ndern. Nicht so wichtig. -->


<!-- ```{r Normalverteilung-2-bis} -->
<!-- #| echo: false -->
<!-- #| eval: true -->
<!-- #| fig-asp: 1 -->
<!-- #| out-width: "50%" -->
<!-- d1 <-  -->
<!--   tibble(  -->
<!--     x=rnorm(1e4),  -->
<!--     y=rnorm(1e4) -->
<!--   ) -->

<!-- ggplot(d1) + -->
<!--   aes(x, y) + -->
<!--   geom_hex() + -->
<!--   scale_fill_viridis_c() + -->
<!--   theme(legend.position = "bottom") -->

<!-- ggplot(d1) + -->
<!--   aes(x, y) + -->
<!--   geom_density2d() -->
<!-- ``` -->



<!-- ### 2D-Normalverteilung mit R, korreliert, r=0.7 -->



<!-- Die ersten paar Zeilen der Daten: -->

<!-- ```{r Normalverteilung-3-bis, echo = FALSE} -->
<!-- d2 <- rnorm_multi( -->
<!--   n = 1e4, -->
<!--   mu = c(0,0), -->
<!--   sd = c(1, 1), -->
<!--   r = (0.7) -->
<!-- ) -->
<!-- ``` -->

<!-- ```{r Normalverteilung-4-bis} -->
<!-- #| echo: false -->
<!-- d2 %>%  -->
<!--   head(n=3) %>%  -->
<!--   gt() %>%  -->
<!--   fmt_number(columns = everything()) -->
<!-- ``` -->

<!-- Berechnen wir die Korrelation `r`: -->

<!-- ```{r Normalverteilung-5-bis, echo = TRUE, eval = FALSE} -->
<!-- d2 %>%  -->
<!--   summarise( -->
<!--     r = cor(X1,X2), -->
<!--     n = n() -->
<!--   ) -->
<!-- ``` -->

<!-- ```{r Normalverteilung-6-bis} -->
<!-- #| echo: false -->
<!-- d2 %>%  -->
<!--   summarise( -->
<!--     r = cor(X1,X2), -->
<!--     n = n() -->
<!--   ) %>%  -->
<!--   gt() %>%  -->
<!--   fmt_number(columns = everything()) -->
<!-- ``` -->




<!-- ```{r Normalverteilung-7-bis} -->
<!-- #| echo: false -->
<!-- ggplot(d2) + -->
<!--   aes(X1, X2) + -->
<!--   geom_hex() + -->
<!--   scale_fill_viridis_c() + -->
<!--   theme(legend.position = "bottom") -->

<!-- ggplot(d2) + -->
<!--   aes(X1, X2) + -->
<!--   geom_density2d() -->
<!-- ``` -->






## Vertiefung

@bourier_2018, Kap. 6.2 und 7.1 erl√§utert einige (grundlegende) theoretische Hintergr√ºnde zu diskreten Zufallsvariablen und Wahrscheinlichkeitsverteilungen. Wichtigstes Exemplar ist dabei die Binomialverteilung.

@mittag_statistik_2020 stellen in Kap. 12 und 13 Zufallsvariablen vor; zum Teil geht die Darstellung dort √ºber die Lernziele bzw. Inhalte dieses Kurses hinaus.


## Aufgaben


Zus√§tzlich zu den Aufgaben in der genannten Literatur sind folgende Aufgaben zu empfehlen.

- [Bsp-Binomial](https://datenwerk.netlify.app/posts/bsp-binomial/bsp-binomial)
- [wuerfel01](https://datenwerk.netlify.app/posts/wuerfel01/wuerfel01.html)
- [wuerfel02](https://datenwerk.netlify.app/posts/wuerfel02/wuerfel02.html)
- [wuerfel03](https://datenwerk.netlify.app/posts/wuerfel03/wuerfel03.html)
- [wuerfel04](https://datenwerk.netlify.app/posts/wuerfel04/wuerfel04.html)

- [iq01](https://datenwerk.netlify.app/posts/iq01/iq01)
- [iq02](https://datenwerk.netlify.app/posts/iq02/iq02)
- [iq03](https://datenwerk.netlify.app/posts/iq03/iq03)
- [iq04](https://datenwerk.netlify.app/posts/iq04/iq04)
- [iq05](https://datenwerk.netlify.app/posts/iq05/iq05)
- [iq06](https://datenwerk.netlify.app/posts/iq06/iq06)
- [iq07](https://datenwerk.netlify.app/posts/iq07/iq07)
- [iq08](https://datenwerk.netlify.app/posts/iq08/iq08)
<!-- - [iq09](https://datenwerk.netlify.app/posts/iq09/iq09) -->
<!-- - [iq10](https://datenwerk.netlify.app/posts/iq010/iq10) -->
- [Bus1](https://datenwerk.netlify.app/posts/bus1/bus1)

- [alphafehler-inflation2](https://datenwerk.netlify.app/posts/alphafehler-inflation2/alphafehler-inflation2.html)
- [alphafehler-inflation3](https://datenwerk.netlify.app/posts/alphafehler-inflation3/alphafehler-inflation3.html)
- [alphafehler-inflation4](https://datenwerk.netlify.app/posts/alphafehler-inflation4/alphafehler-inflation4.html)


## ---



![](img/outro-04.jpg){width=100%}




