# Verteilungen

<!-- TODO Dieses Kapitel hat nicht nur Verteilungen,  -->
<!-- sondern auch Stoff zum Thema Wahrscheinlichkeit, -->
<!-- das sollte entflechtet werden. -->


## Lernsteuerung


### Position im Modulverlauf

@fig-modulverlauf gibt einen √úberblick zum aktuellen Standort im Modulverlauf.


### Lernziele

Nach Absolvieren des jeweiligen Kapitels sollen folgende Lernziele erreicht sein.

Sie k√∂nnen ...


- den Begriff der Zufallsvariablen erl√§utern
- die Begriffe von Wahrscheinlichkeitsdichte und Verteilungsfunktion erl√§utern 
- den Begriff einer Gleichverteilung erl√§utern 
- den Begriff einer Binomialverteilung erl√§utern
- die Parameter einer Normalverteilung nennen und erl√§utern
- zentrale Konzepte in R umsetzen



### Begleitliteratur

Der Stoff dieses Kapitels deckt sich (weitgehend) mit @bourier2011, Kap. 6.1 und 6.3 sowie 7.1 und und 7.2.



### Vorbereitung im Eigenstudium

Dieses Kapitel setzt einige Grundbegriffe voraus,
wie im Buch [Statistik1]() vorgestellt, insbesondere im Kapitel ["Rahmen"](https://statistik1.netlify.app/010-rahmen#was-sind-daten).
Ben√∂tigt wird auch der Begriff der [Normalverteilung](https://statistik1.netlify.app/040-verbildlichen#spezialfall-normalverteilung) sowie der Begriff der [Quantile](https://statistik1.netlify.app/050-zusammenfassen.html#quantile).

Lesen Sie selbst√§ndig, zus√§tzlich  zum Stoff dieses Kapitels, noch in @bourier2011 folgende Abschnitte:

<!-- - Kap. 6.1 (Zum Begriff Zufallsvariable) -->
<!-- - Kap. 6.3 (Stetige Zufallsvariablen) -->
- Kap. 7.1.1 (Binomialverteilung)
- Kap. 7.2.1 (Gleichverteilung)
- Kap. 7.2.3 (Normalverteilung)

L√∂sen Sie auch die √úbungsaufgaben dazu.


Weitere √úbungsaufgaben finden Sie im dazugeh√∂rigen √úbungsbuch, @bourier2022.


### Pr√ºfungsrelevanter Stoff

Beachten Sie, dass neben den Inhalten des Kapitels auch stets der vorzubereitende Stoff pr√ºfungsrelevant ist.


### Ben√∂tigte R-Pakete

```{r}
#| message: false
library(tidyverse)
library(ggpubr)  # f√ºr Plots
```


```{r}
#| include: false
library(gt)
library(patchwork)
#library(faux)
library(openintro)
library(easystats)
library(ggraph)
library(knitr)

```




```{r r-setup}
#| echo: false
#| message: false
theme_set(theme_minimal())
#scale_color_okabeito()
scale_colour_discrete <- function(...) 
  scale_color_okabeito()
```





### Zentrale Begriffe




#### Eigenschaften von Zufallsvariablen

- Zufallsvariable (random variable)
- Diskret vs. stetig
- Wahrscheinlichkeitsdichte (Dichte, (probability) density, f)
- Wahrscheinlichkeitsfunktion (kumulierte Wahrscheinlichkeit, Wahrscheinlichkeitsmasse)


#### Verteilungen

- Gleichverteilung
- Normalverteilung
- Standardnormalverteilung

### Begleitvideos


- [√úberblick zu Verteilungen](https://youtu.be/7GqIE4sKDs4)
- [Gleichverteilung](https://youtu.be/CJxi4e89aDs)
- [Binomialverteilung](https://youtu.be/xe7JL3fEKsg)



## Wichtige Verteilungen

Im Folgenden sind einige wichtige Verteilungen aufgef√ºhrt, die in diesem Skript (und in der Statistik und Wahrscheinlichkeitstheorie) eine zentrale Rolle spielen.

üì∫ [Einstieg in Verteilungen](https://www.youtube.com/watch?v=HKWwondYsW8&list=PLRR4REmBgpIGgz2Oe2Z9FcoLYBDnaWatN&index=5)

## Gleichverteilung

### Indifferenz als Grundlage

Eine Gleichverteilung nimmt an, dass jeder Wert im Ergebnisraum der zugeh√∂rigen Zufallsvariable *gleichwahrscheinlich* ist.
Wenn man keinen hinreichenden Grund hat, eine Realisation einer Zufallsvariablen f√ºr plausibler als einen anderen zu halten,
ist eine Gleichverteilung eine passende Verteilung.
Gleichverteilungen gibt es im diskreten und im stetigen Fall.

Abb. @fig-uniform zeigt ein Beispiel f√ºr eine (stetige) Gleichverteilung.


```{r Normalverteilung-4, fig.asp = 0.5}
#| echo: false
#| fig-cap: "Stetige Gleichverteilung; man beachte jeweils die Y-Achse"
#| fig-subcap: 
#|   - "Beispiel a: Gleichverteilung min=-1, max=1. Dichte: 1/2"
#|   - "Beispiel b: Gleichverteilung min=0, max=3. Dichte: 1/3"
#| label: fig-uniform
#| layout-ncol: 2
#
#source: https://dk81.github.io/dkmathstats_site/rmath-uniform-plots.html

uniform_Plot(-1, 1)
uniform_Plot(0, 3)
```




@fig-uniform, links: Bei $X=0$ hat *eine Einheit* von $X$ (d.h. von -0.5 bis +0.5) die Wahrscheinlichkeitsmasse von 50%, 
da der Bereich $[-0.5, +0.5]$ die H√§lfte (50%) der Wahrscheinlichkeitsmasse der Verteilung beinhaltet. 
Bei jedem anderen Punkt $x$ ist die Dichte gleich.
@fig-uniform, rechts: Bei $X=0$ hat *eine Einheit* von $X$ die Wahrscheinlichkeitsmasse von ca. 33%, 
da der Bereich $[-0.5, +0.5]$ ein Drittel der Wahrscheinlichkeitsmasse der Verteilung beinhaltet. 
Bei jedem anderen Punkt $x$ ist die Dichte gleich.
Definierendes Kennzeichen einer Gleichverteilung ist die *konstante Dichte*.



### Simulation


M√∂chte man die Verteilungsfunktion einer stetigen Zufallsvariablen berechnen,
kann die Mathe ganz sch√∂n kompliziert werden, schlie√ülich muss man Integrale l√∂sen.
Aber es gibt einen Trick, wie man die Sache stark vereinfachen kann: 
man simuliert die Verteilung. Was bedeutet das?



Angenommen, die Wartezeit auf einen Bus ist gleichverteilt (engl. *uniform distribution*); 
der Bus kommt regelm√§√üig und p√ºnktlich alle 10 Minuten. 
Die minimale Wartezeit betr√§gt also 0 Minuten und die maximale 10 Minuten.
Nennen wir die zugeh√∂rige Zufallsvariable $X$, das ist sch√∂n kurz zu schreiben.

Eine gleichverteilte Zufallsvariable $X$ mit Min $m_0$ und Maximum $m_1$ schreibt man auch wie folgt in Kurzschreibweise:

$$X \sim Unif(m_0,m_1).$$



Ja, das sieht fancy aus, ist aber daf√ºr sch√∂n kurz, aber wo ist der versprochene Trick zum Vereinfachen?
Kommt gleich, Moment.

Eine Frage k√∂nnte nun lauten, wie gro√ü ist die Wahrscheinlichkeit, dass man zwischen 3 und 5 Minuten auf den Bus warten muss?
Achtung: Hier ist der Trick. N√§mlich, dass wir Integralrechnung gegen stumpfes Z√§hlen eintauschen.

Computer (und damit R) haben eingebaute Funktionen, die eine beliebige Zufallszahl ziehen k√∂nnen,
zum Beispiel gleichverteilte.
Auf Errisch hei√üt das Zauberwort `runif()`:
Mit dieser Funktion kann man gleichverteilte Zufallszahlen ziehen.
Einfach gesprochen: Der Computer greift in eine S√§ckchen mit Murmeln, die mit verschiedenen Zahlen beschriftet sind, wobei alle Zahlen gleich h√§ufig sind, und greift eine heraus.


```{r}
#| eval: false
set.seed(42)  # Zufallszahl festlegen, nur f√ºr Reproduzierbarkeit
# "r" wie random, "unif" wie "uniform" (gleich):
runif(n = 1, min = 0, max = 10) 
```

Auf Deutsch hei√üt das: 

> üë®‚Äçüè´   "Hey R, ich h√§tte gerne eine (daher `n = 1`) Zufallszahl (*r* wie *random*),
die gleichverteilt ist (*uniform*) mit `min = 0` und `max = 10`.

>   ü§ñ Jawohl, oh herrliches Leberwesen



(Zu) anschaulich gesprochen: R hat den Bus kommen lassen und es hat gut 9.1 Minuten gedauert,
bis er da war.
Achtung, jetzt kommt's: Jetzt lassen wir R mal $10^5$ (`1e5` auf Computersprech) Busse vorfahren. 
R soll jedes Mal notieren, wie lange man auf den Bus warten musste.^[Machen Sie das mal ohne Computer, wenn Sie ein Wochenende lang Langeweile haben.]




```{r}
#| eval: false
x_simu <- runif(n = 1e5, min = 0, max = 10)
```

```{r}
#| echo: false
n <- 1e5
set.seed(42)
x_simu <- runif(n = n, min = 0, max = 10)  # gibt Vektor zur√ºck

x_simu_df <-
  tibble(id = 1:n,
         x = x_simu)
```



Schauen wir uns die Verteilung an, s. @fig-simu-gleichvert.^[Alternativ kann man z.B. auch `ggplot` verwenden: `ggplot(x_simu_df, aes(x = x_simu)) +  geom_histogram(bins = 50)`.]

```{r}
#| eval: false
library(ggpubr)
gghistogram(x_simu_df, x = "x_simu", fill = "grey20")
```




```{r}
#| label: fig-simu-gleichvert
#| fig-cap: "Simulation einer gleichverteiluten Zufallsvariablen"
#| eval: true
#| echo: false

library(ggpubr)
gghistogram(x_simu_df, x = "x_simu", fill = "grey20")
```






Okay, unsere Verteilung sieht nicht *exakt* gleichverteilt, aber einigerma√üen. 
Gut genug f√ºr unsere Zwecke!

So, und jetzt kommt das Ernten.
Wir k√∂nnen jetzt n√§mlich einfach z√§hlen (`count()`), um die Antwort auf unsere Frage (der Wartezeit 3-5 Min.) zu erhalten, s. @tbl-count-simu-bus.


:::: {.columns}

::: {.column width="50%"}
```{r}
#| eval: false
x_simu_df %>% 
  count(Schnittmenge = x > 3 & x < 5)
```

:::

::: {.column width="50%"}
```{r}
#| echo: false
#| tbl-cap: H√§ufigkiten auslesen anstelle von Integralen berechnen
#| label: tbl-count-simu-bus
x_simu_df %>% 
  count(Schnittmenge = x > 3 & x < 5)
```

:::

::::




Das Zeichen `&` ist das logische UND, also die Schnittmenge der zwei Mengen $A$ ($X$ ist gr√∂√üer als 3) und $B$ ($X$ ist kleiner als 5), d.h. $A := \{x|x>3\}$ und $B := \{x|x<5\}$, 
also $A \cap B$.

Wie man sieht, fallen ca. 20% der Stichproben in den entsprechenden Bereich. 


Da viele Probleme, wenn sie komplexer werden, kaum noch "analytisch" (d.h. wie Ausrechnen von Integralen) l√∂sbar sind,
greift man in der modernen (Analyse-)Welt oft lieber auf Simulationsverfahren zur√ºck -- Dank sei den schnellen Rechnern.
F√ºr uns Menschen ist damit die Aufgabe des Integrierens auf schn√∂des Z√§hlen zur√ºckgef√ºhrt.







## Binomialverteilung {#sec-bin-distrib}

### Grundlagen

:::{#def-binvert}
### Binomialverteilung
Die Binomialverteilung dient zur Darstellung der Wahrscheinlichkeit der Ergebnisse eines $n$-fach wiederholten binomialen Zufallexperiments,
eines Zufallsexperiments mit *zwei*^[von lat. *bis* "zweimal"] Ergebnissen bzw. Elementarereignissen also. 
Dabei interessiert die *Anzahl* der $k$ Treffer, aber nicht die Reihenfolge.
Bei jeder Wiederholung liegt die Wahrscheinlichkeit eines Treffers bei $p$.
Typisches Beispiel ist ein (wiederholter) M√ºnzwurf.^[Bei jeder Wiederholung des Zufallexperiments 
bleibt die Wahrscheinlichkeit der Ergebnisse gleich: Die M√ºnze ver√§ndert sich nicht durch die W√ºrfe (*Ziehen mit Zur√ºcklegen*, ZmZ). 
Au√üerdem hat ein bestimmtes Ergebnis im ersten Wurf keinen Einfluss auf die Wahrscheinlichkeit 
eines bestimmten Ergebnisses im zweiten Wurf, etc., sog. "iid": *independent and identically distributed*.] $\square$ 
:::



$$\overbrace{X}^{\text{Unsere Zufallsvariable}} \underbrace{\sim}_{\text{ist verteilt nach der}} \underbrace{\overbrace{\text{Bin}(n, p)}^{\text{Binomialverteilung}}}_{\text{mit den Parametern n, p}}$${#eq-binvert}


<!-- TODO over, underbrace wie hier: https://chrispiech.github.io/probabilityForComputerScientists/en/part2/binomial/ -->



F√ºr eine binomialverteilte Zufallsvariable $X$ schreibt man kurz wie in @thm-binvert gezeigt.

::: {#thm-binvert}

### Notation f√ºr eine binomialverteilte Zufallsvariable

$$X \sim \text{Bin}(n, k) \quad \square$$
:::



:::{#exm-binom1}
Anwendungsbeispiele: Wie viele defekte Teile sind in einer Stichprobe von produzierten Schrauben zu erwarten? Wie wahrscheinlich ist es, dass das neue Blutdruck-Medikament einer bestimmten Anzahl von Menschen hilft? Wie viele Personen stimmen in einer Umfrage der Frage "Ich halte die √∂ffentlich-rechtlichen Sender f√ºr wichtig." zu? $\square$
:::

Stellen wir uns eine Kistchen^[In den Lehrb√ºchern h√§ufig als Urne bezeichnet, was den b√∂sen Spott von "Friedhofstatistik" nach sich zog.] mit sehr vielen^[praktisch unendlich vielen] Losen vor, 
darunter 2/5 *T*reffer (Gewinn) und 3/5 *N*ieten, s. Abb. @fig-urne.
Der Versuch l√§uft so ab: Wir ziehen ein Los, schauen ob es ein Treffer ist oder nicht, legen es zur√ºck und ziehen erneut.^[Da *sehr* viele Lose im K√§stchen liegen, ist es praktisch egal, ob wir das Los wieder zur√ºcklegen. Die Wahrscheinlichkeit f√ºr einen Treffer √§ndert sich (so gut wie) nicht.] 
Jetzt ziehen wir z.B. drei Lose.
Wie gro√ü ist die Wahrscheinlichkeit, 
davon 2 Treffer zu erzielen (egal in welcher Reihenfolge)?



```{r echo = FALSE}
#| fig-cap: Ein Losk√§stchen mit 2/5 Treffer und 3/5 Nieten
#| label: fig-urne
#| fig-asp: 0.3
#| echo: false  



d <- tibble(id = 1:5,
         event = c("T", "T", "N", "N", "N"))

p_urn <- 
  ggplot(d) +
  aes(x = id, label = id) +
  theme_minimal() +
  annotate("rect", xmin = 0, xmax = 6, ymin = 0, ymax = 2, fill = "grey80", alpha = .8) +
  geom_point(size = 10, y = 1, aes(color = event) )+
  geom_text(aes(label = id), y = 1) +
  theme(axis.text = element_blank()) +
  labs(x = "", color = "") +
  scale_color_okabeito()

d2 <-
  tibble(id = 1:1e3,
         Treffer = sample(0:1, 1e3, replace = TRUE, prob = c(3/5, 2/5)))  |> 
  mutate(Treffer = ifelse(Treffer == 1, "Treffer", "Niete"))

p_urn2 <- 
  ggplot(d2) +
  aes(color = Treffer) +
  geom_jitter(aes(x = 1, y = 1)) +
  scale_color_okabeito() +
  theme_void()

p_urn2
  
```



Praktischerweise ist die Binomialverteilung in R eingebaut,  
hier ist Pseudocode f√ºr Ihre Anwendung, s. @lst-binomial.

```{r}
#| lst-cap: "R-Pseudocode f√ºr die Binomialverteilung"
#| lst-label: lst-binomial
#| eval: false
dbinom(x = <Anzahl der Treffer>, 
       size = <Anzahl der W√ºrfe>, 
       prob = <Wahrscheinlichkeit eines Treffers>)
```


### M√∂glichkeiten z√§hlen


:::{#exm-bin1}
### Drei Lose gekauft, davon zwei Treffer?

Wie gro√ü ist die Wahrscheinlichkeit $p$ bei $n=3$ Z√ºgen $k=2$ Treffer zu erzielen (und $n-k=1$ Niete)? 
(Nennen wir dieses Ereignis der K√ºrze halber $A^{\prime}$).
Die Trefferwahrscheinlichkeit ist (bei jedem Zug) $p=2/5$ und die 
Nietenwahrscheinlichkeit $1-p=3/5 \quad \square$.
:::


:::{.to-scource}
```{r}
df_binom <-
  tibble(
    k = 0:3,
    p = dbinom(0:3, size = 3, prob = 2/5))
```
:::



Mit Blick auf @exm-bin1: 
Wir k√∂nnten jetzt ein Baumdiagramm zeichnen und pro Pfad die Wahrscheinlichkeit ausrechnen
(*Multiplikationssatz*, @thm-multtheorem), vgl. @fig-baum3.
Die Summe der Wahrscheinlichkeiten der Pfade ist dann die gesuchte Wahrscheinlichkeit (*Additionssatz*, @thm-add-disjunkt).
Diagramme zeichnen ist einfach, dauert aber.


Beachtet man die *verschiedenen Reihenfolgen nicht*  (in @fig-baum3), 
so z√§hlt man 3 g√ºnstige Pfade (vgl. @fig-sort3):

1. TTN
2. TNT
3. NTT.



```{r}
#| fig-cap: Wie viele M√∂glichkeiten gibt es, 3 Lose zu sortieren, von denen 2 Treffer sind und 1 Niete?
#| label: fig-sort3
#| fig-asp: 0.2
#| echo: false

d <- 
  tibble(id = 1:3,
         event = c("T", "T", "N"))

p_urn2 <- ggplot(d) +
  aes(x = id) +
  theme_minimal() +
  annotate("rect", xmin = 0, xmax = 4, ymin = 0, ymax = 2, fill = "grey80", alpha = .8) +
  geom_point(size = 10, y = 1, aes(color = event) )+
  geom_text(aes(label = event), y = 1) +
  theme(axis.text = element_blank()) +
  labs(x = "", color = "")

p_urn2
```




Wir haben also die M√∂glichkeiten (2 Treffer und 1 Niete zu erhalten)

- *ohne Beachtung der Reihenfolge* und
- *ohne Zur√ºcklegen* (der M√∂glichkeiten)

gez√§hlt.




```{mermaid}
%%| fig-cap: "Baumdiagramm f√ºr das Ziehen von 2 Treffern und 1 Niete"
%%| label: fig-baum3
flowchart LR
  A[Start] -->B[Zug 1 - T]
  A -->C[Zug 1 - T]
  A -->D[Zug 1 - N]
  B -->E[Zug 2 - T]
  B -->F[Zug 2 -  N]
  C -->G[Zug 2 - T]
  C -->H[Zug 2 - N]
  D -->I[Zug 2 - T]
  D -->J[Zug 2 - T]
  E -->K[Zug 3 - N]
  F -->L[Zug 3 - T]
  G -->M[Zug 3 - N]
  H -->N[Zug 3 - T]
  I -->O[Zug 3 - T]
  J -->P[Zug 3 - T]
  K -->Q[TTN]
  L -->R[TNT]
  M -->S[TTN]
  N -->T[TNT]
  O -->U[NTT]
  P -->V[NTT]
  
```


Schneller geht es, wenn man rechnet. 
Wir k√∂nnten auch R auffordern, die Anzahl der g√ºnstigen Pfade zu berechnen, s. @thm-binkoeff:

```{r}
choose(3,2)
```




### Anzahl Pfade mal Pfad-Wahrscheinlichkeit



In diesem Fall ist die Wahrscheinlichkeit *eines* (g√ºnstigen) Pfades, $A$:

$Pr(A) = Pr(T)^2 \cdot Pr(N)^1 = \left( \frac{2}{5} \right)^2 \cdot \left( \frac{3}{5} \right)^1$.

```{r}
p_a = (2/5)^2 * (3/5)^1
p_a
```

Damit ist die Wahrscheinlichkeit des gesuchten Ereignisses $A^{\prime}$ (2 Treffer bei 3 Z√ºgen) gleich der Anzahl der g√ºnstigen Pfade mal der Wahrscheinlichkeit eines Pfades, , s. @eq-binkoeff-hand:

$Pr(A^{\prime}) = 3 \cdot Pr(A)$.

```{r}
p_a_strich = 3 * p_a
p_a_strich
```

Die Wahrscheinlichkeit, bei 3 Z√ºgen 2 Treffer zu erzielen, betr√§gt also ca. 29%, vgl. @eq-binkoeff-hand.



$$Pr(A^{\prime}) = k \cdot Pr(A)$${#eq-binkoeff-hand}

Dabei steht $k$ f√ºr die Anzahl der g√ºnstigen Pfade und $Pr(A)$ f√ºr die Wahrscheinlichkeit eines g√ºnstigen Pfades 
(d.h. 2 Treffer und 1 Nieten) und alle Pfade haben die gleiche Wahrscheinlichkeit.


### Rechnen mit der Binomialverteilung





Die Binomialverteilung ist in R eingebaut; man kann sich leicht entsprechende Wahrscheinlichkeiten ausrechnen lassen.
Das ist komfortabler als selber rechnen.



```{r}
dbinom(x = 2, size = 3, prob = 2/5)
```

Die Wahrscheinlichkeit,  2 Treffer bei 3 Z√ºgen zu erzielen mit $p=2/5$, betr√§gt demnach ca. 29%.


Dabei gehen wir davon aus, dass die Wahrscheinlichkeit eines Treffers stets $p=2/5$ betr√§gt.


:::{#exm-binkoeff}
### Lotto

Wie viele Zahlenkombinationen gibt es im Lotto f√ºr 6 Richtige?
Der Binomialkoeffizient verr√§t es uns:
$\tbinom{49}{6}= 13\,983\,816\square$
:::

<!-- Auf Errisch geht das so: -->


<!-- :::: {.columns} -->

<!-- ::: {.column width="50%"} -->
<!-- >    üë®‚Äçüè´ Hey R, Wie viele M√∂glichkeiten gibt es, aus $n=3$ Z√ºgen $k=2$ auszuw√§hlen? -->


<!-- ::: -->

<!-- ::: {.column width="50%"} -->
<!-- >    ü§ñ √Ñh, Moment, oh herzliches Leberwesen -->
<!-- ```{r} -->
<!-- choose(5,2) -->
<!-- ``` -->
<!-- ::: -->

<!-- :::: -->



<!-- :::{#exm-2aus3} -->
<!-- Wie viele M√∂glichkeiten gibt es,  2 Treffer bei 3 Z√ºgen zu erzielen? -->

<!-- ```{r} -->
<!-- choose(4, 2) -->
<!-- ``` -->
<!-- ::: -->



:::{#exm-2aus4}
Wie viele M√∂glichkeiten gibt es,  2 Treffer bei 4 Z√ºgen zu erzielen?

1. TTNN, 2. TNTN, 3. TNNT, 4. NTTN ,5. NTNT, 6. NNTT. 

$\tbinom{4}{2} = \frac{4!}{2! \cdot (4-2)!} \overset{\text{k√ºrzen}}= \frac{2\cdot 3}{1}=6$ 

Es sind also 6 M√∂glichkeiten. 
:::


In R kann man sich die Fakult√§t mit dem Befehl `factorial` ausrechnen lassen. Der R-Befehl `choose` berechnet den Binomialkoeffizienten.^[Bei Taschenrechnern ist diese Funktion oft als "nCr" zu finden.]



```{r}
anzahl_pfade_2_aus_4 <- 
  factorial(4) / (factorial(2) * factorial(4-2))
anzahl_pfade_2_aus_4

choose(4, 2)
```


$\square$


:::{#exm-2aus5}
Hier sind die 10 Kombinationen, um aus 5 Losen genau 2 Treffer und 3 Nieten zu ziehen:

TTNNN, TNTNN, TNNTN, TNNNT, NTTNN, NTNTN, NTNNT, NNTTN, NNTNT, NNNTT 

```{r}
choose(5, 2)

anzahl_pfade_2_aus_5 <- 
  factorial(5) / (factorial(2) * factorial(5-2))
anzahl_pfade_2_aus_5
```

$\square$
:::


```{r}
#| echo: false
# Define the values of n and k
n <- 5  # Total number of items
k <- 2  # Number of items to choose

# Generate all combinations
combinations <- combn(n, k) |> t() |> as_tibble()
```








:::{#exm-bef√∂rdern}
### Bef√∂rderung
Aus einem Team mit 25 Personen sollen 11 Personen bef√∂rdert werden. Wie viele m√∂gliche Kombinationen (von bef√∂rderten Personen) k√∂nnen gebildet werden?

$\tbinom{25}{11} = \frac{25!}{11!\cdot(25-11)!} = 4\,457\,400$


```{r}
choose(n = 25, k = 11)
```

Es gibt 4457400 Kombinationen von Teams; dabei ist die Reihenfolge der Ziehung nicht ber√ºcksichtigt.$\square$
:::



::: {#exm-binom}
### Pumpstation-Beispiel zur Binomialverteilung

In einer Pumpstation arbeiten 7 Motoren, die wir als identisch annehmen. Mit einer Wahrscheinlichkeit von 5% f√§llt ein Motor aus und ist f√ºr den Rest des Tages nicht einsatzbereit. Der Betrieb kann aufrecht erhalten werden, solange mindestens 5 Motoren arbeiten. Wie gro√ü ist die Wahrscheinlichkeit, dass die Pumpstation aus dem Betrieb f√§llt?

$Pr(X=k)$ (oder kurz: $Pr(k)$) gibt die Wahrscheinlichkeit (Wahrscheinlichkeitsfunktion) an f√ºr das Ereignis, dass *k* Motoren arbeiten.

Lassen wir R mal $Pr(X=5)$ ausrechnen.


```{r}
dbinom(x = 5, size = 7, prob = .95)
```


Es gilt also $Pr(X=5) \approx .04$. Die Wahrscheinlichkeit, dass (nur) 5 Motoren laufen an einem beliebigen Tag ist relativ gering^[wobei "gering" subjektiv ist, die Betreiberfirma findet diese Wahrscheinlichkeit, dass 2 Pumpen ausfallen, wohl viel zu hoch.].
Die Wahrscheinlichkeit, dass $k=0 \ldots 7$ Motoren laufen, ist in @fig-motoren dargestellt.


`dbinom()` steht f√ºr die Wahrscheinlichkeits*d*ichte (im diskreten Fall, wie hier, Wahrscheinlichkeitsfunktion genannt) und `binom` f√ºr die Binomialverteilung. `x` gibt die Anzahl der Treffer an (das gesuchte Ereignis, hier 5 Motoren arbeiten); `size` gibt die Stichprobengr√∂√üe an (hier 7 Motoren).

Damit gilt:

$Pr(X\ge 5) = Pr(X=5) + Pr(X=6) + Pr(X=7)$

Berechnen wir zun√§chst die Wahrscheinlichkeit, dass 5,6 oder 7 Motoren laufen mit Hilfe der Binomialverteilung. 


```{r}
p_5 <- dbinom(x = 5, size = 7, prob = .95)
p_6 <- dbinom(x = 6, size = 7, prob = .95)
p_7 <- dbinom(x = 7, size = 7, prob = .95)

p_5
p_6
p_7
```

Das sind `r round(p_5, 2)`, `r round(p_6, 2)`, `r round(p_7, 2)`.



Die gesuchte Wahrscheinlichkeit, `p_mind_5`,
ist die Summe der drei Einzelwahrscheinlichkeiten.



```{r}
p_mind_5 <- p_5 + p_6 + p_7

p_mind_5
```


Die Wahrscheinlichkeit, dass mind. 5 Motoren arbeiten betr√§gt also 
`r round(p_mind_5, 4)`.


Das komplement√§re Ereignis zu diesem Ereignis ist, 
dass *nicht* mind. 5 Motoren arbeiten, 
also *h√∂chstens 4* und es daher zu einem Ausfall kommt.
Es gilt also $Pr(\bar{X}) = 1- Pr(X)$.


```{r}
p_weniger_als_4 <- 1 - p_mind_5
p_weniger_als_4
```

Das sind also `r round(p_weniger_als_4, 4)` oder 0.0003, also 0.03% Wahrscheinlichkeit, dass die Pumpstation ausf√§llt.


```{r}
#| echo: false
#| fig-cap: "Wahrscheinlichkeit, dass genau k = 0..7 Motoren laufen"
#| fig-subcap: 
#|   - "In 'normaler' Wahrscheinlichkeit, 0<p<1"
#|   - "In Log-Einheiten (Basis 2), 'Halbierungen'"
#| label: fig-motoren
#| layout-ncol: 2
#| 

font_size <- 6

ps <- dbinom(x = 0:7, size = 7, prob = .95)

ps_df <-
  tibble(Motorenzahl = 0:7,
         Pr = ps,
         Pr_log = log(ps, base = 2))

ps_df |> 
  ggplot(aes(x = Motorenzahl, y = Pr)) +
  geom_col() +
  scale_x_continuous(breaks = 0:7) +
  geom_label(aes(label = round(Pr, 3)), size = font_size)

ps_df |> 
  ggplot(aes(x = Motorenzahl, y = Pr_log)) +
  geom_col() +
  scale_x_continuous(breaks = 0:7) +
  geom_label(aes(label = round(Pr_log, 0)), size = font_size)
```


Alternativ kann man mit der Verteilungsfunktion `pbinom()` rechnen, 
die $Pr(X \le 4)$ berechnet.




In R kann man die Funktion `pbinom()` nutzen (p f√ºr (kumulierte) Wahrscheinlichkeit), um die Verteilungsfunktion der Binomialverteilung zu berechnen.

```{r}
pbinom(q = 4, size = 7, prob = .95)
```

`q = 4` steht f√ºr $X \le 4$, also f√ºr h√∂chstens 4 Treffer (arbeitende Motoren); `size = 7` meint die Stichprobengr√∂√üe, hier 7 Motoren; `prob` gibt die Trefferwahrscheinlichkeit an. $\square$

:::


:::callout-important

Die Funktion, die die Wahrscheinlichkeit daf√ºr angibt, dass die diskrete Zufallsvariable $X$ eine Realisation annimmt, die *kleiner oder gleich* (h√∂chstens) einem Wert $X=x$ ist, hei√üt *Verteilungsfunktion*.

$F(X=x) = Pr(X \le x)$

:::





#### Formel der Binomialverteilung




@thm-binomial zeigt die mathematische Definition der Binomialverteilung.
Dabei liegt immer ein Zufallsversuch mit $n$ Durchg√§ngen und $k$ Treffern zugrunde. Jeder Durchgang hat die Trefferwahrscheinlichkeit $p$ und jeder Durchgang ist unabh√§ngig von allen anderen.


::: {#thm-binomial}

### Binomialverteilung

$$Pr(X=k|p,n) = \frac{n}{k!(n-k)!}p^k(1-p)^{n-k}\quad \square$$
:::


 @thm-binomial kann wie folgt auf Deutsch √ºbersetzen:

>   Die Wahrscheinlichkeit f√ºr das Ereignis $X$ gegeben $p$ und $n$ berechnet als Produkt von zwei Termen. Der erste Term ist der Quotient von der Fakult√§t von n im Z√§hler und im Nenner das Produkt von erstens der Fakult√§t von k mit zweitens der Fakult√§t von (n-k). Der zweite Term ist das Produkt von p hoch k mal der komplement√§ren Wahrscheinlichkeit von p hoch (n-k).

Oder noch k√ºrzer:

>    Die Wahrscheinlichkeit f√ºr das Ereignis "X" gegeben p und k berechnet als Produkt von zwei Termen. Erstens der Anzahl der g√ºnstigen Pfade, k und zweitens der Wahrscheinlichkeit f√ºr einen g√ºnstigen Pfad, P(A).




Die Anzahl der (g√ºnstigen) Pfade kann man mit dem *Binomialkoeffizient* ausrechnen, den man so darstellt, s. @thm-binkoeff.^[wobei gelten muss $n \ge k$]

:::{#def-binkoeff}
### Binomialkoeffizient
Der Binomialkoeffizient gibt an, auf wie vielen verschiedenen Arten man aus einer Menge von $n$ verschiedenen Objekten $k$ Objekte ziehen kann (ohne Zur√ºcklegen und ohne Beachtung der Reihenfolge). $\square$
:::

::: {#thm-binkoeff}

### Binomialkoeffizient

$$k = \tbinom{n}{k}= \frac{n!}{k!(n-k)!} \quad \square$$

Lies: "W√§hle aus $n$ m√∂glichen Ereignissen (Pfade im Baum) $k$ g√ºnstige Ereignisse (g√ºnstige Pfade) oder k√ºrzer "k aus n".\square
:::



Puh, Formeln sind vielleicht doch ganz praktisch, 
wenn man sich diese lange √úbersetzung der Formel in Prosa duchliest.
Noch praktischer ist es aber, dass es Rechenmaschinen gibt, 
die die Formel kennen und f√ºr uns ausrechnen. 




::: {#exm-klausur20}

#### Klausur mit 20-Richtig-Falsch-Fragen

Ei Professi stellt einen Klausur mit 20 Richtig-Falsch-Fragen.
Wie gro√ü ist die Wahrscheinlichkeit, 
durch blo√ües M√ºnze werfen genau 15 Fragen richtig zu raten?^[Hey, endlich mal was f√ºr echte Leben!]

```{r QM2-Thema2-kleineModelle-23, echo = TRUE}
# Wskt f√ºr genau 15 Treffer bei 20 Versuchen mit einer fairen M√ºnze:
dbinom(x = 15, size = 20, prob = .5)
```

Um *h√∂chstens* 15 Treffer zu erzielen, 
m√ºssten wir die Wahrscheinlichkeiten von 0 bis 15 Treffern addieren.

Praktischerweise gibt es einen R-Befehl, der das f√ºr uns √ºbernimmt: `pbinom`.

```{r}
pbinom(q = 15, size = 20, prob = .5)
```


Die Wahrscheinlichkeit 0, 1, 2, ... oder 15 Treffer zu erzielen, liegt laut Binomialverteilung mit `pbinom` bei gut 99%.


:::



::: {#exm-globus4}

#### 3 M√ºnzw√ºrfe mit 3 Treffern

Was ist die Wahrscheinlichkeit bei 3 M√ºnzw√ºrfen (genau) 3 Treffer (Kopf) zu erzielen, s. @fig-p-bin-exms?

Das ist eine Frage an die Binomialverteilung;
in R kann man das mit der Funktion `dbinom` beantworten.

```{r QM2-Thema2-kleineModelle-24, echo = TRUE}
dbinom(x = 3, size = 3, prob = 1/2)
```

Die L√∂sung lautet also $p=1/8 = .125.\qquad \square$

Man kann sich auch vor Augen f√ºhren,
dass es genau 1 g√ºnstigen Pfad gibt, n√§mlich TTT.
Nach dem Multiplikationssatz gilt also: $Pr(X=3) = 1 \cdot \left( \frac{1}{2} \right)^3 = \frac{1}{8} = .125$.

```{r}
loesung <- (1/2)^3
loesung
```


:::


<!-- Todo: XXX -->

```{r QM2-Thema2-kleineModelle-25}
#| echo: false
#| label: fig-p-bin-exms
#| fig-cap: "Verschiedene Binomialverteilungen"
#| layout-ncol: 2
#| fig-subcap: 
#|   - "n=3, p=1/2"
#|   - "n=9, p=.7"

binomial_plot(3, 1/2)

binomial_plot(9, .7)
```




:::{#exr-binom}
üèãÔ∏èÔ∏è Was f√§llt Ihnen bei der Binomialverteilung auf? Ist sie symmetrisch? Ver√§ndert sich die Wahrscheinlichkeit linear?
:::




## Die Exponentialverteilung

### Die Apfel-f√§llt-nicht-weit-vom-Stamm-Verteilung

<!-- :::{#exm-klasurnote} -->
<!-- Sie haben in Statistik eine Klausur geschrieben. -->
<!-- Sie gehen davon aus, dass Sie etw -->

Bevor wir unser Kung-Modell spezifizieren k√∂nnen,
sollten wir noch √ºberlegen, welches Vorab-Wissen wir zur *Streuung um den Mittelwert* herum haben.
Da wir uns nicht 100% sicher zur gesuchten Gr√∂√üe sind,
m√ºssen wir diese Ungewissheit quantifizieren.
Wir m√ºssen also angeben, wie gro√ü die Streuung um den Mittelwert unserer Meinung nach ist.
Hier werden wir eingestehen, dass wir uns auch nicht 100% sicher sind,
wie gro√ü die Streuung exakt ist.
Also geben wir eine Verteilung f√ºr die Streuung an,
um diese Ungewissheit zu quantifizieren.

Etwas Wissen √ºber diese Verteilung haben wir:

- Eine Streuung muss positiv sein (es gibt keine negative Streuung).
- Eine Gleichverteilung der Streuung ist vielleicht m√∂glich, aber nicht sehr plausibel.
- Wenn wir der Meinung sind, der Mittelwert betrage "ungef√§hr 178 cm", so halten wir  180 cm als K√∂rpergr√∂√üe einer erwachsenen Person f√ºr plausibel, aber 18000 cm f√ºr (praktisch) unm√∂glich und schon 200 cm f√ºr recht unplausibel. 
Also: Je gr√∂√üer die die Abweichung vom Mittelwert, desto unplausibler der Wert der K√∂rpergr√∂√üe.

Diese Anforderungen^["Desiderata"] spiegeln sich in @fig-exp wider;
die eine  *Exponentialverteilung* zeigt (mit dem Parameter $\lambda=1$),
Eine Exponentialverteilung erf√ºllt unsere W√ºnsche ziemlich gut.
Au√üerdem zeigt die Abbildung verschiedene Quantile, wie das 95%-Quantil,
das bei 3 liegt;
95% der Werte dieser Verteilung sind also nicht gr√∂√üer als 3.



```{r Post-Regression-18}
#| fig-asp: 0.5
#| label: fig-exp
#| fig-cap: "Die Exponentialverteilung: Die meisten √Ñpfel fallen nicht weit vom Stamm ..."
#| echo: false
#| dev: "ragg_png"
#| 

# Set the rate parameter for the exponential distribution
rate <- 1

# Create a data frame for the exponential distribution
x_vals <- seq(0, 5, by = 0.01) # X-axis range
y_vals <- dexp(x_vals, rate = rate) # Density values
data <- data.frame(x = x_vals, y = y_vals)

ggplot(data, aes(x = x, y = y)) +
  geom_line(color = okabeito_colors()[1], size = 1) +
  theme_minimal() +
  labs(title = "Exponential Distribution", x = "", y = "Density") +
  theme(plot.title = element_text(hjust = 0.5)) +
  # Add emojis as text annotations
  annotate("text", x = 0, y = 0, label = "üå≥", size = 12, vjust = 0) +
  annotate("text", x = 1, y = 0, label = "üçé", size = 6, vjust = 0) +
  annotate("text", x = 2, y = 0, label = "üçé", size = 6, vjust = 0) +
  annotate("text", x = 3, y = 0, label = "üçé", size = 6, vjust = 0)


```


F√ºr eine exponentialverteilte Variable $X$ schreibt man auch:


$$X \sim \operatorname{Exp}(1)$$


Eine Verteilung dieser Form nennt man *Exponentialverteilung*.
Sie hat einige n√ºtzliche Eigenschaften:




1. Eine *Exp*onentialverteilung ist nur f√ºr positive Werte, $x>0$, definiert.
2. Steigt X um eine *Einheit*, so √§ndert sich Y um einen *konstanten Faktor*.
3. Sie hat nur einen Parameter, genannt *Rate* oder $\lambda$ ("lambda").
4. $\frac{1}{\lambda}$  gibt gleichzeitig Mittelwert und Streuung ("Gestrecktheit") der Verteilung an.
5. Je gr√∂√üer die Rate $\lambda$, desto schneller der "Verfall" der Kurve und desto *kleiner* die Streuung und der Mittelwert der Verteilung (und umgekehrt: Je gr√∂√üer  $1/\lambda$, desto *gr√∂√üer* die Streuung und der Mittelwert der Verteilung.)



Ohne auf die mathematischen Eigenschaften im Detail einzugehen,
halten wir fest, dass der Graph dieser Funktion gut zu unseren Pl√§nen passt.



### Visualisierung verschiedener Exponentialverteilungen

Schauen wir uns einige Beispiele von Exponentialverteilungen an.
Unterschiede in Exponentialverteilungen sind rein auf Unterschiede in $\lambda$ (lambda) zur√ºckzuf√ºhren, s. @fig-exps.

```{r gg-exp-fun}
#| echo: false
gg_exp <- function(r, max_x = 25, probs = c(0.05, .25, .50, .75, .95)) {
  
  d <-
  tibble(
    x = seq(0, 5,.1),
    y = dexp(x, rate = r)
  )
  
  d_qs <-
    tibble(
      prob = probs,
      q = qexp(prob, rate = r) 
    )
  
  med_exp <- round(log(2) / r, 2)
  
  
  p <- 
    ggplot(NULL, aes(c(0, max_x))) +
    geom_area(stat = "function", 
              fun = dexp, 
              fill = "grey", 
              args = list(rate = r))  +
    labs(title = paste0("Exp(", r, ")"),
         x = "sigma",
         caption = paste0("Median (Md): ", med_exp)) +
    geom_vline(data = d_qs,
             aes(xintercept = q)) +
    geom_label(data = d_qs,
             aes(x = q, 
                 label = prob,
                 y = prob))
 
}
```

::::{#fig-exps}

::: {.panel-tabset}



### lambda = 2

```{r}
#| echo: false
gg_exp(r = 2) + labs(subtitle = "lambda = 2") + theme_modern()
```


### lambda = 1

```{r}
#| echo: false
gg_exp(r = 1) + labs(subtitle = "lambda = 1") + theme_modern()
```

### lambda = 1/2

```{r}
#| echo: false
gg_exp(r = 1/2) + labs(subtitle = "lambda = 1/2") + theme_modern()
```


### lambda = 1/4

```{r}
#| echo: false
gg_exp(r = 1/4) + labs(subtitle = "lambda = 1/4") + theme_modern()
```



### lambda = 1/8

```{r}
#| echo: false
gg_exp(r = 1/8) + labs(subtitle = "lambda = 1/8") + theme_modern() 
```

:::
::::


<!-- ```{r Kung-9-bis} -->
<!-- #| echo: false -->
<!-- #| fig-asp: 0.8 -->
<!-- #| label: fig-exps -->
<!-- #| fig-cap: Beispiele von Expnentialverteilungen mit unterschiedlichem lambda  -->
<!-- #| fig-width: 9 -->

<!-- p_r1 <- gg_exp(r = 1) + labs(subtitle = "lambda = 1") -->

<!-- p_r2 <-  gg_exp(r = 1/2)  + labs(subtitle = "lambda = 1/2") -->

<!-- p_r1_8 <- gg_exp(r = 1/4) + labs(subtitle = "lambda = 1/4") -->

<!-- p_r1_25 <- gg_exp(r = 1/8) + labs(subtitle = "lambda = 1/8") -->


<!-- plots(p_r1_25, p_r1_8, p_r2, p_r1) -->
<!-- ``` -->




Wie wir in @fig-exps sehen, k√∂nnte eine Exponentialverteilung mit $\lambda=1/8$ grob passen f√ºr unser !Kung-Beispiel:
Bei einer Exponentialverteilung mit Rate $\lambda=1/8$ ist der Median bei ca. 5.5 cm, also einer Streuung  ($\sigma$, typischen Abweichung vom Mittelwert) von 5.5 cm.
Der Median ist der "mittlere Wert" der Verteilung, also der Wert, 
bei dem 50% der Werte kleiner und 50% der Werte gr√∂√üer sind.
Insofern eignet sich der Median gut als Sch√§tzer f√ºr einen "typischen" Wert der Verteilung.



:::callout-note
Die "richtigen" Priori-Verteilung zu finden, bzw. die richtigen Parameter f√ºr die Priori-Verteilung zu w√§hlen, ist nicht m√∂glich, denn es gibt nicht *die* eine, richtige Priori-Verteilung.
Eine "gut passende" Verteilung zu finden, ist nicht immer einfach.
Gut beraten ist man mit der Regel, im Zweifel lieber eine liberale Verteilung zu w√§hlen,
die einen breiteren Raum an m√∂glichen Werten zul√§sst. 
Allerdings sollte man nicht das Baby mit dem Wasser auskippen und extreme Werte, wie mehrere Meter K√∂rpergr√∂√üe Streuung, erlauben.
:::

Man kann sich die Quantile der Exponentialverteilung mit `qexp()` ausgeben lassen,
wobei mit man `p` den Wert der Verteilungsfunktion angibt,
f√ºr den man das Quantil haben m√∂chte 
(mit `pexp()` kann man sich analaog die die Verteilungsfunktion ausgeben lassen.) 
Mit `rate` wird $\lambda$ (lambda) bezeichnet.^[Es gibt auch [Online-Apps, die diese Werte ausgeben](https://homepage.divms.uiowa.edu/~mbognar/applets/exp-like.html.]

Dieser Aufruf zum Beispiel, `qexp(p = .5, rate = 1/8)`,
gibt uns das 50%-Quantil einer Exponentialverteilung mit Rate ($\lambda$) 1/8 zur√ºck, ca. 5.5: 
Mit einer Wahrscheinlichkeit von 50% wird ein Wert von 5.5 nicht √ºberschritten bei dieser Verteilung.

Die Grenzen der inneren 95% dieser Verteilung kann man sich auch ausgeben.

```{r}
qexp(p = c(0.025, .975), rate = 1/8)
```

Diese Grenzen scheinen hinreichend weit, das wir noch von den Daten √ºberrascht werden k√∂nnen, aber schmal genug, um unsinnige Werte auszuschlie√üen.
Ein guter Start! Weiter geht's!















### Vertiefung





#### Logarithmus

Ein Logarithmus ist die Umkehrung der Potenzierung.

Der Logarithmus beantwortet folgende Frage:
Mit welchem Exponenten muss ich eine bestimmte Zahl (die Basis) potenzieren, um eine andere Zahl zu erhalten? Die Antwort auf diese Frage ist der Logarithmus.

Formal ausgedr√ºckt:

:::{#def-logarithmus}
### Logarithmus
Der Logarithmus von einer Zahl $a$ zur Basis $b$ ist die Zahl $x$, mit der man $b$ potenzieren muss, um $a$ zu erhalten, s. @thm-logarithmus. $\square$
:::

:::{#thm-logarithmus}
### Logarithmus
$$\log b(x) = y \quad \text{wenn und nur wenn} \quad b^y = x$$
:::

Der Logarithmus zur Basis 2^[als "Logarithmus Dualis", ld, bezeichnet] gibt die "Verdopplungen" bzw. "Halbierungen" der Wahrscheinlichkeit an, wobei $ld(1/2) = -1.\square$



:::{#exm-log2-1}
$ld(1/2) = -1$
```{r}
log(.5, base = 2)
```

1/2 ist genau "minus 1 Verdopplung" von 1 entfernt, d.h. eine Halbierung.

$ld(1/4) = -2$
```{r}
log(1/4, base = 2)
```
1/4 ist genau "minus 2 Verdopplungen" von 1 entfernt, d.h. zwei Halbierungen.

$ld(1/8) = -3$
```{r}
log(1/8, base = 2)
```

1/8 (0.125) ist 3 Halbierungen von 1 entfernt.$\square$
:::





#### Simulieren wir eine Binomialverteilung 


Die Binomialverteilung l√§sst sich gut als "M√ºnzwurf-Verteilung" auffassen.

Werfen wir eine M√ºnze und sehen wir, was passiert.


```{r}
sample(x = c(0, 1), size = 1)
```


Mit `sample()` ziehen wir eine Stichprobe aus dem Ereignisraum `x`, hier 0 und 1. 
Dabei vereinbaren wir (willk√ºrlich), dass 0 f√ºr "Kopf" steht und 1 f√ºr "Zahl".
`size = 1` bedeutet, wir werfen die M√ºnze ein Mal (d.h. Stichprobengr√∂√üe *size* ist 1).

Okay, noch an Bord? Dann werfen wir die M√ºnze 10 Mal.


```{r}
sample(x = c(0, 1), size = 10, replace = TRUE)
```

`replace = TRUE` hei√üt, wir legen die M√ºnze wieder zur√ºck auf den Tisch, wenn wir sie geworfen haben.
Oder anders ausgedr√ºckt: *Ziehen mit Zur√ºcklegen*.



R, mach dich bereit, wirf die M√ºnze 1000 ($n=10^3$ oder `1e3`) Mal^[R meckert nicht bei langweiligen Aufgaben.].


```{r}
n <- 1e3

muenze_oft <- 
  sample(x = c(0, 1), size = n, replace = TRUE) 


muenze_oft %>% 
  sum()
```


Mit `sum()` nach dem Pfeifensymbol `%>%` haben wir aus dem Vektor `muenze_oft`, der aus der ersten Zeile resultiert,
die Summe ausgerechnet. 

Jetzt wissen wir, wie oft die M√ºnze "Zahl" gezeigt hat, n√§mlich `r sum(muenze_oft)` Mal.



::: callout-note
Wenn Sie einen Zufallsversuch wiederholen, muss nicht jedes Mal das gleiche Ergebnis resultieren. Entsprechend wird bei wiederholten Ausf√ºhrung der Funktion `sample()` nicht immer das gleiche Ergebnis resultieren. Wundern Sie sich also nicht, wenn bei Ihrem Computer eine √§hnliche, aber nicht gleiche, Zahl herauskommt.
:::


Visualisieren wir mal unsere M√ºnzw√ºrfe. Dazu erstellen wir zuerst eine geeignete Tabelle, @tbl-muenz.


```{r}
muenz_tab <-
  tibble(
    id = 1:n,
    x = muenze_oft,
    x_cumsum = cumsum(x) / id  # gibt Anteil von "Zahl" wieder
  )
```


```{r}
#| echo: false
#| label: tbl-muenz
#| tbl-cap: "Die kumulierte Summe beim M√ºnzwurf (nur die ersten paar Zeilen)"
head(muenz_tab)
```


Und hier der Anteil von "Zahl" im Verlauf unserer M√ºnzw√ºrfe, s. @fig-lln.^[`library(ggpubr); ggline(muenz_tab, x = "id", y = "x_cumsum")`]


```{r}
#| label: fig-lln
#| echo: false
#| fig-cap: "Das Gesetz der gro√üen Zahl am Beispiel der Stabilisierung des Trefferanteils beim wiederholten M√ºnzwurf"
#| fig-asp: 0.3

muenz_tab %>% 
  slice_head(n = 1e3) %>% 
  ggplot() +
  aes(x = id, y = x_cumsum) +
  geom_line() +
  theme_minimal()
```



Grob gesagt scheint sich ein M√ºnzwurf nach, naja, vielleicht 500 W√ºrfen "einigerma√üen" zu stabilisieren.^[Was "einigerma√üen" bedeuten soll, ist kein statistischer Begriff, sondern einer, der im echten Leben von den Menschen beantwortet werden muss, die eine Entscheidung zu treffen haben.]



::: callout-important
#### Das Gesetz der gro√üen Zahl

Zieht man (zuf√§llig) immer mehr Werte aus einer Verteilung (mit endlichem Mittelwert), n√§hert sich der Mittelwert der Stichprobe immer mehr mit dem Mittelwert (oft als *Erwartungswert* bezeichnet) der Verteilung an.
:::



```{r lln, out.width = "100%", fig.align="center", fig.asp = .5}
#| eval: false
#| echo: false
source(paste0(here::here(),"/R-Code/img15.R"))
```



<!-- 3b1b hat ein [nettes Video zu diesem Thema](https://youtu.be/8idr1WZ1A7Q), das sich als Vertiefung eignet. -->





## Vertiefung

@bourier2011, Kap. 6.2 und 7.1 erl√§utert einige (grundlegende) theoretische Hintergr√ºnde 
zu diskreten Zufallsvariablen und Wahrscheinlichkeitsverteilungen. 
Wichtigstes Exemplar f√ºr den Stoff dieses Kapitels ist dabei die Binomialverteilung.

@mittag2020 stellen in Kap. 12 und 13 Zufallsvariablen vor; 
zum Teil geht die Darstellung dort √ºber die Lernziele bzw. Inhalte dieses Kurses hinaus.


## Aufgaben


Zus√§tzlich zu den Aufgaben in der genannten Literatur sind folgende Aufgaben zu empfehlen.





### Paper-Pencil-Aufgaben



- [alphafehler-inflation3](https://datenwerk.netlify.app/posts/alphafehler-inflation3/alphafehler-inflation3.html)
- [alphafehler-inflation4](https://datenwerk.netlify.app/posts/alphafehler-inflation4/alphafehler-inflation4.html)
- [iq1a](https://datenwerk.netlify.app/posts/iq01a/)
- [iq2a](https://datenwerk.netlify.app/posts/iq02a/)
- [iq3a](https://datenwerk.netlify.app/posts/iq03a/)
- [simu-uniform](https://datenwerk.netlify.app/posts/simu-uniform/)
- [simu-unif2](https://datenwerk.netlify.app/posts/simu-unif2/)
- [simu-unif3](https://datenwerk.netlify.app/posts/simu-unif3/)
- [Quiz zum Thema Verteilungen](https://datenwerk.netlify.app/#category=Verteilungen-Quiz24)
- [Bsp-Binomial](https://datenwerk.netlify.app/posts/bsp-binomial/bsp-binomial)


### Aufgaben, f√ºr die man einen Computer braucht

- [alphafehler-inflation2](https://datenwerk.netlify.app/posts/alphafehler-inflation2/alphafehler-inflation2.html)
- [Quiz (Aufgabensammlung) zu Verteilungen](https://datenwerk.netlify.app/#category=Verteilungen-Quiz)
- [wuerfel01](https://datenwerk.netlify.app/posts/wuerfel01/wuerfel01.html)
- [wuerfel02](https://datenwerk.netlify.app/posts/wuerfel02/wuerfel02.html)
- [wuerfel03](https://datenwerk.netlify.app/posts/wuerfel03/wuerfel03.html)
- [wuerfel04](https://datenwerk.netlify.app/posts/wuerfel04/wuerfel04.html)
- [iq01](https://datenwerk.netlify.app/posts/iq01/iq01)
- [iq02](https://datenwerk.netlify.app/posts/iq02/iq02)
- [iq03](https://datenwerk.netlify.app/posts/iq03/iq03)
- [iq04](https://datenwerk.netlify.app/posts/iq04/iq04)
- [iq05](https://datenwerk.netlify.app/posts/iq05/iq05)
- [iq06](https://datenwerk.netlify.app/posts/iq06/iq06)
- [iq07](https://datenwerk.netlify.app/posts/iq07/iq07)
- [iq08](https://datenwerk.netlify.app/posts/iq08/iq08)
<!-- - [iq09](https://datenwerk.netlify.app/posts/iq09/iq09) -->
<!-- - [iq10](https://datenwerk.netlify.app/posts/iq010/iq10) -->
- [Bus1](https://datenwerk.netlify.app/posts/bus1/bus1)


## ---



![](img/outro-04.jpg){width=100%}




