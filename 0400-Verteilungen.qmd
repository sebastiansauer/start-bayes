# Verteilungen




## Lernsteuerung


### Lernziele

Nach Absolvieren des jeweiligen Kapitels sollen folgende Lernziele erreicht sein.

Sie können ...


- den Begriff der Zufallsvariablen erläutern
- die Begriffe von Wahrscheinlichkeitsdichte und Verteilungsfunktion erläutern 
- den Begriff einer Gleichverteilung erläutern 
- die Parameter einer Normalverteilung nennen und erläutern
- zentrale Konzepte in R umsetzen



### Vorbereitung im Eigenstudium



Lesen Sie selbständig, zusätzlich  zum Stoff dieses Kapitels noch @bourier_2018; dort folgende Abschnitte:

- Kap. 6.1 (Zum Begriff Zufallsvariable)
- Kap. 6.3 (Stetige Zufallsvariablen)
- Kap. 7.1.1 (Binomialverteilung)
- Kap. 7.2.1 (Gleichverteilung)
- Kap. 7.2.3 (Normalverteilung)

Lösen Sie auch die Übungsaufgaben dazu.


Weitere Übungsaufgaben finden Sie im dazugehörigen Übungsbuch, @bourier_statistik-ubungen_2022.


### Prüfungsrelevanter Stoff

Beachten Sie, dass neben den Inhalten des Kapitels auch stets der vorzubereitende Stoff prüfungsrelevant ist.


### Benötigte R-Pakete

```{r}
#| message: false
library(tidyverse)
```


```{r}
#| include: false
library(gt)
library(patchwork)
library(faux)
library(openintro)
library(easystats)
library(ggraph)
library(knitr)

source("funs/uniformplot.R")
```




```{r r-setup}
#| echo: false
#| message: false
theme_set(theme_minimal())
#scale_color_okabeito()
scale_colour_discrete <- function(...) 
  scale_color_okabeito()
```





### Zentrale Begriffe




#### Eigenschaften von Zufallsvariablen

- Zufallsvariable (random variable)
- Diskret vs. stetig
- Wahrscheinlichkeitsdichte (Dichte, (probability) density, f)
- Wahrscheinlichkeitsfunktion (kumulierte Wahrscheinlichkeit, Wahrscheinlichkeitsmasse)


#### Verteilungen

- Gleichverteilung
- Normalverteilung
- Standardnormalverteilung

### Begleitvideos


- [Video 1 zum Thema Verteilungen](https://youtu.be/7GqIE4sKDs4)
- [Video 2 zum Thema Verteilungen](https://youtu.be/HKWwondYsW8)



## Zufallsvariable


:::{#exm-thesis}
Schorsch sucht eine Betreuerin für seine Abschlussarbeit.
An die ideale Betreuerin setzt er 4 Kriterien an: a) klare, schriftliche fixierte Rahmenbedingungen, b) viel Erfahrung, c) guten Ruf und d) interessante Forschungsinteressen.
Je mehr dieser 4 Kriterien erfüllt sind, desto besser. 
Schorsch geht davon aus, dass die 4 Kriterien voneinander unabhängig sind (ob eines erfüllt ist oder nicht, ändert nichts an der Wahrscheinlichkeit eines anderen Kriteriums).
Schorsch interessiert sich also für die *Anzahl* der erfüllten Kriterien, also eine Zahl von 0 bis 4.
Er schätzt die Wahrscheinlichkeit für einen "Treffer" in jedem seiner 4 Kriterien auf 50%.
Viel Glück, Schorsch!
Sein Zufallsexperiment hat 16 Ausgänge, s. @fig-4muenzen und @tbl-schorsch-zufall. Ganz schön komplex.
Eigentlich würden ihm ja eine Darstellung mit 5 Ergebnissen, also der "Gutachter-Score" von 0 bis 4 ja reichen. 
Wie können wir es übersichtlicher für Schorsch?$\square$
:::


 

```{r}
#| echo: false
#| label: fig-4muenzen
#| fig-cap: Ein Baumdiagramm mit 16 Ausgängen, analog zur 4 Münzwürfen
my_tree <- tidygraph::create_tree(31, 2, mode = "out")

my_tree %>%
  mutate(lab = 1:31) %>% 
  ggraph(circular = FALSE) +
  geom_edge_link() +
  geom_node_label(mapping = aes(label = lab)) +
  coord_flip() +
  scale_y_reverse() +
  theme_void() +
  theme(text = element_text(size = 1))
```



```{r}
#| echo: false
#| tbl-cap: Schorschs Zufallsexperiment, Auszug der Elementarereignisse
#| label: tbl-schorsch-zufall
d <- tibble::tribble(
   ~i, ~Elementarereignis, ~`Pr(EE)`, ~Trefferzahl, ~`Pr(Trefferzahl)`,
  "1",             "NNNN",    "1/16",          "0",             "1/16",
  "2",             "NNNT",    "1/16",          "1",              "1/4",
  "3",             "NNTN",    "1/16",          "1",              "1/4",
  "4",             "NTNN",    "1/16",          "1",              "1/4",
  "5",             "TNNN",    "1/16",          "1",              "1/4",
  "6",             "NNTT",    "1/16",          "2",                "…",
  "…",                "…",       "…",          "…",                "…"
  )


kable(d, booktabs = TRUE)
```


Schorsch braucht also eine übersichtlichere Darstellung;
die Zahl der Treffer und ihre Wahrscheinlichkeit würde ihm ganz reichen.
In vielen Situationen ist man an der *Anzahl der Treffer* interessiert.
Die Wahrscheinlichkeit für eine bestimmte Trefferanzahl bekommt man einfach durch Addieren der Wahrscheinlichkeiten der zugehörigen Elementarereignisse, s. @tbl-schorsch-zufall.
Hier kommt die *Zufallsvariable* ins Spiel.
Wir nutzen sie, um die Anzahl der Treffer in einem Zufallsexperiment zu zählen.


:::{#def-zufallsvariable}
### Zufallsvariable
Die Zuordnung der Elementarereignisse eines Zufallsexperiment zu genau einer Zahl $\in \mathbb{R}$ nennt man Zufallsvariable.$\square$
:::

Die den Elementarereignisse zugewiesenen Zahlen nennt man *Realisationen* oder Ausprägungen der Zufallsvariablen

:::{#exm-lotto}
### Lotto
Ein Lottospiel hat ca. 14 Miollionen Elementarereignisse. Die Zufallsvariable "Anzahl der Treffer" hat nur 7 Realisationen: 0,1,...,6.$\square$
:::

Es hat sich eingebürgert, Zufallszahlen mit $X$ zu bezeichnen (oder anderen Buchstaben weit hinten aus dem Alphabet).

Man schreibt kurz: $X: \Omega \rightarrow \mathbb{R}$. Um die Vorschrift der Zuordnung genauer zu bestimmen, kann man folgende Kurzschreibweise nutzen:


${\displaystyle X(\omega )={\begin{cases}1,&{\text{wenn }}\omega ={\text{Kopf}},\\[6pt]0,&{\text{wenn }}\omega ={\text{Zahl}}.\end{cases}}}$


@fig-zv stellt diese Abbildung dar.



```{mermaid}
%%| fig-cap: Eine Zufallsvariable ist eine Abbildung vom Ereignisraum zu den Realisationen der Zufallsvariable. Außerdem sieht man, wie Zufallsvariablen genutzt werden, um Wahrsscheinlichkeiten zu bestimmen.
%%| label: fig-zv
flowchart LR
  subgraph A[Ereignisraum]
    Kopf
    Zahl
  end
  subgraph B[Zufallsvariable]
    null[0]
    eins[1]
  end
  subgraph C[Wahrscheinlichkeit]
    half[50%]
  end
  
  Kopf --> null
  Zahl --> eins
  null --> half
  eins --> half
```



Zufallsverteilungen kann im zwei Artein einteilen:

1. diskrete Zufallsvariablen
2. stetige Zufallsvariablen



### Diskrete Zufallsvariable

#### Grundlagen

Eine diskrete Zufallsvariable ist dadurch gekennzeichnet, dass nur bestimmte Realisationen möglich sind, zumeist natürliche Zahlen, wie 0, 1, 2,..., .
@fig-zuv-disk versinnbildlicht die Zufallsvariable des "Gutachter-Scores", s. @exm-thesis.


```{r}
#| echo: false
#| fig-asp: 0.2
#| label: fig-zuv-disk
#| fig-cap: Sinnbild einer diskreten Zufallsvariablen X für Schorschs Suche nach einer Betreuerin seiner Abschlussarbeit. X gibt den Score der Gutachterin wider.
ggplot(data.frame(x=c(0:4), y = 0), aes(x,y)) +
  geom_point(size = 5, alpha = .7) +
  scale_y_continuous(limits = c(-1,1), breaks = NULL) +
  scale_x_continuous(breaks = 0:4, labels = 0:4) +
  theme_minimal() +
  labs(y = "", x = "")

```



:::{#exm-zv-disk}
### Diskrete Zufallsvariablen

- Anzahl der Bewerbungen bis zum ersten Job-Interview
- Anzahl Anläufe bis zum Bestehen der Statistik-Klausur
- Anzahl der Absolventen an der HS Ansbach pro Jahr
- Anzahl Treffer beim Kauf von Losen
- Anzahl Betriebsunfälle
- Anzahl der Produkte in der Produktpalette$\square$
:::



:::{#exm-zweiwuerfel}
Der zweifache Würfelwurf ist ein typisches Lehrbuchbeispiel für eine diskrete Zufallsvariable.^[da einfach und deutlich]
Hier ist $S$^[S wie Summe] die Augen*s*umme des zweifachen Würfelwurfs und $S$ ist eine Zahl zwischen 2 und 12.
Für jede Realisation $X=x$ kann die Wahrscheinlichkeit berechnen, @fig-zweiwuerfel-vert versinnbildlicht die Wahrscheinlichkeit für jede Realisation von $X$.$\square$
:::

![Augensumme des zweifachen Würfelwurfs; für jede Realisation von S ist die zugehörige Wahrscheinlichkeit dargestellt. Bildquelle: Tim Stellmach, Wikipedia, PD](img/Dice_Distribution_(bar).svg.png){#fig-zweiwuerfel-vert width=50%}



*Wahrscheinlichkeitsverteilungen* dienen dazu, den Realisiationen einer Zufallsvariable eine Wahrscheinlichkeit zuzuordnen.








:::{#def-wvert-disk}
### Diskrete Wahrscheinlichkeitsverteilung
Eine *diskrete* Wahrscheinlichkeitsverteilung der (diskreten) Zufallsvariablen $X$ ordnet jeder der $k$ Ausprägungen $X=x$ eine Wahrscheinlichkeit $p$ zu.$\square$
:::

:::{#exm-babies}
### Wahrscheinlichkeit des Geschlechts bei der Geburt
So hat die Variable *Geschlecht eines Babies* die beiden Ausprägungen *Mädchen* und *Junge* mit den Wahrscheinlichkeiten $p_M = 51.2\%$ bzw. $p_J = 48.8\%$, laut einer Studie [@gelman_regression_2021].$\square$
:::


Zwischen der deskriptiven Statistik und der Wahrscheinlichkeitstheorie bestehen enge Parallelen, @tbl-wkeit-desk stellt einige zentrale Konzepte gegenüber.

```{r}
#| echo: false
#| label: tbl-wkeit-desk
#| tbl-cap: "Gegenüberstellung von Wahrscheinlichkeitstheorie und deskriptiver Statistik"
d <- tibble::tribble(
         ~Wahrscheinlichkeitstheorie,                      ~Desktiptive.Statistik,
                   "Zufallsvariable",                                   "Merkmal",
                "Wahrscheinlichkeit",               "relative Häufigkeit, Anteil",
       "Wahrscheinlichkeitsfunktion",   "einfache relative Häufigkeitsverteilung",
               "Verteilungsfunktion", "kumulierte relative Häufigkeitsverteilung",
                    "Erwartungswert",                                "Mittelwert",
                           "Varianz",                                   "Varianz"
       )

```




```{r}
#| echo: false
#| message: false
#| warning: false
dice_outcomes <- expand.grid(Die1 = 1:6, Die2 = 1:6)

# Calculate the sum of the two dice for each outcome
dice_outcomes$Sum <- dice_outcomes$Die1 + dice_outcomes$Die2

# Calculate the probability of each sum using the table function
sum_counts <- table(dice_outcomes$Sum)
total_outcomes <- sum(sum_counts)
probabilities <- sum_counts / total_outcomes

twodice <- tibble(
  Augensumme = 2:12,
  p = probabilities) |> 
  mutate(p_cum = cumsum(p))


p_twodice <- ggplot(twodice, aes(x = Augensumme, y = p)) + 
  geom_col() +
  geom_label(aes(y = p, label = round(p, 2), nudge_y = .1))
  
```




```{r}
#| echo: false
#| message: false
#| warning: false
#| 
num_trials <- 1000  # You can change this to the desired number of trials

# Simulate repeated throws of two dice
results <- replicate(num_trials, {
  die1 <- sample(1:6, 1, replace = TRUE)  # Simulate the first die
  die2 <- sample(1:6, 1, replace = TRUE)  # Simulate the second die
  c(Die1 = die1, Die2 = die2)  # Return the results as a vector
}) |> 
  t() |> 
  as_tibble() |> 
  mutate(Augensumme  = Die1 + Die2)

# Display

results_count <-
  results |> 
  count(Augensumme) |> 
  mutate(prop = n/num_trials) |> 
  mutate(n_cum = cumsum(n),
         prop_cum = cumsum(prop))

p_sim2dice <-
  ggplot(results_count) +
  aes(x = Augensumme, y = n) +
  geom_col() +
  geom_label(aes(y = n, label = round(prop, 2)))
```





Eine *Verteilung* zeigt, welche Ausprägungen eine Variable aufweist und wie häufig bzw. wahrscheinlich diese sind. 
Einfach gesprochen veranschaulicht eine Balken- oder Histogramm eine Verteilung. Man unterscheidet Häufigkeitsverteilungen (s. Abb. @fig-2dice-sim) von Wahrscheinlichkeitsverteilungen (Abb. @fig-2dice-prob).




:::: {.columns}

::: {.column width="50%"}
```{r}
#| echo: false
#| fig-cap: Wahrscheinlichkeiten des zweifachen Würfelwurfs
#| label: fig-2dice-prob
p_twodice
```

:::

::: {.column width="50%"}
```{r}
#| echo: false
#| fig-cap: (relative und absolute) Häufigkeiten des zweifachen Würfelwurfs, 1000 Mal wiederholt
#| label: fig-2dice-sim
p_sim2dice
```
:::

::::



:::{#exm-wert-wuerfel}
### Wahrscheinlichkeitsfunktion eines Würfels
@fig-w-wuerfel zeigt die Wahrscheinlichkeitsfunktion eines einfachen Würfelwurfs.$\square$
:::

![Wahrscheinlichkeitsfunktion eines einfachen Würfelwurfs, Bildrechte: Olex Alexandrov, Wikipedia, PD](img/220px-Fair_dice_probability_distribution.svg.png){#fig-w-wuerfel width=33%}




:::{#exm-mtcars)
<!-- ### Häufigkeitsverteilung bei `mtcars` -->

Die Häufigkeitsverteilung eines *diskreten* Merkmals $X$ mit $k$ Ausprägungen zeigt (vgl.@ tbl-hauef),
wie häufig die einzelnen Ausprägungen sind.
So hat die Variable *Zylinder* (in einem Datensatz) etwa die Ausprägungen 4,6 und 8.$\square$
:::


:::: {.columns}

::: {.column width="70%"}


```{r}
#| echo: false
#| fig-cap: "Häufigkeitsverteilung von `cyl` und `hp` (diskretisiert in 10 Körbe oder Gruppen)"
#| label: fig-mtcars-freq
p1 <- 
  mtcars %>% 
  ggplot(aes(x = cyl)) +
  geom_bar()


p2 <- mtcars %>% 
  ggplot(aes(x = hp)) +
  geom_histogram(bins=10)

plots(p1, p2, n_rows = 1)
```
  
:::

::: {.column width="30%"}
```{r }
#| eval: true
#| echo: false
#| tbl-cap: Eine diskrete Häufigkeitsverteilung, dargestellt in einer Häufigkeitstabelle
#| label: tbl-hauef-tab
data(mtcars)
  mtcars %>% 
    count(cyl)
```
:::

::::



  
  
Abb. @fig-mtcars-freq, links, visualisiert die Häufigkeitsverteilung von `cyl`.
Ein *stetiges* Merkmal, wie `hp` (PS-Zahl), lässt sich durch Klassenbildung in ein diskretes umwandeln (diskretisieren), s. Abb. @fig-mtcars-freq, rechts.

#### Wahrscheinlichkeitsfunktion

:::{#def-w-fun}
### Wahrscheinlichkeitsfunktion

Die Funktion $f$, die den möglichen Realisationen $x_i$ der diskreten Zufallsvariablen $X$ die Eintrittswahrscheinlichkeiten zuordnet, heißt Wahrscheinlichkeitsfunktion.$\square$
:::


:::{#exm-w-fun}
Die Wahrscheinlichkeitsfunktion für $X$ "Augensumme im zweifachen Würfelwurf" ist in @fig-2dice-prob visualisiert.$\square$ 
:::


:::{#exm-muenz}
Die Wahrscheinlichkeitsfunktion für $X$ "Treffer im einfachen Münzwurf, mit Zahl ist Treffer" ist $Pr(X=1)=1/2.$, vgl. @fig-zv.$\square$
:::

#### Verteilungsfunktion



:::{#def-vert-fun}
### Verteilungsfunktion
Die Verteilungsfunktion $F$ gibt die Wahrscheinlichkeit an, dass die diskrete Zufallsvariable $X$ eine Realisation annimmt, die kleiner oder gleich $x$ ist.$\square$
:::



Die Berechnung von $F(x)$ erfolgt, indem die Wahrscheinlichkeiten aller möglichen Realisationen $x_i$, die kleiner oder gleich dem vorgegebenen Realisationswert $x$ sind, addiert werden:

$F(x) = \sum_{x_ \le x} Pr(X=x_i).$


```{r}
#| echo: false
p_F <- 
  ggplot(twodice, aes(x = Augensumme, y = p_cum)) + 
  geom_col() +
  geom_line() +
  geom_label(aes(label = round(p_cum, 2))) + 
  scale_x_continuous(breaks = 1:12) +
  labs(y = "Verteilungsfunktion F")


y_lab <- "empirische Verteilungsfunktion F emp."

p_F_emp <-
  ggplot(results_count) +
  aes(x = Augensumme, y = prop_cum) +
  geom_col() +
  geom_line() +
  geom_label(aes(y = prop_cum, label = round(prop_cum, 2))) +
  labs(y = y_lab) +
  scale_x_continuous(breaks = 2:12)
```


Die Verteilungsfunktion ist das Pendant zur *kumulierten Häufigkeitsverteilung*, vgl. @fig-kum-h-vert und @fig-kum-h-vert-emp.


:::: {.columns}

::: {.column width="50%"}
```{r}
#| echo: false
#| fig-cap: Kumulierte Häufigkeitsverteilung $H(X \le x_i)$
#| label: fig-kum-h-vert
p_F
```

:::

::: {.column width="50%"}
```{r}
#| echo: false
#| fig-cap: Empirische Verteilungsfunktion (kumulierte Häufigkeitsverteilung) $H(X \le x_i)$ von 1000 zweifachen Münzwürfen
#| label: fig-kum-h-vert-emp
p_F_emp 
```

:::

::::


### Stetige Zufallsvariablen

@fig-zv-stetig-groesse versinnbildlicht die stetige Zufallsvariable "Körpergröße", die (theoretisch, in Annäherung) jeden beliebigen Wert zwischen 0 und (vielleicht) 3m annehmen kann.

```{r echo = FALSE}
#| fig-cap: Sinnbild für eine stetige Zufallsvariable X "Körpergröße"
#| label: fig-zv-stetig-groesse
#| fig-asp: 0.2
 
ggplot(data.frame(x=0, y = 0), aes(x,y)) +
  #geom_point(size = 5, alpha = .7) +
  scale_y_continuous(limits = c(-1,1), breaks = NULL) +
  scale_x_continuous(breaks = c(0, 50, 100, 150, 200)) +
  annotate("segment", x = 0, xend = 200, y = 0, yend = 0, color = "red")  +
  theme_minimal() +
  annotate("label", x = 200, y = 0, label = "...") +
  labs(y = "", x = "")
```


:::{#def-zv-stetig}
### Stetige Zufallsvariable
Eine stetige Zufallsvariable gleicht einer diskreten, nur dass alle Werte im Intervall erlaubt sind.$\square$
:::


:::{#exm-zu-stetig}
- Spritverbrauch
- Körpergewicht von Professoren
- Schnabellängen von Pinguinen
- Geschwindigkeit beim Geblitztwerden$\square$
:::


:::{#exr-bus-42}
### Warten auf den Bus, 42 Sekunden
Sie stehen an der Bushaltestellen und warten auf den Bus.
Langweilig.
Da kommt Ihnen ein Gedanken in den Sinn: 
Wie hoch ist wohl die Wahrscheinlichkeit, dass Sie *exakt* 42 Sekunden auf den Bus warten müssen, s. @fig-p42?
Weiterhin überlegen Sie, dass davon auszugehen ist, dass jede Wartezeit zwischen 0 und 10 Minuten gleich wahrscheinlich ist.
Spätestens nach 10 Minuten kommt der Bus, so ist die Taktung (extrem zuverlässig).
Exakt heißt *exakt*, also nicht 42.1s, nicht 42.01s, nicht 42.001s, etc. bis zur x-ten Dezimale.$\square$
:::


Nicht so einfach (?). Hingegen ist die Frage, wie hoch die Wahrscheinlichkeit ist, zwischen 0 und 5 Minuten auf den Bus zu warten ($0<x<5$), einfach: Sie beträgt 50%, wie man in @fig-p05 gut sehen kann.




:::: {.columns}

::: {.column width="50%"}


```{r}
#| echo: false
#| #| label: "Wie groß ist die Wahrscheinlichkeit, zwischen 0 und 5 Minuten auf den Bus zu warten? 50%!"
#| fig-cap: fig-p05
p_bus2 <-
  uniform_Plot(0, 10) + 
  geom_rect(xmin = 0, xmax = 5, ymin = 0, ymax = 0.1, fill = "#E69F00FF", alpha = .3) +
  annotate("label", x = 2.5, y = .05, hjust = 0.5, label = "Pr(0<x<5)=50%",
           color = "#E69F00FF")  +
  annotate("label", x= 5, y = 0.1, label = "f(x) = 1/10", color = "#009E73FF", size = 10)

p_bus2
```
:::

::: {.column width="50%"}
```{r}
#| echo: false
#| lafig-cap: "Wie groß ist die Wahrscheinlichkeit, genau 42 Sekunden auf den Bus zu warten? Hm."
#| label: fig-p42
p_bus1 <- 
  uniform_Plot(0, 10) + 
  geom_vline(xintercept = .42, color = "#56B4E9FF", size = 1) +
  annotate("label", x = .42, y = .05, hjust = 0, label = "Pr(X=0.42)=?", color="#56B4E9FF") +
  annotate("point", x = .42, y = 0, size = 5, color = "#56B4E9FF", alpha = .7) +
  annotate("label", x= 5, y = 0.1, label = "f(x) = 1/10", color = "#009E73FF", size = 10)

p_bus1
```
:::

::::

Vergleicht man @fig-p42 und @fig-p05 kommt man (vielleicht) zu dem Schluss, dass die Wahrscheinlichkeit exakt 42s auf den Bus zu warten, praktisch Null ist.
Der Grund ist, dass die Fläche des Intervalls gegen Null geht, wenn das Intervall immer schmäler wird.
Aus diesem Grund kann man bei stetigen Zufallszahlen nicht von einer Wahrscheinlichkeit eines bestimmten Punktes $X=x$ sprechen.
Für einen bestimmten Punkt $X=x$ kann man aber die *Dichte* der Wahrscheinlichkeit angeben.

Was  gleich ist in beiden Situationen ($Pr(X=.42)$ und $Pr(0<x<0.5)$) ist die *Wahrscheinlichkeitsdichte*, $f$.
In @fig-p42 und @fig-p05 ist die Wahrscheinlichkeitsdichte gleich, $f=1/10=0.1$.

:::{#def-wdichte}
### Wahrscheinlichkeitsdichte
Die Wahrscheinlichkeitsdichte $f(x)$ gibt an, wie viel Wahrscheinlichkeitsmasse pro Einheit von $X$ an an der Stelle $x$ ist.$\square$
:::


Die Wahrscheinlichkeitsdichte zeigt an, an welchen Stellen $x$ die Wahrscheinlichkeit besonders "geballt" oder "dicht" sind, s. @fig-wdichte-sinnbild.

![Die Wahrscheinlichkeit, dass eine Zufallsvariable einen Wert zwischen und annimmt, entspricht dem Inhalt der Fläche unter dem Graph der Wahrscheinlichkeitsdichtefunktion. Bildrechte: 4C, Wikipedia, CC-BY-SA .](img/260px-Integral_as_region_under_curve.svg.png){#fig-wdichte-sinnbild width=33%}


### Verteilungsfunktion


:::: {.columns}

::: {.column width="50%"}
:::{#def-vert-fun-stetig}
Die Verteilungsfunktion einer stetigen Zufallsvariablen gibt wie im diskreten Fall an, wie groß die Wahrscheinlichkeit für eine Realisation kleiner oder gleich einem vorgegebenen Realisationswert $x$ ist.$\square$

Die Verteilungsfunktion $F(x)$ ist analog zur kumulierten Häufigkeitsverteilung zu verstehen, vgl. @fig-F-Bus.

:::
:::

::: {.column width="50%"}



```{r}
#| echo: false
#| fig-cap: 'Verteilungsfunktion F für X="Wartezeit auf den Bus"'
#| label: fig-F-Bus
#| fig-asp: 0.3
d <- 
  tibble(x=1:10,
         y= 1:10/10) 

ggplot(d, aes(x,y)) +
  geom_point(alpha = .5) +
  geom_line() +
  scale_x_continuous(breaks = 1:10) +
  theme_minimal()
```




:::

::::






:::{#def-wvert-stetig}
### Stetige Wahrscheinlichkeitsverteilung
Bei *stetigen* Zufallsvariablen $X$ geht man von unendlich vielen Ausprägungen aus; die Wahrscheinlichkeit einer bestimmten Ausprägung ist (praktisch) Null: $p(X=x_j)=0, \quad j=1,...,+\infty \square$.
:::


:::{#exm-groesse}
### Wahrscheinlichkeitsverteilung für die Körpergröße
So ist die Wahrscheinlichkeit, dass eine Person exakt 166,66666666... cm groß ist, (praktisch) Null.
Man gibt stattdessen die *Dichte* der Wahrscheinlichkeit an: Das ist die Wahrscheinlichkeit(smasse) pro  Einheit von $X$.$\square$
:::



## Wichtige Verteilngen


### Gleichverteilung

#### Indifferenz als Grundlage

Eine Gleichverteilung nimmt an, dass jeder Wert im Ergebnisraum der zugehörigen Zufallsvariable *gleichwahrscheinlich* ist.
Wenn man keinen hinreichenden Grund hat, eine Realisation einer Zufallsvariablen für plausibler als einen anderen zu halten,
ist eine Gleichverteilung eine passende Verteilung.
Gleichverteilungen gibt es im diskreten und im stetigen Fall.

Abb. @fig-uniform zeigt ein Beispiel für eine (stetige) Gleichverteilung.


```{r Normalverteilung-4, fig.asp = 0.5}
#| echo: false
#| fig-cap: Stetige Gleichverteilung
#| fig-subcap: 
#|   - "Beispiel a: Gleichverteilung min=-1, max=1. Dichte: 1/2"
#|   - "Beispiel b: Gleichverteilung min=0, max=3. Dichte: 1/3"
#| label: fig-uniform
#| layout-ncol: 2
#
#source: https://dk81.github.io/dkmathstats_site/rmath-uniform-plots.html

uniform_Plot(-1, 1)
uniform_Plot(0, 3)
```




@fig-uniform, links: Bei $X=0$ hat eine Einheit von $X$ die Wahrscheinlichkeitsmasse von 50%, da der Bereich $[-0.5, +0.5]$ die Hälfte (50%) der Wahrscheinlichkeitsmasse der Verteilung beinhaltet. Bei jedem anderen Punkt $x$ ist die Dichte gleich.
@fig-uniform, rechts: Bei $X=0$ hat eine Einheit von $X$ die Wahrscheinlichkeitsmasse von ca. 33%, da der Bereich $[-0.5, +0.5]$ ein Drittel der Wahrscheinlichkeitsmasse der Verteilung beinhaltet. Bei jedem anderen Punkt $x$ ist die Dichte gleich.
Definierendes Kennzeichen einer Gleichverteilung ist die *konstante Dichte*.



#### Simulation


Möchte man die Verteilungsfunktion einer stetigen Zufallsvariablen berechnen,
kann die Mathe ganz schön kompliziert werden, schließlich muss man Integrale lösen.
Aber es gibt einen Trick, wie man die Sache stark vereinfachen kann: 
man simuliert die Verteilung. Was bedeutet das?



Angenommen, die Wartezeit auf einen Bus ist gleichverteilt (engl. *uniform distribution*); 
der Bus kommt regelmäßig und pünktlich alle 10 Minuten. 
Die minimale Wartezeit beträgt also 0 Minuten und die maximale 10 Minuten.
Nennen wir die zugehörige Zufallsvariable $X$, das ist schön kurz zu schreiben.

Dann schreibt man auch:

$$X \sim Unif(0,10).$$



Ja, das sieht fancy aus, ist aber dafür schön kurz, aber wo ist der versprochene Trick zum Vereinfachen?
Kommt gleich, Moment.

Eine Frage könnte nun lauten, wie groß ist die Wahrscheinlichkeit, dass man zwischen 3 und 5 Minuten auf den Bus warten muss?
Achtung: Hier ist der Trick. Nämlich, dass wir Integralrechnung gegen stumpfes Zählen eintauschen.

Computer (und damit R) haben eingebaute Funktionen, die eine beliebige Zufallszahl ziehen können,
zum Beispiel gleichverteilte.
Auf Errisch heißt das Zauberwort `runif()`:

```{r}
#| eval: false
runif(n = 1, min = 0, max = 10)
```


```{r}
#| echo: false
set.seed(42)
runif(n = 1, min = 0, max = 10)
```

Auf Deutsch heißt das: 

>   "Hey R, ich hätte gerne eine (daher `n = 1`) Zufallszahl (*r* wie *random*),
die gleichverteilt ist (*uniform*) mit `min = 0` und `max = 10`.

>   🤖 Jawohl, oh herrliches Leberwesen



(Zu) anschaulich gesprochen: R hat den Bus kommen lassen und es hat gut 9.1 Minuten gedauert,
bis er da war.
Achtung, jetzt kommt's: Jetzt lassen wir R mal $10^5$ (`1e5` auf Computersprech) Busse vorfahren. 
R soll jedes Mal notieren, wie lange man auf den Bus warten musste.^[Machen Sie das mal ohne Computer, wenn Sie ein Wochenende lang Langeweile haben.]




```{r}
#| eval: false
x_simu <- runif(n = 1e5, min = 0, max = 10)
```

```{r}
#| echo: false
n <- 1e5
set.seed(42)
x_simu <- runif(n = n, min = 0, max = 10)  # gibt Vektor zurück

x_simu_df <-
  tibble(id = 1:n,
         x = x_simu)
```



Schauen wir uns die Verteilung an, s. @fig-simu-gleichvert.^[Alternativ kann man z.B. auch `ggplot` verwenden: `ggplot(x_simu_df, aes(x = x_simu)) +  geom_histogram(bins = 50)`.]

```{r}
#| eval: false

library(ggpubr)
gghistogram(x_simu_df, x = "x_simu", fill = "grey20")
```




```{r}
#| label: fig-simu-gleichvert
#| fig-cap: "Simulation einer gleichverteiluten Zufallsvariablen"
#| eval: true
#| echo: false

library(ggpubr)
gghistogram(x_simu_df, x = "x_simu", fill = "grey20")
```






Okay, unsere Verteilung sieht nicht *exakt* gleichverteilt, aber einigermaßen. 
Gut genug für unsere Zwecke!

So, und jetzt kommt das Ernten.
Wir können jetzt nämlich einfach zählen (`count()`), um die Antwort auf unsere Frage (der Wartezeit 3-5 Min.) zu erhalten, s. @tbl-count-simu-bus.


:::: {.columns}

::: {.column width="50%"}
```{r}
#| eval: false
x_simu_df %>% 
  count(Schnittmenge = x > 3 & x < 5)
```

:::

::: {.column width="50%"}
```{r}
#| echo: false
#| tbl-cap: Häufigkiten auslesen anstelle von Integralen berechnen
#| label: tbl-count-simu-bus
x_simu_df %>% 
  count(Schnittmenge = x > 3 & x < 5)
```

:::

::::




Das Zeichen `&` ist das logische UND, also die Schnittmenge der zwei Mengen $A := \{x|x>3\}$ und $B := \{x|x<5\}$, 
also $A \cap B$.

Wie man sieht, fallen ca. 20% der Stichproben in den entsprechenden Bereich. 


Da viele Probleme, wenn sie komplexer werden, kaum noch "analytisch" (d.h. wie Ausrechnen von Integralen) lösbar sind,
greift man in der modernen (Analyse-)Welt oft lieber auf Simulationsverfahren zurück - Dank sei den schnellen Rechnern.
Für uns Menschen ist damit die Aufgabe des Integrierens auf schnödes Zählen zurückgeführt.







### Binomialverteilung {#sec-bin-distrib}

:::{#def-binvert}
### Binomialverteilung
Die Binomialverteilung dient zur Darstellung der Wahrscheinlichkeit der Ergebnisse eines wiederholten binomialen Zufallexperiments,
eines Zufallsexperiments mit *zwei* Ergebnissen bzw. Elementarereignissen also. 
Typisches Beispiel ist ein Münzwurf.
Bei jeder Wiederholung des Zufallexperiments bleibt die Wahrscheinlichkeit der Ergebnisse gleich: Die Münze verändert sich nicht durch die Würfe (*Ziehen mit Zurücklegen*, ZmZ). 
Außerdem hat ein bestimmtes Ergebnis im ersten Wurf keinen Einfluss auf die Wahrscheinlichkeit eines bestimmten Ergebnisses im zweiten Wurf, etc.$\square$ 
:::




#### Veranschaulichung

Stellen wir uns eine Kistchen^[In den Lehrbüchern häufig als Urne bezeichnet, was den bösen Spott von "Friedhofstatistik" nach sich zog.] mit 5 Losen vor, darunter 2 *T*reffer (Gewinn) und 3 *N*ieten, s. Abb. @fig-urne.
Der Versuch läuft so ab: Wir ziehen ein Los, schauen ob es ein Treffer ist oder nicht, legen es zurück und ziehen erneut.



```{r echo = FALSE}
#| fig-cap: Ein Kästchen mit 5 Losen, darunter 2 Treffer und 3 Nieten.
#| label: fig-urne
#| fig-asp: 0.2
d <- 
  tibble(id = 1:5,
         event = c("T", "T", "N", "N", "N"))


ggplot(d) +
  aes(x = id) +
  theme_minimal() +
  annotate("rect", xmin = 0, xmax = 6, ymin = 0, ymax = 2, fill = "grey80", alpha = .8) +
  geom_point(size = 10, y = 1, aes(color = event) )+
  geom_text(aes(label = event), y = 1) +
  theme(axis.text = element_blank()) +
  labs(x = "", fill = "")
```





:::callout-important

Um die Wahrscheinlichkeitsverteilung einer binomialverteilte Zufallsvariable ausrechnen zu können, muss man zwei Dinge wissen: Erstens die Anzahl der Züge, $n$ (Stichprobengröße) und zweitens die Trefferwahrscheinlichkeit, $p$. 

:::


:::{exr-bin1}
### Vier Lose, zwei Treffer
Wie groß ist die Wahrscheinlichkeit von $A^{\prime}$, d.h. bei $n=4$ Zügen $x=2$ Treffer zu erzielen (und $n-x=2$ Nieten), gegeben dass die Trefferwahrscheinlichkeit bei $p=2/4$ liegt?
Wir ziehen dabei *ohne* Zurücklegen (ZoZ). Außerdem sind die Lose nicht zu unterscheiden (abgesehen davon, ob es Treffer oder Nieten sind).$\square$
:::


Wir könnten jetzt ein Baumdiagramm zeichnen und pro Pfad die Wahrscheinlichkeit ausrechnen  (Multiplikationssatz), vgl. @fig-dreimuenzen.
Die Summe der Wahrscheinlichkeiten der Pfade ist dann die gesuchte Wahrscheinlichkeit, $W$ (Additionssatz).
Das ist einfach, dauert aber.
In diesem Fall ist die Wahrscheinlichkeit eines (günstigen) Pfades, $A$:

$P(A) = P(T)^2 \cdot P(N)^2 = \left( \frac{2}{5} \right)^2 \cdot \left( \frac{3}{5} \right) ^2$.

```{r}
p_a = (2/5)^2 * (3/5)^2
p_a
```

Etwas mühevolles Zählen der Pfade würde uns zeigen, dass es $k=6$ Pfade gibt, die alle die gleiche Wahrscheinlichkeit, $P(A)$, aufweisen.
Damit beträgt die Wahrscheinlichkeit des gesuchten Ereignisses $A^{\prime}$ (2 Treffer bei 4 Zügen):

$P(A^{\prime}) = 6 \cdot P(A)$.

```{r}
p_a_strich = 6 * p_a
p_a_strich
```


Mithilfe der Formel der Binomialverteilung lässt sich das Ergebnis, die Wahrscheinlichkeit von $A^{\prime}$  schneller ausrechnen. 
Einfach gesprochen sieht sie so aus:



$$P(A^{\prime}) = k \cdot P(A)$$
Dabei steht $k$ für die Anzahl der günstigen Pfade und $P(A)$ für die Wahrscheinlichkeit eines günstigen Pfades (d.h. 2 Treffer und 2 Nieten) und alle Pfade haben die gleiche Wahrscheinlichkeit.

Die Anzahl der Pfade kann man mit dem *Binomialkoeffizient* ausrechnen, den man so darstellt:^[wobei gelten muss $n \ge k$]

$k = \tbinom{n}{k}= \frac{n!}{k!(n-k)!}$

Lies: "Wähle aus $n$ möglichen Ereignissen (Pfade im Baum) $k$ günstige Ereignisse (günstige Pfade) oder kürzer "k aus n".

Auf Errisch geht das so:


:::: {.columns}

::: {.column width="50%"}
>    👨‍🏫 Hey R, Wie viele Möglichkeiten gibt es, aus $n=4$ Pfaden "$k=2$ auszuwählen?


:::

::: {.column width="50%"}
>    🤖 Äh, Moment, oh herzliches Leberwesen
```{r}
choose(4,2)
```
:::

::::

Hier ist ein Überblick der möglichen 6 Elementareignisse des Experiments:
1. TTNN, 2. TNTN, 3. TNNT, 4. NTTN ,5. NTNT, 6. NNTT.

```{r}
#| echo: false
# Define the values of n and k
n <- 4  # Total number of items
k <- 2  # Number of items to choose

# Generate all combinations
combinations <- combn(n, k) |> t() |> as_tibble()

```








#### Rechnen mit R


Die Binomialverteilung ist in R eingebaut; man kann sich leicht entsprechende Wahrscheinlichkeiten ausrechnen lassen.

Die Wahrscheinlichkeit, bei 4 Zügen 2 Treffer zu erzielen mit $p=2/5$ unter der Annahme einer Binomialverteilung lässt sich so mit R berechnen:

```{r}
dbinom(x = 2, size = 4, prob = 2/5)
```



::: {#exm-binom}
### Pumpstation-Beispiel zur Binomialverteilung

In einer Pumpstation arbeiten 7 Motoren, die wir als identisch annehmen. Mit einer Wahrscheinlichkeit von 5% fällt ein Motor aus und ist für den Rest des Tages nicht einsatzbereit. Der Betrieb kann aufrecht erhalten werden, solange mindestens 5 Motoren arbeiten. Wie groß ist die Wahrscheinlichkeit, dass die Pumpstation aus dem Betrieb fällt?

$P(X=k)$ (oder kurz: $P(k)$) gibt die Wahrscheinlichkeit (Wahrscheinlichkeitsfunktion) an für das Ereignis, dass *k* Motoren arbeiten.

Lassen wir R mal $P(X=5)$ ausrechnen.


```{r}
dbinom(x = 5, size = 7, prob = .95)
```


Es gilt also $P(X=5) \approx .04$. Die Wahrscheinlichkeit, dass (nur) 5 Motoren laufen an einem beliebigen Tag ist relativ gering^[wobei "gering" subjektiv ist, die Betreiberfirma findet diese Wahrscheinlichkeit, dass 2 Pumpen ausfallen, wohl viel zu hoch.]. 


`dbinom()` steht für die Wahrscheinlichkeits*d*ichte (im diskreten Fall, also hier, Wahrscheinlichkeitsfunktion genannt) und `binom` für die Binomialverteilung. `x` gibt die Anzahl der Treffer an (das gesuchte Ereignis, hier 5 Motoren arbeiten); `size` gibt die Stichprobengröße an (hier 7 Motoren).

Damit gilt:

$P(X\ge 5) = P(X=5) + P(X=6) + P(X=7)$

```{r}
p_5 <- dbinom(x = 5, size = 7, prob = .95)
p_6 <- dbinom(x = 6, size = 7, prob = .95)
p_7 <- dbinom(x = 7, size = 7, prob = .95)

p_mind_5 <- p_5 + p_6 + p_7

p_mind_5
```

Die Wahrscheinlichkeit, dass mind. 5 Motoren arbeiten beträgt also `r round(p_mind_5, 4)`.


Das Komplement zu diesem Ereignis ist, dass *nicht* mind. 5 Motoren arbeiten, also höchstens 4 und es daher zu einem Ausfall kommt.

Natürlich gilt $P(\bar{X}) = 1- P(X)$.

```{r}
p_weniger_als_4 <- 1 - p_mind_5
p_weniger_als_4
```


Alternativ kann man mit der Verteilungsfunktion rechnen: $P(X \le 4)$.




In R kann man dafür die Funktion `pbinom()` nutzen (p für (kumulierte) Wahrscheinlichkeit).

```{r}
pbinom(q = 4, size = 7, prob = .95)
```

`q = 4` steht für $X \le 4$, also für höchstens 4 Treffer (arbeitende Motoren); `size = 7` meint die Stichprobengröße, hier 7 Motoren.

:::


:::callout-important

Die Funktion, die die Wahrscheinlichkeit dafür angibt, dass die diskrete Zufallsvariable $X$ eine Realisation annimmt, die kleiner oder gleich (höchstens) einem Wert $X=x$ ist, heißt *Verteilungsfunktion*.

$F(X=x) = P(X \le x)$

:::



#### Simulieren


Die Binomialverteilung lässt sich gut als "Münzwurf-Verteilung" auffassen.

Werfen wir eine Münze und sehen wir, was passiert.


```{r}
sample(x = c(0, 1), size = 1)
```


Mit `sample()` ziehen wir eine Stichprobe aus dem Ereignisraum `x`, hier 0 und 1. 
Dabei vereinbaren wir (willkürlich), dass 0 für "Kopf" steht und 1 für "Zahl".
`size = 1` bedeutet, wir werfen die Münze ein Mal (d.h. Stichprobengröße *size* ist 1).

Okay, noch an Bord? Dann werfen wir die Münze 10 Mal:


```{r}
sample(x = c(0, 1), size = 10, replace = TRUE)
```

`replace = TRUE` heißt, wir legen die Münze wieder zurück auf den Tisch, wenn wir sie geworfen haben.
Oder anders ausgedrückt: *Ziehen mit Zurücklegen*.



R, mach dich bereit, wirf die Münze 1000 ($n=10^3$ oder `1e3`) Mal^[R meckert nicht bei langweiligen Aufgaben.]:


```{r}
n <- 1e3

muenze_oft <- 
  sample(x = c(0, 1), size = n, replace = TRUE) 


muenze_oft %>% 
  sum()
```


Mit `sum()` nach dem Pfeifensymbol `%>%` haben wir aus dem Vektor `muenze_oft`, der aus der ersten Zeile resultiert,
die Summe ausgerechnet. 

Jetzt wissen wir, wie oft die Münze "Zahl" gezeigt hat, nämlich `r sum(muenze_oft)` Mal.



::: callout-note
Wenn Sie einen Zufallsversuch wiederholen, muss nicht jedes Mal das gleiche Ergebnis resultieren. Entsprechend wird bei wiederholten Ausführung der Funktion `sample()` nicht immer das gleiche Ergebnis resultieren. Wundern Sie sich also nicht, wenn bei Ihrem Computer eine ähnliche, aber nicht gleiche, Zahl herauskommt.
:::


Visualisieren wir mal unsere Münzwürfe. Dazu erstellen wir zuerst eine geeignete Tabelle, @tbl-muenz.


```{r}
muenz_tab <-
  tibble(
    id = 1:n,
    x = muenze_oft,
    x_cumsum = cumsum(x) / id  # gibt Anteil von "Zahl" wieder
  )
```


```{r}
#| echo: false
#| label: tbl-muenz
#| tbl-cap: "Die kumulierte Summe beim Münzwurf (nur die ersten paar Zeilen)"
head(muenz_tab)
```


Und hier der Anteil von "Zahl" im Verlauf unserer Münzwürfe, s. @fig-lln.


```{r}
#| label: fig-lln
#| fig-cap: "Das Gesetz der großen Zahl am Beispiel der Stabilisierung des Trefferanteils beim wiederholten Münzwurf"
#| fig-asp: 0.3

muenz_tab %>% 
  slice_head(n = 1e3) %>% 
  ggplot() +
  aes(x = id, y = x_cumsum) +
  geom_line()
```


Grob gesagt scheint sich ein Münzwurf nach, naja, vielleicht 500 Würfen "einigermaßen" zu stabilisieren.^[Was "einigermaßen" bedeuten soll, ist kein statistischer Begriff, sondern einer, der im echten Leben von den Menschen beantwortet werden muss, die eine Entscheidung zu treffen haben.]



::: callout-important
Das Gesetz der großen Zahl

Zieht man (zufällig) immer mehr Werte aus einer Verteilung (mit endlichem Mittelwert), nähert sich der Mittelwert der Stichprobe immer mehr mit dem Mittelwert (oft als Erwartungswert bezeichnet) der Verteilung an 
:::



```{r lln, out.width = "100%", fig.align="center", fig.asp = .5}
#| eval: false
#| echo: false
source(paste0(here::here(),"/R-Code/img15.R"))
```



<!-- 3b1b hat ein [nettes Video zu diesem Thema](https://youtu.be/8idr1WZ1A7Q), das sich als Vertiefung eignet. -->




### Normalverteilung


Normalverteilungen haben eine charakteristische Glockenform;
sie sind symmetrisch^[d.h. die Schiefe (`skewness`) ist 0].
Normalverteilungen können sich unterscheiden   Mittelwert $\mu$ und ihrer Streuung, $\sigma$.
Diese beiden Größen ("Parameter") determinieren den Graphen einer bestimmten Normalverteilungsfunktion, s. @fig-norms.
Sind diese beiden Parameter bekannt, so ist die Dichte jedes beliebigen Datenpunkts (aus dieser Normalverteilung) bestimmt.

::: {def-parameter}

### Parameter

Ein Parameter (einer Verteilung) legt die "Varianten" einer Verteilung fest. Durch die Wahl der Parameterwerte nimmt eine Verteilung eine genaue Form an.

:::

![Beispiele von Normalverteilungen mit verschiedenen Mittelwerten und Streuungen, Quelle: Wikipedia](img/normals.png){#fig-norms width=50% fig-align="center"}


Beispiel: Wie groß sind Studentis ([Quelle des Datensatzes](https://rdrr.io/cran/openintro/man/speed_gender_height.html))? 

Das Quantil von z.B. 25% zeigt die Körpergröße der 25% kleinsten Studentis an, analog für 50%, 75%:

```{r QM2-Thema3-Teil1-5, echo = FALSE}
data(speed_gender_height, package = "openintro")

height_summary <- 
  speed_gender_height %>% 
  drop_na(height) %>% 
  mutate(height = height * 2.54) %>% 
  summarise(q25 = quantile(height, prob = .25),
            q50 = quantile(height, prob = .5),
            q75 = quantile(height, prob = .75))

height_summary %>% 
  gt()
```

@fig-quantiles zeigt eine Visualisierung der Quantile.

```{r QM2-Thema3-Teil1-6}
#| echo: false
#| results: hold
#| fig-width: 12
#| fig-asp: 0.618
#| fig-cap: "Quantile verschieden visualisiert"
#| label: fig-quantiles

speed_gender_height <-
  speed_gender_height %>% 
  mutate(height_cm = height * 2.54)

p1 <- 
  speed_gender_height %>% 
  ggplot() +
  aes(x = 1, y = height_cm) +
  geom_boxplot() +
  labs(x = "",
       y = "Größe in cm",
       title = "Die Box zeigt das 25%-, 50%- und 75%-Quantil")

height_summary_long <- 
  height_summary %>% 
  pivot_longer(everything(),
               names_to = "q",
               values_to = "height_cm") 

p2 <- 
  speed_gender_height %>% 
  ggplot() +
  aes(x = height_cm) +
  geom_histogram() +
  geom_vline(data = height_summary_long,
             aes(xintercept = height_cm)) +
  geom_text(data = height_summary_long,
             aes(x = height_cm+1,
                 y = 0,
                 label = paste0(q, ": ",height_cm)),
             angle = 90,
            hjust = 0,
            color = "white"
             ) +
  labs(title = "Die vertikalen Striche zeigen die Quantile",
       y = "Häufigkeit")  +
  theme_minimal()
 
plots(p1, p2)
```


::: callout-note
Das 25%-Quantil nennt man *1. Quartil*, das 50%-Quantil auch *2. Quartil*, das 75%-Quantil das *3. Quartil*, und das 100%-Quantil (Maximalwert) das *4. Quartil*.
:::








#### Normal auf dem Fußballfeld

Sie und 100 Ihrer besten Freunde stehen auf der Mittellinie eines Fußballfelds. Auf Kommando werfen alle jeweils eine Münze; bei Kopf geht man einen Schritt nach links, bei Zahl nach rechts. Das wird 16 Mal wiederholt. Wie wird die Verteilung der Positionen wohl aussehen?


```{r Normalverteilung-6, fig.asp = .45, fig.width=7}
#| echo: false
source(paste0(here::here(),"/R-Code/img13.R"))
```

[@mcelreath_statistical_2020]







#### Normal durch Addieren

Die Summe vieler (gleich starker) Zufallswerte (aus der gleichen Verteilung) erzeugt eine Normalverteilung; egal aus welcher Verteilung die Zufallswerte kommen (Zentraler Grenzwertsatz), vgl. @fig-fussball.


```{r Normalverteilung-7, out.width="100%", fig.asp = 0.5, fig.align="center", fig.width=7}
#| echo: false
#| fig-cap: "Entstehen einer Normalverteilung durch Addition vieler unabhgängiger Ereignisse"
#| label: fig-fussball
source(paste0(here::here(),"/R-Code/img14.R"))
```



Nicht verwechseln:


```{r Normalverteilung-8, echo = FALSE, out.width="30%", fig.align='center'}
knitr::include_graphics("img/ch33910f1.jpg")
```






#### Normalverteilung vs. randlastige Verteilungen


```{r Normalverteilung-9, fig.asp=0.5}
#| echo: false
ggplot(data = data.frame(x = c(-3, 3)), aes(x)) +
  stat_function(fun = dnorm, n = 101, args = list(mean = 0, sd = 1), color = "blue") +
  labs(y = "Dichte", x = "Merkmal, X") +
  stat_function(fun = dt, n = 101, args = list(df = 1, ncp =0), color = "red") +
  labs(caption = "Blau: Normalverteilung\nRot: randlastige Verteilung (t-Verteilung mit df=1)")
```


Bei randlastigen Verteilungen ("fat tails") kommen Extremereignisse viel häufiger vor als bei Normalverteilungen. Deshalb ist es wichtig sein, zu wissen, ob eine Normalverteilung oder eine randlastige Verteilung vorliegt. Viele statistische Methoden sind nicht zuverlässig bei (stark) randlastigen Methoden.



:::::{#exm-normal-rand}
### Beispiele für Normal- und randlastige Verteilungen


:::: {.columns}

::: {.column width="50%"}
Normal verteilt:

- Größe 
- Münzwürfe
- Gewicht
- IQ
- Blutdruck
- Ausschuss einer Maschine
:::

::: {.column width="50%"}
Randlastig verteilt:

- Vermögen
- Verkaufte Bücher
- Ruhm
- Aktienkurse
- Erdbeben
- Pandemien
- Kriege
- Erfolg auf Tinder
- Meteroritengröße
- Stadtgrößen

:::

::::

:::::





#### Formel der Normalverteilung

Vereinfacht ausgedrückt lässt die Normalverteilung $\mathcal{N}$ durch Exponenzieren einer Quadratfunktion beschreiben:

$$\mathcal{N} \propto e^{-x^2}$$

mit $e=2.71...$, der Eulerschen Zahl.^[Das Zeichen $y \propto x$ bedeutet "x ist proportional zu y", also $y = mx$.]


Wie man sieht (@fig-normal1) ergibt sich eine Normalverteilung. 


```{r norm, eval = FALSE, echo = TRUE}
d <-
  tibble(
    x = seq(-3, 3, 
            length.out = 100),
    y = exp(-x^2)
  )

ggline(d, x = "x",y = "y")  # aus {ggpubr}
```



```{r Normalverteilung-10, echo = FALSE}
#| fig-asp: 0.5
#| label: fig-normal1
#| fig-cap: "Wir basteln uns eine Normalverteilung"
d <-
  tibble(
    x = seq(-3, 3, 
            length.out = 100),
    y = exp(-x^2)
  )

d %>% 
  ggplot() +
  aes(x = x, y = y) +
  geom_line()
```


Eine Normalverteilung mit $\mu=0$ und $\sigma=1$ nennt man auch *Standardnormalverteilung* und 
man schreibt:


$$IQ \sim \mathcal{N}(0,1)$$

Die Normalverteilung wird auch *[Gauss](https://de.wikipedia.org/wiki/Carl_Friedrich_Gau%C3%9F)-Verteilung* oder *Glockenkurve* genannt.





#### Simulation einer Normalverteilung


R hat eine Funktion eingebaut zur Erzeugung von Zufallszahlen (Zufallszahlengenerator), z.B. normalverteilte.
Man übergibt dieser Funktion den gewünschten Mittelwert und die gewünschte Streuung und die Funktion zieht dann zufällig Werte aus dieser Verteilung.

Diesen Zufallszahlengenerator kann man mit einem Duschkopf vergleichen, s. @fig-shower.
An diesem Duschkopf kann man einen Schwenker einstellen, der den Duschkopf
ausrichtet, also steuert, ob die Wassertropfen weit in die eine oder die andere
Richtugn fallen.
Zweitens hat unser Duschkopf noch einen Streuregler,
der den Wasserstrahl entweder eng bündelt^[Massagedusche, behauptet der Hersteller] oder weit auseinanderfächert. Im ersten Fall fällt der Wasserstrahl eng und schmal aus. Im zweiten Fall fällt der Wasserstrahl breit aus.

![Zufallszahlengenerator als Duschkopf](img/shower-data.png){#fig-shower width="50%"}

[Quelle](https://jkkweb.sitehost.iu.edu/KruschkeFreqAndBayesAppTutorial.html#data_are_described_by_mathematical_models): John Kruschke.



Eine Zufallszahl (*r*andom number), die *norm*alverteilt ist, mit $\mu=0$ und $\sigma=1$ kann man in R so erzeugen:


```{r}
rnorm(n = 1, mean = 0, sd = 1)
```



Ein Fallbeispiel: Der Inhalt einer Tüte mit Zucker, $X$, sei normalverteilt mit $\mu = 10002$ g und $\sigma=1.5$ g. Aus vertragsrechtlichen Gründen darf das Füllgewicht von 1000g nicht unterschritten werden, sonst drohen Konventionalstrafen.

Wie groß ist die Wahrscheinlichkeit, dass 1000g unterschritten werden?


Simulieren wir uns 1e4 Zuckertüten!

```{r}
n <- 1e4
d <- 
  tibble(
    id = 1:n,
    x = rnorm(n = n, mean = 1002, sd = 1.5)
  )

head(d)
```


Zählen wir, viele der Zuckertüten ein Gewicht von weniger als 1000g aufweisen:


```{r}
d %>% 
  count(x < 1000)
```


Ein ziemlich^["Ziemlich" ist natürlich subjektiv; je nach Situation kann es zu viel oder nicht zu viel sein.] kleiner Anteil. Rechnen wir uns noch die Anteile (*prop*ortion) aus:

```{r}
d %>% 
  count(x < 1000) %>% 
  mutate(prop = n/1e4)
```







### IQ-Verteilung

Die Verteilung der Zufallsvariablen IQ ist normalverteilt mit einem Mittelwert von 100 und einer Streuung von 15, s. @fig-norm-100-15:

$IQ \sim \mathcal{N}(100,15)$

:::{#exr-iq}
### Wie schlau muss man (nicht) sein?
- Wie schlau muss man sein, um zu den unteren 75%, 50%, 25%, 5%, 1% zu gehören?
- Anders gesagt: Welcher IQ-Wert wird von 75%, 50%, ... der Leute nicht überschritten?$\square$
:::


![Visualisierung der theoretischen IQ-Verteilung](img/norm-100-15.png){#fig-norm-100-15 width="50%"}

[Quelle:](https://jkkweb.sitehost.iu.edu/KruschkeFreqAndBayesAppTutorial.html#data_are_described_by_mathematical_models): John Kruschke.


Ziehen wir zufällig $1e4$ Stichproben aus $\mathcal{N}(100,15)$ und berechnen die Quantile:

```{r echo = TRUE}
d <-
  tibble(
  iq = rnorm(n = 1e4, 
             mean = 100, 
             sd = 15))

probs <- c(0.75,.5,.25,.05,.01)

d_summary <- d %>% 
  summarise(p = probs,
            q = quantile(iq, probs))
```



```{r}
#| echo: false
d_summary %>% 
  gt() %>% 
  fmt_number(p, decimals = 2) %>% 
  fmt_number(q, decimals = 0)
```

Das *Quantil* $q$ zur kumulierten Wahrscheinlichkeit $p=75$ ist 110, etc. 






Umgekehrt können wir uns auch fragen: Gegeben einer Realisation der Zufallsvariablen (z.B. IQ), was ist die zugehörige Wahrscheinlichkeit (Wert der Verteilungsfunktion?)


:::{#exr-schlau2}
### Wie schlau muss man (nicht) sein, Teil 2
- Welcher Anteil der Fläche unter der Kurve $p$ gehört zu den IQ-Werten 75, 100, 115, 130?
- Anders gesagt: Welcher Anteil der Wahrscheinlichkeitsmasse der Verteilung liegt unter IQ=75, IQ=100, etc.?$\square$
:::


Ziehen wir Stichproben aus $\mathcal{N}(100,15)$:

```{r echo = TRUE, eval = FALSE}
d <-
  tibble(
    iq = rnorm(1e4, 
               mean = 100, 
               sd = 15)) %>% 
  mutate(iq = round(iq))

qs <- c(75,100,115,130)

d %>% 
  count(p_100 = iq < 100) %>% 
  mutate(prop = n / sum(n)) 
```



```{r echo = FALSE, eval = TRUE}
d <-
  tibble(
    iq = rnorm(1e4, 
               mean = 100, 
               sd = 15)) %>% 
  mutate(iq = round(iq))

qs <- c(75,100,115,130)

d %>% 
  count(p_100 = iq < 100) %>% 
  mutate(prop = n / sum(n)) %>% 
  gt() %>% 
  fmt_number(columns = 3)
```

Anstelle von `iq < 100` kann man `iq < 115` einsetzen, etc.
 
```{r eval= FALSE}
#| echo: false
d %>% 
  mutate(prop = percent_rank(iq)) %>% 
  filter(iq %in% qs) %>% 
  distinct(iq, .keep_all = TRUE)
```

Die *Verteilungsfunktion* (der Anteil der Wahrscheinlichkeitsmasse), `p`, für IQ-Werte nicht größer als 100,  $IQ\le100$, ist 50%, etc. 



#### Quantile der Normalverteilung 





- *Quantile* teilen eine Verteilung so ein, dass ein Anteil $p$ kleiner oder gleich und der andere Teil $1-p$ größer  dem Quantil $q$ ist.
    - *Beispiel*: "50%-Quantil = 100" meint, dass 50% der Elemente der Verteilung einen Wert kleiner oder gleich als 100 haben.
    
- Die *Verteilungsfunktion F* (für einen Wert $x$) gibt die Wahrscheinlichkeit an, dass die zugehörige Zufallsvariable $X$ einen Wert höchstens so groß wie $x$ annimmt. Sie zeigt also die kumulierte Wahrscheinlichkeit $[-\infty, q)$.
    - *Beispiel*: "F(100) = 50%" meint: Die Wahrscheinlichkeit für eine Ausprägung von höchstens als 100 beträgt 50%.


Schauen wir uns die Quartile der Normalverteilung einmal näher an.
Wir gehen von einer Normalverteilung aus, wie sie zur Beschreibung von Intelligenz (IQ) verwendet wird, s. @fig-nv-quants.


```{r }
#| echo: false
#| label: fig-nv-quants
#| fig-cap: Quantile der Normalverteiltung

  

p1 <- 
  ggplot(NULL, aes(c(60,145))) +
  geom_area(stat = "function", fun = dnorm, fill = "grey", args = list(mean=100,sd = 15), xlim= c(60, 100)) +
  geom_line(stat = "function", fun = dnorm, args = list(mean=100,sd = 15)) +
  labs(x = "X", y = "Dichte",
       title = "50%-Quantil: 100; Verteilungsfunktion von 100:50%") +
  scale_y_continuous(breaks = NULL)

p2 <-
    ggplot(NULL, aes(c(60,145))) +
  geom_area(stat = "function", fun = dnorm, fill = "grey", args = list(mean=100,sd = 15), xlim= c(60, 125)) +
  geom_line(stat = "function", fun = dnorm, args = list(mean=100,sd = 15)) +
  labs(x = "X", y = "Dichte",
       title = "95%-Quantil: 125; Verteilungsfunktion von 125:95%") +
  scale_y_continuous(breaks = NULL)


plots(p1, p2)
```

$$IQ \sim \mathcal{N}(100, 15)$$
Mit R kann man sich die beiden Größen komfortabel berechnen lassen:

```{r echo = TRUE, eval = FALSE}
qnorm(.50, mean = 100, sd = 15)  # 50%-Quantil
pnorm(100, mean = 100, sd = 15)  # Verteilungsfunktion für IQ=100
```


Betrachten wir einige wichtigen Quantile, s. @fig-nv-quants2.

```{r}
#| echo: false
q_p50 <-
  ggplot(NULL, aes(c(60,145))) +
  geom_area(stat = "function", fun = dnorm, fill = "grey", args = list(mean=100,sd = 15), xlim= c(60, 100)) +
  geom_line(stat = "function", fun = dnorm, args = list(mean=100,sd = 15)) +
  labs(x = "IQ", y = "Dichte",
       title = "50%-Quantil: 100") +
  scale_y_continuous(breaks = NULL)
```


```{r}
#| echo: false
q_inv <- .25
q_p <- qnorm(q_inv, mean = 100, sd= 15)
p_q <- pnorm(q_p, mean = 100, sd= 15)

q_p25 <-
  ggplot(NULL, aes(c(60,145))) +
  geom_area(stat = "function", fun = dnorm, fill = "grey", args = list(mean=100,sd = 15), xlim= c(60, q_p)) +
  geom_line(stat = "function", fun = dnorm, args = list(mean=100,sd = 15)) +
  labs(x = "IQ", y = "Dichte",
       title = paste0(q_inv,"-Quantil: ",round(q_p)),
       caption = "MW-0.68sd") +
  scale_y_continuous(breaks = NULL)
```


```{r}
#| echo: false
q_inv <- .95
q_p <- qnorm(q_inv, mean = 100, sd= 15)

q_p95 <-
  ggplot(NULL, aes(c(60,145))) +
  geom_area(stat = "function", fun = dnorm, fill = "grey", args = list(mean=100,sd = 15), xlim= c(60, q_p)) +
  geom_line(stat = "function", fun = dnorm, args = list(mean=100,sd = 15)) +
  labs(x = "IQ", y = "Dichte",
       title = paste0(q_inv,"-Quantil: ",round(q_p)),
       caption = "MW+1.64sd") +
  scale_y_continuous(breaks = NULL)
```



```{r}
#| echo: false
q_inv <- .975
q_p <- qnorm(q_inv, mean = 100, sd= 15)
#pnorm(115, mean= 100, sd = 15)

q_p975 <-
  ggplot(NULL, aes(c(60,145))) +
  geom_area(stat = "function", fun = dnorm, fill = "grey", args = list(mean=100,sd = 15), xlim= c(60, q_p)) +
  geom_line(stat = "function", fun = dnorm, args = list(mean=100,sd = 15)) +
  labs(x = "IQ", y = "Dichte",
       title = paste0(q_inv,"-Quantil: ",round(q_p)),
       caption = "MW+2SD") +
  scale_y_continuous(breaks = NULL)
```





```{r}
#| echo: false
q_inv <- .84
q_p <- qnorm(q_inv, mean = 100, sd= 15)
#pnorm(115, mean= 100, sd = 15)

q_p84 <-
  ggplot(NULL, aes(c(60,145))) +
  geom_area(stat = "function", fun = dnorm, fill = "grey", args = list(mean=100,sd = 15), xlim= c(60, q_p)) +
  geom_line(stat = "function", fun = dnorm, args = list(mean=100,sd = 15)) +
  labs(x = "IQ", y = "Dichte",
       title = paste0(q_inv,"-Quantil: ",round(q_p)),
       caption = "MW+1sd") +
  scale_y_continuous(breaks = NULL)
```

```{r}
#| echo: false
q_inv <- .69
q_p <- qnorm(q_inv, mean = 100, sd= 15)  # halbe SD
#pnorm(107.5, mean= 100, sd = 15)

q_p69 <-
  ggplot(NULL, aes(c(60,145))) +
  geom_area(stat = "function", fun = dnorm, fill = "grey", args = list(mean=100,sd = 15), xlim= c(60, q_p)) +
  geom_line(stat = "function", fun = dnorm, args = list(mean=100,sd = 15)) +
  labs(x = "IQ", y = "Dichte",
       title = paste0(q_inv,"-Quantil: ",round(q_p)),
       caption = "MW+0.5sd") +
  scale_y_continuous(breaks = NULL)
```



```{r fig-nv-quants2}
#| echo: false
#| label: fig-nv-quants2
#| fig-width: 10
#| fig-cap: Verschiedene Quantil der Normalverteilung

(q_p50 + q_p25 + q_p69) / (q_p95 + q_p975 + q_p84)
```


#### Standardnormalverteilung


```{r Normalverteilung-3}
#| echo: false
ggplot(data = data.frame(x = c(-3, 3)), aes(x)) +
  stat_function(fun = dnorm, n = 101, args = list(mean = 0, sd = 1)) +
  labs(y = "Dichte", x = "Merkmal, X") +
  ggtitle("Normalverteilung mit Mittelwert 0 und SD 1")
```

Bei $X=0$:

- hat eine Einheit von $X$ die Wahrscheinlichkeitsmasse von 40% (Wahrscheinlichkeitsdichte)
- sind 50% der Wahrscheinlichkeitsmasse (Fläche unter der Kurve) kleiner als dieser Wert (Verteilungsfunktion).

In Summe liegen 100% der Wahrscheinlichkeitsmasse unter der Kurve.


#### Normalverteilung als konservative Wahl

Dem Mathematiker [Carl Friedrich Gauss](https://de.wikipedia.org/wiki/Carl_Friedrich_Gau%C3%9F) (s. @fig-gauss) wird die Ehre zuerkannt,
die Normalverteilung eingeführt zu haben.

```{r Normalverteilung-1-bis, echo = FALSE }
#| fig-align: "center"
#| label: fig-gauss
#| fig-cap: "Zehn-Mark-Geldschein mit Gauss und Normalverteilung"
knitr::include_graphics("img/10_Deutsche_Mark_-_detail.png")
```

Quelle: Uni Greifswald, Public domain, via Wikimedia Commons


::: callout-note
*Ontologische Begründung*

- Wirken viele, gleichstarke Einflüsse additiv zusammen, entsteht eine Normalverteilung  [@mcelreath_statistical_2020], Kap. 4.1.4.

*Epistemologische Begründung*

- Wenn wir nur wissen, dass eine Variable über einen endlichen Mittelwert und eine endliche Varianz verfügt und wir keine weiteren Annahmen treffen bzw. über kein weiteres Vorwissen verfügen, dann ist die Normalverteilung die plausibelste Verteilung (maximale Entropie) [@mcelreath_statistical_2020], Kap. 7 und 10.
:::


<!-- ### Zweidimensionale Normalverteilung, unkorreliert -->


<!-- ```{r Normalverteilung-11, out.width="70%", fig.align="center"} -->
<!-- #| echo: false -->
<!-- knitr::include_graphics("https://github.com/sebastiansauer/QM2-Folien/raw/main/img/mult-norm.png") -->
<!-- ``` -->


<!-- [Quelle](https://tex.stackexchange.com/questions/31708/draw-a-bivariate-normal-distribution-in-tikz) -->

<!-- [Vgl. auch dieses Diagramm](http://ballistipedia.com/index.php?title=File:Bivariate.png)] -->




<!-- ### 2D-Normalverteilung mit R, unkorreliert -->

<!-- $r(X,Y) = 0$ -->




<!-- ```{r norm-plot1} -->
<!-- #| echo: false -->
<!-- #| eval: false -->
<!-- #| fig-asp: 1 -->
<!-- #| out-width: "50%" -->
<!-- d1 <-  -->
<!--   tibble(  -->
<!--     x=rnorm(1e4),  -->
<!--     y=rnorm(1e4) -->
<!--   ) -->

<!-- ggplot(d1) + -->
<!--   aes(x, y) + -->
<!--   geom_hex()  -->

<!-- ggplot(d1) + -->
<!--   aes(x, y) + -->
<!--   geom_density2d() -->
<!-- ``` -->

<!-- [[ggplot-Referenz](https://ggplot2.tidyverse.org/reference/geom_density_2d.html), [Quellcode](https://www.r-graph-gallery.com/2d-density-plot-with-ggplot2.html)] -->

<!-- Mit `scale_fill_continuous(type = "viridis")`kann man die Farbpalette der Füllfarbe ändern. Nicht so wichtig. -->


<!-- ```{r Normalverteilung-2-bis} -->
<!-- #| echo: false -->
<!-- #| eval: true -->
<!-- #| fig-asp: 1 -->
<!-- #| out-width: "50%" -->
<!-- d1 <-  -->
<!--   tibble(  -->
<!--     x=rnorm(1e4),  -->
<!--     y=rnorm(1e4) -->
<!--   ) -->

<!-- ggplot(d1) + -->
<!--   aes(x, y) + -->
<!--   geom_hex() + -->
<!--   scale_fill_viridis_c() + -->
<!--   theme(legend.position = "bottom") -->

<!-- ggplot(d1) + -->
<!--   aes(x, y) + -->
<!--   geom_density2d() -->
<!-- ``` -->



<!-- ### 2D-Normalverteilung mit R, korreliert, r=0.7 -->



<!-- Die ersten paar Zeilen der Daten: -->

<!-- ```{r Normalverteilung-3-bis, echo = FALSE} -->
<!-- d2 <- rnorm_multi( -->
<!--   n = 1e4, -->
<!--   mu = c(0,0), -->
<!--   sd = c(1, 1), -->
<!--   r = (0.7) -->
<!-- ) -->
<!-- ``` -->

<!-- ```{r Normalverteilung-4-bis} -->
<!-- #| echo: false -->
<!-- d2 %>%  -->
<!--   head(n=3) %>%  -->
<!--   gt() %>%  -->
<!--   fmt_number(columns = everything()) -->
<!-- ``` -->

<!-- Berechnen wir die Korrelation `r`: -->

<!-- ```{r Normalverteilung-5-bis, echo = TRUE, eval = FALSE} -->
<!-- d2 %>%  -->
<!--   summarise( -->
<!--     r = cor(X1,X2), -->
<!--     n = n() -->
<!--   ) -->
<!-- ``` -->

<!-- ```{r Normalverteilung-6-bis} -->
<!-- #| echo: false -->
<!-- d2 %>%  -->
<!--   summarise( -->
<!--     r = cor(X1,X2), -->
<!--     n = n() -->
<!--   ) %>%  -->
<!--   gt() %>%  -->
<!--   fmt_number(columns = everything()) -->
<!-- ``` -->




<!-- ```{r Normalverteilung-7-bis} -->
<!-- #| echo: false -->
<!-- ggplot(d2) + -->
<!--   aes(X1, X2) + -->
<!--   geom_hex() + -->
<!--   scale_fill_viridis_c() + -->
<!--   theme(legend.position = "bottom") -->

<!-- ggplot(d2) + -->
<!--   aes(X1, X2) + -->
<!--   geom_density2d() -->
<!-- ``` -->









## Aufgaben


Zusätzlich zu den Aufgaben im Buch:


- [Lose-Nieten-Binomial-Grid](https://datenwerk.netlify.app/posts/lose-nieten-binomial-grid/lose-nieten-binomial-grid)
- [Bsp-Binomial](https://datenwerk.netlify.app/posts/bsp-binomial/bsp-binomial)






## ---



![](img/outro-04.jpg){width=100%}




