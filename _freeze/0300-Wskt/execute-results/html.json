{
  "hash": "1b1b309e675731338692b1e9c6c7ec29",
  "result": {
    "markdown": "\n# Wahrscheinlichkeit\n\n\n\n\n::: {.cell hash='0300-Wskt_cache/html/r-setup_348d541a626e3a643fc820de2e96691a'}\n\n:::\n\n::: {.cell hash='0300-Wskt_cache/html/define-plots-16_2c68c699a8d30428ed84f2445218ca3a'}\n\n:::\n\n\n## Lernsteuerung\n\n\n### Lernziele\n\nNach Absolvieren des jeweiligen Kapitels sollen folgende Lernziele erreicht sein.\n\nSie können ...\n\n\n- die Grundbegriffe der Wahrscheinlichkeitsrechnung erläuternd definieren\n- die drei Arten der direkten Ermittlung von Wahrscheinlichkeit erläutern\n- typische Relationen (Operationen) von Ereignissen anhand von Beispielen veranschaulichen\n- mit Wahrscheinlichkeiten rechnen\n\n\n\n### Eigenstudium\n\n\n:::{.callout-note}\nDieses Kapitel ist selbständig im Eigenstudium vorzubereiten vor dem Unterricht.\nLesen Sie dazu die angegebene Literatur.$\\square$\n:::\n\n\n### Prüfungsrelevanter Stoff\n\nLesen Sie dazu @bourier_2018, Kap. 2-4. \nWeitere Übungsaufgaben finden Sie im dazugehörigen Übungsbuch, @bourier_statistik-ubungen_2022.\n\n\n\n\n\n\n\n### Zentrale Begriffe\n\n\n#### Grundbegriffe\n\n- Zufallsvorgang (Zufallsexperiment)\n- Elementarereignis\n- Ereignisraum\n- Zufallsereignis (zufälliges Ereignis)\n- Sicheres Ereignis\n- Unmögliches Ereignis\n\n\n#### Wahrscheinlichkeitsbegriffe\n\n- Klassische Wahrscheinlichkeit (LaPlace'sche Wahrscheinlichkeit)\n- Statistische (empirische) Wahrscheinlichkeitsermittlung\n- Subjektive (Bayes) Wahrscheinlichkeitsermittlung\n\n\n#### Wahrscheinlichkeitsrelationen\n\n- Vereinigung von Ereignissen\n- Schnitt(menge) von Ereignissen\n- Komplementärereignis\n- Vollständiges Ereignissystem\n- Kolmogorovs Definition von Wahrscheinlichkeit\n\n\n#### Wahrscheinlichkeitsrechnung\n\n- Allgemeiner Additionsssatz\n- Disjunkte Ereignisse\n- Additionssatz für disjunkte Ereignisse\n- Bedingte Wahrscheinlichkeit\n- (Stochastische) Unabhängigkeit\n- Baumdiagramm für gemeinsame Wahrscheinlichkeit\n- Allgemeiner Multiplikationssatz\n- Multiplikationssatz für unabhängige Ereignisse\n- Totale Wahrscheinlichkeit\n- Satz von Bayes\n\n\n### Begleitvideos\n\n\n- [Video zum Thema Wahrscheinlichkeit](https://youtu.be/rR6NspapEyo)\n\n\n\n\n<!-- ## Unterstützung: Wahrscheinlichkeit in Bildern -->\n\n<!-- Wahrscheinlichkeit in Bildern: zur einfachen Erschließung des Materials, -->\n<!-- ein Unterstützungsangebot. -->\n\n\n<!-- Im Folgenden sind einige Schlüsselbegriffe und -regeln in (ver-)einfach(t)er Form schematisch bzw. visuell dargestellt mit dem Ziel, den Stoff einfacher zu erschließen. -->\n\n\n## Grundbegriffe\n\n\n\n\n\n:::{#exm-muenz}\nKlassisches Beispiel für einen Zufallsvorgang ist das (einmalige oder mehrmalige) Werfen einer Münze.$\\square$\n:::\n\n::{#exr-muenze}\nWerfen Sie eine Münze!\nDiese hier zum Beispiel:\n\n![](img/1024px-Coin-155597.svg.png){width=10% fig-align=\"center\"}\n\n[Quelle: By OpenClipartVectors, CC0]( https://pixabay.com/pt/moeda-euro-europa-fran%C3%A7a-dinheiro-155597)\n\nWiederholen Sie den Versuch 10, nein, 100, nein 1000, nein: $10^6$ Mal.\n\nNotieren Sie das Ergebnis!\n\nOder probieren Sie die [App der Brown University](https://seeing-theory.brown.edu/basic-probability/index.html#section1).\n\n\n:::{#def-zufall}\n### Zufallsvorgang\nEin Zufallsvorgang oder Zufallsexperiment ist eine einigermaßen klar beschriebene Tätigkeit, deren Ergebnis nicht bekannt ist. Allerdings ist die Menge möglicher Ergebnisse sicher und die Wahrscheinlichkeit für die Ergebnisse kann quantifiziert werden.$\\square$\n:::\n\n\n:::{#exr-zufall2}\nNennen Sie Beispiele für Zufallsvorgänge!\\square^[Beispiele für Zufallsexperimente sind das Werfen einer Münze, das Ziehen einer Karte aus einem Kartenspiel, das Messen eines Umweltphänomens wie der Temperatur oder die Anzahl der Kunden, die einen Laden betreten. In jedem dieser Fälle sind die möglichen Ergebnisse nicht im Voraus bekannt und hängen von nicht komplett bekannten Faktoren ab.]\n:::\n\n\n:::callout-caution\nZufall heißt nicht, dass ein Vorgang keine Ursachen hätte. So gehorcht der Fall einer Münze komplett den Gesetzen der Gravitation. Würden wir diese Gesetze und die Ausgangsbedingungen (Luftdruck, Fallhöhe, Oberflächenbeschaffenheit, Gewichtsverteilungen, ...) exakt kennen, könnten wir theoretisch sehr genaue Vorhersagen machen. Der \"Zufall\" würde aus dem Münzwurf verschwinden. Man sollte \"Zufall\" also besser verstehen als \"unbekannt\".$\\square$\n::::\n\n\n\n\n:::{#def-ereignisraum}\n### Grundraum\nDie möglichen Ergebnisse eines Zufallvorgangs fasst man als Menge mit dem Namen *Ereignisraum*[synonym: *Ereignisraum* oder *Grundraum*] zusammen. Man verwendet den griechischen Buchstaben $\\Omega$ für diese Menge.\nDie Elemente $\\omega$ (kleines Omega) von $\\Omega$ nennt man *Ergebnisse*.$\\square$\n:::\n\n:::{#exm-grundraum}\nBeobachtet man beim Würfelwurf die oben liegende Augenzahl, so ist \n\n$$\\Omega = \\{ 1,2,3,4,5,6 \\}$$\n\nein natürlicher Grundraum [@henze_stochastik_2019].$\\square$\n:::\n\n:::{#def-ereignis}\n### Ereignis\nJede Teilmenge von $\\Omega$ heißt *Ereignis.$\\square$\n:::\n\n:::{#exm-ereignis}\nBeim Mensch-ärger-dich-nicht Spielen habe ich eine 6 geworfen.^[Schon wieder.]\nDas Nennen wir das Ereignis $A$: \"Augenzahl 6 liegt oben\"-$\\square$\n\n$A= \\{6\\}$\n:::\n\n\n:::{#def-unm-sich}\n### Unmögliches und sicheres Ereignis\nDie leere Menge $\\varnothing$ heißt das *umögliche*, der Grundraum $\\Omega$ heißt das *sichere Ereignis*. $\\square$\n:::\n\n:::{#exm-unm}\n### Unmögliches Ereignis\nAlois behauptet, er habe mit seinem Würfel eine 7 geworfen.\nSchorsch ergänt, sein Würfel liege auf einer Ecke, so dass keine Augenzahl oben liegt.\nDraco hat seinen Würfel runtergeschluckt.$\\square$\n:::\n\n\n:::{#exm-sicher}\nNach dem der Würfel geworfen wurde, liegt eine Augenzahl zwischen 1 und 6 oben.$\\square$\n:::\n\n\n:::{#def-elementarereignis}\n### Elementarereignis\n\nJede einelementige Teilmenge $\\{\\omega\\}$ heißt *Elementarereignis*.$\\square$\n:::\n\n\n\n### Direkte Ermittlung von Wahrscheinlichkeiten\n\n#### Epistemologische Wahrscheinlichkeit\n\n\nVor uns liegt ein Würfel. Schlicht, ruhig, unbesonders.\nWir haben keinen Grund anzunehmen, dass eine seiner $n=6$ Seiten bevorzugt nach oben zu liegen kommt. \nJedes der sechs Elementarereignisse ist uns gleich plausibel;\nder Würfel erscheint uns fair.\nIn Ermangelung weiteres Wissens zu unserem Würfel gehen wir schlicht davon aus, dass jedes der $n$ Elementarereignis gleich wahrscheinlich ist.\nEs gibt keinerlei Notwendigkeit, den Würfel in die Hand zu nehmen,\num zu einer Wahrscheinlichkeitsaussage auf diesem Weg zu kommen.\nNatürlich *könnten* wir unsere Auffassung eines fairen Würfels testen,\naber auch ohne das Testen können wir eine stringente Aussage (basierend auf unserer Annahme der Indifferenz der $n$ Elementarereignisse) zur Wahrscheinlichkeit eines bestimmten Ereignisses A kommen [@briggs_uncertainty:_2016].\n\n\n$$Pr(A) = 1/n$$\n\n:::{#exm-briggs}\nSei $A$ = \"Der Würfel wird beim nächsten Wurf eine 6 zeigen.\"\nDie Wahrscheinlichkeit für $A$ ist $1/6$.$square$\n:::\n\n\n\n#### Frequentistische Wahrscheinlichkeit\n\nIn Ermangelung einer Theorie zum Verhalten eines (uns) unbekannten Zufallsvorgangs und unter der Vermutung, dass die Elementarereignisse nicht gleichwahrscheinlich sind, bleibt uns ein einfacher (aber aufwändiger) Ausweg: Ausprobieren.\n\nAngenommen, ein Statistik-Dozent, bekannt für seine Vorliebe zum Glücksspiel und scheinbar endlosen Glückssträhnen, er wirft andauernd eine 6, hat seinen Lieblingswürfel versehentlich liegen gelassen. Das ist *die* Gelegenheit!\nSie greifen sich den Würfel, und ... Ja, was jetzt?\nNach kurzer Überlegung kommen Sie zum Entschluss, den Würfel einen \"Praxistest\" zu unterziehen: Sie werfen ihn 1000 Mal (Puh!) und zählen den Anteil der `6`.\nFalls der Würfel fair ist, müsste gelten $Pr(A=6)=1/6\\approx .17$. Schauen wir mal!\n\n\n\n\n::: {.cell hash='0300-Wskt_cache/html/unnamed-chunk-2_0786cd50389f86aa4147d84b032c83ae'}\n\n:::\n\n\n\nUnd hier der Anteil von  `6` im Verlauf unserer Würfe, s. @fig-wuerfel.\n\n\n\n::: {.cell fig.asp='0.5' hash='0300-Wskt_cache/html/fig-wuerfel_26aeb235a2886943b3c914ed04206f9f'}\n\n```{.r .cell-code}\nwuerfel_tab %>% \n  slice_head(n = 1e3) %>% \n  ggplot() +\n  aes(x = id, y = ist_6_cumsum) +\n  geom_hline(yintercept = 1/6, color = \"grey80\", size = 3) +\n  geom_line()\n```\n\n::: {.cell-output-display}\n![Das Gesetz der großen Zahl am Beispiel der Stabilisierung des Trefferanteils beim wiederholten Würfelwurf](0300-Wskt_files/figure-html/fig-wuerfel-1.png){#fig-wuerfel width=672}\n:::\n:::\n\n\nHm, leider ist auf den ersten Blick kein Anzeichen für Schummeln bzw. einen gezinkten Würfel zu finden (zumindest nicht zu Gunsten des Zwielichten Dozenten).\n\n\n\n#### Subjektive Wahrscheinlichkeit\n\nUm subjektiv zu einer Wahrscheinlichkeit zu kommen, sagt man einfach seine Meinung.\nDas hört sich natürlich total plump an. \nUnd tatsächlich besteht die Gefahr, dass die so ermittelten Wahrscheinlichkeiten aus der Luft gegriffen, also haltlos, sind.\n\nAllerdings kann diese Art von Wahrscheinlichkeitsermittlung auch sehr wertvoll sein.\nIn komplizierten Situation im echten Leben kommt man oft in die Situation, dass weder die epistemologische noch die frequentistische Variante verwendet werden kann.\nDann muss man auf Schätzungen, Vorwissen, Erfahrung, theoretischen Überlengungen etc. zurückgreifen.\n\n### Indirekte Ermittlung von Wahrscheinlichkeiten\n\nDie indirekte Ermittlung von Wahrscheinlichkeiten meint das Ableiten von Wahrscheinlichkeitsaussagen, wenn man schon etwas über die Wahrscheinlichkeiten des Grundraums weiß. \nDazu greift man auf Rechenregeln der Stochastik zurück.\nDas hört sich vielleicht wild an, ist aber oft ganz einfach.\n\n:::{exm-ind}\n### Gezinkter Würfel\nEin gezinkter Würfel hat eine erhöhte Wahrscheinlichkeit für das Ereignis $A=$\"6 liegt oben\", und zwar gelte $Pr(A)=1/3$.\nWas ist die Wahrscheinlichkeit, *keine*  `6` zu würfeln?$\\square$^[Die Wahrscheinlichkeit, keine `6` zu würfeln, liegt bei $2/3$.]\n:::\n\nFür das Rechnen mit Wahrscheinlichkeiten ist es hilfreich, ein paar Werkzeuge zu kennen, die wir uns im Folgenden anschauen.\n\n\n### Relationen von Mengen\n\n\n\n\n\nWir gehen von Grundraum $\\Omega$ aus, mit dem Ereignis $A$ als Teilmenge: $A \\subset B$.\n\n\nDa wir Ereignisse als Mengen auffassen, verwenden wir im Folgenden die beiden Begriffe synonym.\n\n\nDabei nutzen wir u.a. Venn-Diagramme.\nVenn-Diagramme eigenen sich, um typische Operationen (Relationen) auf Mengen zu visualisieren. Die folgenden Venn-Diagramme stammen von [Wikipedia (En)](https://en.wikipedia.org/wiki/Venn_diagram).\n\n:::callout-note\n### Wozu sind die Venn-Diagramme gut? Warum soll ich die lernen?\nVenn-Diagramme zeigen Kreise und ihre überlappenden Teile;\ndaraus lassen sich Rückschlüsse auf Rechenregeln für Wahrscheinlichkeiten ableiten.\nViele Menschen tun sich leichter, Rechenregeln visuell aufzufassen als mit Formeln und Zahlen alleine. Aber entscheiden Sie selbst!$\\square$\n:::\n\n\n#### Vereinigung von Ereignissen\n\n:::{#def-mengen-verein}\n### Vereinigung von Ereignissen\nVereinigt man zwei Ereignisse $A$ und $B$, dann besteht das neue Ereignis $C$ genau aus den Elementarereignissen der vereinigten Ereignisse.\nMan schreibt $C = A \\cup B$, lies: \"C ist A vereinigt mit B\".$\\square$\n:::\n\n@fig-cup zeigt ein Venn-Diagramm zur Verdeutlichung der Vereinigung von Ereignissen.\n\n![$A \\cup B$](img/Venn0111.svg.png){fig-cup width=25%}\n\n:::{#exm-mengen-verein}\n\nUm einen (hohen!) Geldpreis zu gewinnen, muss bei ihrem nächsten Wurf mindestens eines der beiden Ereignisse $A$ oder $B$ eintreten.\n\n\\begin{aligned}\nA = \\{1,2\\} \\qquad \\boxed{\\boxed{1\\; 2}\\; \\color{gray}{ 3\\; 4\\; 5\\; 6}} \\\\\nB = \\{2,3\\} \\qquad  \\boxed{1\\; \\boxed{2\\; 3}\\; \\color{gray}{ 4\\; 5\\; 6}} \\\\\n\\newline\n\\hline \\\\\nA \\cup B = \\{1,2,3\\} \\qquad \\boxed{\\boxed{1\\; 2\\; 3}\\; \\color{gray}{4\\; 5\\; 6}}\n\\end{aligned}\n:::\n\n\n\n\n\n#### (Durch-)Schnitt von Ereignissen\n\n\n:::{#def-mengen-schnitt}\nDie Schnittmenge zweier Ereignisse $A$ und $B$ umfasst genau die Elementarereignisse, die Teil beider Ereignisse sind.$\\square$\n:::\n\n@fig-cap zeigt ein Sinnbild zur Schnittmenge zweier Ereignisse.\n\n\n\n![$A \\cap B$](img/Venn0001.svg.png){fig-cap width=25%}\n\n\n:::{#exm-mengen-schnitt}\n\nUm einen (hohen!) Geldpreis zu gewinnen, muss bei ihrem nächsten Wurf sowohl das Ereignis $A$ = \"gerade Augenzahl\" als auch $B$ = \"Augenzahl größer 4\".\n\n\n\\begin{align}\n& A = \\{2,4,6\\} \\qquad \\hfill \\boxed{\\color{gray}{1}\\; \\boxed{\\color{black}{2}}\\; \\color{gray}{3}\\; \\boxed{\\color{black}{4}}\\; \\color{gray}{5}\\; \\boxed{\\color{black}{6}}\\;} \\\\\n& B = \\{5,6\\} \\qquad \\qquad \\hfill  \\boxed{ \\color{gray}{1\\; 2\\; 3\\; 4\\;} \\boxed{\\color{black}{5\\; 6}}} \\\\\n\\newline\n\\hline \\\\\n& A \\cap B = \\{6\\} \\qquad \\qquad \\hfill  \\boxed{\\color{gray}{1\\; 2\\; 3\\; 4\\; 5\\;} \\color{black}{6}}\n\\end{align}\n:::\n\n\n#### Komplementärereignis\n\n\n:::{#def-menge-komplement}\nEin Ereignis $A$ ist genau dann ein Komplementärereignis zu $B$, wenn es genau die Elementarereignisse von $\\Omega$ umfasst, die nicht Elementarereignis des anderen Ereignisses sind, s. @fig-neg.$\\square$\n:::\n\n\nMan schreibt für das Komplementärereignis von $A$ oft $\\bar{A}$ oder $\\neg A$; lies \"Nicht-A$ oder \"A-quer\".\n\n\n:::{#exm-mengen-komplement}\n\nBeim normalen Würfelwurf sei $A$ das Ereignis \"gerade Augenzahl\"; \ndas Komplementärereignis ist dann $\\neg A$ \"ungerade Augenzahl\".\n\n\n\\begin{align}\nA = \\{2,4,6\\} \\qquad \\hfill \\boxed{\\color{gray}{1}\\; \\boxed{\\color{black}{2}}\\; \\color{gray}{3}\\; \\boxed{\\color{black}{4}}\\; \\color{gray}{5}\\; \\boxed{\\color{black}{6}}\\;} \\\\\n\\hline \\\\\n\\neg B = \\{1,3,5\\} \\qquad  \\hfill \\boxed{\\boxed{\\color{black}{1}}\\; \\color{gray}{2}\\; \\boxed{\\color{black}{3}}\\; \\color{gray}{4}\\; \\boxed{\\color{black}{5}}\\; \\color{gray}{6}\\; } \\\\\n\\end{align}\n:::\n\n\n![$\\bar{A}$](img/2560px-Venn1010.svg.png){#fig-neg width=25%}\n\n\n#### Logische Differenz\n\n:::{#def-mengen-diff}\nDie logische Differenz der Ereignisse $A$ und $B$ ist das Ereignis, \ndas genau aus den Elementarereignissen besteht von $A$ besteht, die nicht zugleich Elementarereignis von $B$ sind, s. @fig-setminus.$\\square§\n\n\n![$A \\setminus B$](img/Venn0100.svg.png){#fig-setminus width=25%}\n\n\n\nBeim normalen Würfelwurf sei $A$ das Ereignis \"gerade Augenzahl\"; \ndas Komplementärereignis ist dann $\\neg A$ \"ungerade Augenzahl\".\n\n\n\\begin{align}\nA = \\{2,4,6\\} \\qquad \\hfill \\boxed{\\color{gray}{1}\\; \\boxed{\\color{black}{2}}\\; \\color{gray}{3}\\; \\boxed{\\color{black}{4}}\\; \\color{gray}{5}\\; \\boxed{\\color{black}{6}}\\;} \\\\\n\\hline \\\\\n\\neg B = \\{1,3,5\\} \\qquad  \\hfill \\boxed{\\boxed{\\color{black}{1}}\\; \\color{gray}{2}\\; \\boxed{\\color{black}{3}}\\; \\color{gray}{4}\\; \\boxed{\\color{black}{5}}\\; \\color{gray}{6}\\; } \\\\\n\\end{align}\n:::\n\n\n#### Disjunkte Ereignisse\n\n\n\n\nSeien $A= \\{1,2,3\\}; B= \\{4,5,6\\}$.\n\n$A$ und $B$ sind disjunkt^[engl. disjoint]: ihre Schnittmenge ist leer: $A \\cap B = \\emptyset$,\ns. @fig-disjunkt\n\n\n\n\n\n![Zwei disjunkte Ereignisse, dargestellt noch überlappungsfreie Kreise](img/2880px-Disjunkte_Mengen.svg.png){#fig-disjunkt width=\"25%\" fig-align=\"center\"}\n\n\n\n\n\n#### Eselsbrücke zur Vereinigungs- und Schnittmenge\n\nDas Zeichen für eine Vereinigung zweier Mengen kann man leicht mit dem Zeichen für einen Schnitt zweier Mengen leicht verwechseln; daher kommt eine Eselbrücke gelesen, s. @fig-esel.\n\n![Eselsbrücke für Vereinigungs- und Schnittmenge](img/ven_cup_cap.jpeg){#fig-esel width=55%}\n\n[Quelle: rither.de](http://www.rither.de/a/mathematik/stochastik/mengentheorie-und-venn-diagramme/)\n\n\n#### Animationen\n\n\n[Animation zu Mengenoperationen](https://seeing-theory.brown.edu/compound-probability/index.html)\n\n\n\n[Animation zur Vereinigung von Mengen](https://www.geogebra.org/m/GEZV4xXc#material/cmXR8fHN)\n\n[Quelle](Geogebra, J. Merschhemke)\n\n\n### Additionssatz \n\nDer Additionssatz wird verwendet, wenn wir an der Wahrscheinlichkeit interessiert sind, dass *mindestens eines der Ereignisse* eintritt.\n\n#### Disjunkte Ereignisse\n\n$\\Omega = {1,2,3,4,5,6}$\n\n\n\n$\\boxed{1\\; 2\\; 3\\; 4\\; 5\\; 6}$\n\nGesucht sei die Wahrscheinlichkeit des Ereignis $A=\\{1,2\\}$.\n\n\n$\\boxed{\\boxed{1\\; 2}\\; \\color{gray}{ 3\\; 4\\; 5\\; 6}}$\n\n$P(1 \\cup 2) = \\frac{1}{6} + \\frac{1}{6} = \\frac{2}{6}$\n\n\n#### Allgemein (disjunkt oder nicht disjunkt)\n\n\nBei der Addition der Wahrscheinlichkeiten für $A$ und $B$ wird der Schnitt $A\\cap B$ doppelt erfasst. Er muss daher noch abgezogen werden (vgl. @fig-sets2):\n\n\n\n$$P(A \\cup B) = P(A) + P(B) - P(A\\cap B)$$\n\n\n\n<!-- ::: {#fig-sets2 layout-ncol=2} -->\n\n![$A \\cup B$](img/Venn0111.svg.png){width=20%}\n\n\n\n\n![$A \\cap B$](img/Venn0001.svg.png){width=20%}\n\n\n\nDie Schnittmenge muss beim Vereinigen abgezogen werden,\ndamit sie nicht doppelt gezählt wird.\n\n\n<!-- ::: -->\n\n\n\n### Bedingte Wahrscheinlichkeit\n\n\n#### Animation\n\n\nSchauen Sie sich mal diese [Wahnsinnsanimation von Victor Powell an](https://setosa.io/conditional/). Hammer!\n\n\n#### Schema\n\n\nAbb. @fig-schema-p illustriert gemeinsame Wahrscheinlichkeit, $P(A \\cap B) und bedingte Wahrscheinlichkeit, $P(A|B)$.\n\n\n\n::: {.cell hash='0300-Wskt_cache/html/fig-schema-p_d096613430e847de1696dfa33f8faa79'}\n::: {.cell-output-display}\n![Illustration von gemeinsamer und bedingter Wahrscheinlichkeit](0300-Wskt_files/figure-html/fig-schema-p-1.png){#fig-schema-p width=672}\n:::\n:::\n\n\n\n\nBedingte Wahrscheinlichkeit ist vergleichbar zu Filtern einer Tabelle:\n\n\n::: {.cell hash='0300-Wskt_cache/html/unnamed-chunk-4_766e3c9a5f20633bd7dfe76e090532cb'}\n\n```{.r .cell-code}\nd <- \n  tibble::tribble(\n      ~id, ~A, ~B,\n      \"1\", 0L, 0L,\n      \"2\", 0L, 1L,\n      \"3\", 1L, 0L,\n      \"4\", 1L, 1L,\n  \"SUMME\", 2L, 2L\n  )\n```\n:::\n\n\nEs ergeben sich folgende Wahrscheinlichkeiten:\n\n$P(A) = 2/4$\n\n$P(B) = 2/4$\n\n$P(A \\cap B) = 1/4$\n\n$P(A|B) = 1/2$\n\n\n\n\n\n\n### (Un-)Abhängigkeit\n\nStochastische Unabhängigkeit ist ein Spezialfall von Abhängigkeit: Es gibt sehr viele Ausprägungen für Abhängigkeit, aber nur eine für Unabhängigkeit.\nKönnen wir Unabhängigkeit nachweisen, haben wir also eine starke Aussage getätigt.\n\n\n\n\n\n\n\n::: {.cell hash='0300-Wskt_cache/html/unnamed-chunk-5_e2f7d9a5f7868b604e8ae85f0fd07fb1'}\n\n:::\n\n\n\n\n*Abhängig*, s. @fig-abh, links: Überleben auf der Titanic ist offenbar *abhängig* von der Passagierklasse.\nAuf der anderen Seite: Das Ereignis *Überleben* auf der Titanic ist *un*abhängig vom Ereignis *Alter ist eine Primzahl*, s. @fig-abh, rechts.\n\n\n\n\n\n\n::: {.cell hash='0300-Wskt_cache/html/fig-abh_2b1887586a16c62de8a2930b34e3c725'}\n::: {.cell-output-display}\n![Abhängigkeit und Unabhängigkeit zweier Ereignisse](0300-Wskt_files/figure-html/fig-abh-1.png){#fig-abh width=100%}\n:::\n:::\n\n\n\n\n\n\n\n\n\nZur Ab- bzw. Un-Abhängigkeit zweier Variablen, an Beispielen illustriert.\n\n\n\n\n:::{#exm-covid}\n\n## Zusammenhang von Covidsterblichkeit und Impfquote\n\n\nSind die Ereignisse *Tod durch Covid*  bzw. *Impfquote* ($A$) und *Land*^[hier mit den zwei Ausprägungen *DEU* und *USA*] ($B$) voneinander abhängig (Abb. @fig-covid1)?\n\n\n::: {.cell hash='0300-Wskt_cache/html/fig-covid1_174476aa3df72630dd26576404ad0993'}\n::: {.cell-output-display}\n![Impfquote und Sterblichkeit sind voneinander abhängig (bezogen auf Covid, auf Basis vorliegender Daten)](0300-Wskt_files/figure-html/fig-covid1-1.png){#fig-covid1 width=672}\n:::\n:::\n\n\nJa, da in beiden Diagrammen gilt: $P(A|B) \\ne Pr(A) \\ne Pr(A|\\neg B)$.\n\n\nDaten von [Our World in Data](https://ourworldindata.org/covid-deaths).\n\n\n:::\n\n\n\n\n\n\n\n\n### Multiplikationssatz\n\nDer Multiplikationssatz wird verwendet, wenn wir an der Wahrscheinlichkeit interessiert sind, dass *alle Ereignisse* eintreten.\n\n\n#### Unabhängige Ereignisse\n\n\nWir werfen eine faire Münze *zwei* Mal (Abb. @fig-2muenzen).\n\n\n![Wir werfen 2 faire Münzen](img/muenz1.png){#fig-2muenzen width=\"50%\"}\n\nAbb. @fig-2muenzen zeigt ein *Baumdiagramm*. Jeder *Kasten* (Knoten) zeigt ein *Ergebnis.* \nDie Pfeile (Kanten) symbolisieren die Abfolge des Experiments: Vom \"Start\" (schwarzer Kreis) \nführen zwei mögliche Ergebniss ab, jeweils mit Wahrscheinlichkeit 1/2.\nDie untersten Knoten nennt man auch *Blätter* (Endknoten), sie zeigen das Endresultat des (in diesem Fall) zweifachen Münzwurfs.\nDer Weg vom Start zu einem bestimmten Blatt nennt man *Pfad*. \nDie Anzahl der Pfade entspricht der Anzahl der Blätter.\nIn diesen Diagramm gibt es vier Pfade (und Blätter).\n\n\n\n\n\n\n::: {.cell hash='0300-Wskt_cache/html/unnamed-chunk-6_0a0538eed52e455231a7b30e90b3a46d'}\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"Ereignis\"],\"name\":[1],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"Pr\"],\"name\":[2],\"type\":[\"chr\"],\"align\":[\"left\"]}],\"data\":[{\"1\":\"0K\",\"2\":\"1/2 * 1/2 = 1/4\"},{\"1\":\"1K\",\"2\":\"1/4 + 1/4 = 1/2\"},{\"1\":\"2K\",\"2\":\"1/2 * 1/2 = 1/4\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n\nWir werfen eine faire Münze *drei* Mal (Abb. @fig-3muenzen)\n\n![Wir werfen drei faire Münzen](img/muenz2.png){#fig-3muenzen}\n\n\n\n\n::: {.cell hash='0300-Wskt_cache/html/QM2-Thema1-WasistInferenz-27_3f134654b8fce3b6e21aed5f3d1d42ef'}\n::: {.cell-output-display}\n<div class=\"kable-table\">\n\n|Ereignis |Pr                    |\n|:--------|:---------------------|\n|0K       |1/2 * 1/2 * 1/2 = 1/8 |\n|1K       |1/8 + 1/8 + 1/8 = 3/8 |\n|2K       |3 * 1/8 = 3/8         |\n|3K       |1/2 * 1/2 * 1/2 = 1/8 |\n\n</div>\n:::\n:::\n\n\n\n\n\n$Pr(AB) = Pr(A) \\cdot Pr(B) = 50\\% \\cdot 50\\% = 25\\%$\n\n\n::: {.cell hash='0300-Wskt_cache/html/fig-unabh-e_d24e80de68fc90b66a242f0cbc744dfb'}\n::: {.cell-output-display}\n![Unabhängige Ereignisse visualisiert](0300-Wskt_files/figure-html/fig-unabh-e-1.png){#fig-unabh-e width=672}\n:::\n:::\n\n\n\nAbb. @fig-unabh-e zeigt, dass gilt: $P(A\\cap B) = P(A) \\cdot P(B) = P(B) \\cdot P(A)$.\n\n\n#### Kalt und Regen\n\nVon @mcelreath_statistical_2020 stammt diese Verdeutlichung der gmeinsamen Wahrscheinlichkeit:\n\nWas ist die Wahrscheinlichkeit für *kalt ❄ und Regen ⛈️*?\n\nDie Wahrscheinlichkeit für kalt und Regen ist die Wahrscheinlichkeit von *Regen* ⛈, wenn's *kalt* ❄ ist mal die Wahrscheinlichkeit von *Kälte* ❄.\n\nEbenfalls gilt:\n\nDie Wahrscheinlichkeit für kalt und Regen ist die Wahrscheinlichkeit von *Kälte* ❄, wenn's *regnet* ⛈️ mal die Wahrscheinlichkeit von *Regen* ⛈️.\n\nDas Gesagte als Emoji-Gleichung:\n\n$P(❄️ und ⛈️) = P(⛈️ |❄️ ) \\cdot P(❄️) =  P(❄️ |⛈️ ) \\cdot P(⛈️) = P(⛈️ und  ❄️)$\n\n\n\nAllgemein:\n\n$P(A\\cap B) = P(A) \\cdot P(B|A) = P(B) \\cdot P(A|B)$\n\n\nMan kann also die \"Gleichung drehen\".\n\n#### Abhängige Ereignisse\n\n\n\nEin Baumdiagramm bietet sich zur Visualisierung abhängiger Ereignisse an, s. Abb. @fig-baum-abh. Für unabhängige Ereignisse übrigens auch.\n\n\nIn einer Urne befinden sich fünf Kugeln, von denen vier rot sind und eine blau ist.\n\nwie groß ist die Wahrscheinlichkeit, dass bei zwei Ziehungen ohne Zurücklegen (*ZOZ*) *zwei rote Kugeln* gezogen werden [@bourier_2018], S. 47.\n\n\nHier ist unsere Urne:\n\n$$\\boxed{\\color{red}{R, R, R, R}, \\color{blue}B}$$\n\nUnd jetzt ziehen wir. Hier ist das Baumdiagramm, s. Abb. @fig-baum-abh.\n\n\n\n```{mermaid}\n%%| fig-cap: \"Baumdiagramm für ein ein zweistufiges Zufallsereignis, wobei der 2. Zug (Stufe) abhängig ist vom 1. Zug.\"\n%%| label: fig-baum-abh\nflowchart LR\n  A[Start] -->|4/5|B[1. Zug: R]\n  A -->|1/5|C[1. Zug: B]\n  B -->|3/4|D[2. Zug: R]\n  B -->|1/4|E[2. Zug: B]\n  D --- H[Fazit: RR:  12/20]\n  E --- I[Fazit: RB: 4/20]\n  C -->|4/4|F[2. Zug: R]\n  C -->|0/4|G[2. Zug: B]\n  F --- J[Fazit: BR: 4/20]\n  G --- K[Fazit: BB: 0/20]\n```\n\n\n\n\nEs gilt also: $P(A\\cap B) = P(A) \\cdot P(B|A)$.\n\n\n\n### Totale Wahrscheinlichkeit\n\n\n\n\n@fig-tot-wskt zeigt das Baumdiagramm für die Aufgabe @bourier_2018, S. 56.\n\n\n```{mermaid}\n%%| fig-cap: Totale Wahrscheinlichkeit\n%%| label: fig-tot-wskt\nflowchart LR\n  A[Start] -->|0.6|B[A1]\n  A -->|0.1|C[A2]\n  A -->|0.3|D[A3]\n  B -->|0.05|E[B]\n  B -.->|0.95|F[Nicht-B]\n  C -->|0.02|G[B]\n  C -.->|0.98|H[Nicht-B]\n  D -->|0.04|I[B]\n  D -.->|0.96|J[Nicht-B]\n```\n\n\n\n\nGesucht ist die Wahrscheinlichkeit $P(B)$.\n\nDazu addieren wir die Warhscheinlichkeiten der relevanten Äste.\n\n\n::: {.cell hash='0300-Wskt_cache/html/unnamed-chunk-9_31276ebbb997d9303479d5d37e35aed4'}\n\n```{.r .cell-code}\nW_total <- 0.6 * 0.05 + 0.1 * 0.02 + 0.3 * 0.04\nW_total\n## [1] 0.044\n```\n:::\n\n\nDie totale Wahrscheinlichkeit beträgt also $P(B) = 4.4\\%$.\n\nEinfacher noch ist es, wenn man anstelle von Wahrscheinlichkeiten absolute Häufigkeiten verwendet.\n\n\n### Bayes\n\n#### Bayes als Baum\n\nGesucht sei $P(A_1|B)$.\n\nFür Bayes' Formel setzt man die Wahrscheinlichkeit des  *günstigen* Ast zur Wahrscheinlichkeit aller relevanten Äste, $P(B)$.\n\nDer günstige Ast ist hier schwarz gedruckt, die übrigen Äste gestrichelt, s. @fig-tot-wskt2.\n\n\n```{mermaid}\n%%| fig-cap: Günstige Pfade\n%%| label: fig-tot-wskt2\nflowchart LR\n  A[Start] -->|0.6|B[A1]\n  A -.->|0.1|C[A2]\n  A -.->|0.3|D[A3]\n  B --->|0.05|E[B]\n  B -.->|0.95|F[Nicht-B]\n  C -.->|0.02|G[B]\n  C -.->|0.98|H[Nicht-B]\n  D -.->|0.04|I[B]\n  D -.->|0.96|J[Nicht-B]\n```\n\n\n\n\n$$P(A|B) = \\frac{P(A1 \\cap B)}{P(B)} = \\frac{0.6 \\cdot 0.05}{0.03 + 0.002 + 0.012} = \\frac{0.03}{0.044} \\approx 0.68$$\n\n$P(A|B)$ beträgt also ca. 68%.\n\nZur Erinnerung: $P(B)$ ist die totale Wahrscheinlichkeit.\n\n\n## Bayes' Theorem\n\n### Bayes als bedingte Wahrscheinlichkeit\n\n\nBayes' Theorem ist auch nur eine normale bedingte Wahrscheinlichkeit:\n\n\n$P(A|B) = \\frac{\\overbrace{ P(A\\cap B)}^\\text{umformen}}{P(B)}$\n\n$P(A\\cap B)$ kann man  umformen, s. @eq-bayes1:\n\n$$P(A|B) =\\frac{P(A\\cap B)}{P(B)} = \\frac{P(B|A) \\cdot P(A)}{P(B)}$${#eq-bayes1}\n\nMan kann sich Bayes' Theorem  auch wie folgt herleiten:\n\n\n\n$P(A\\cap B) = P(B \\cap A) = P(A) \\cdot P(B|A) = P(B) \\cdot P(A|B)$\n\nDann lösen wir nach P$(A|B)$ auf:\n\n\n$P(A|B) = \\frac{P(A) \\cdot P(B|A)}{P(B)}$\n\n\n### Wozu wird Bayes in der Praxis genutzt?\n\n\nIn der Praxis nutzt man Bayes häufig, wenn man Daten zu einer Wirkung $W$ hat,\nund auf die Ursache $U$ zurückschließen möchte, sinngemäß:\n\n$W \\quad \\underrightarrow{Bayes} \\quad U$.\n\nDann kann man @eq-bayes1 so schreiben, s. @eq-bayes2:\n\n$$P(U|W) = \\frac{ P(U) \\cdot P(W|U) }{P(W)}$${#eq-bayes2}\n\nEine ähnliche Situation, die in der Praxis häufig ist,\ndass man Daten $D$ hat und auf die Wahrscheinlichkeit einer Hypothese $H$ schließen möchte, s. @eq-bayes3.\n\n$D \\quad \\underrightarrow{Bayes} \\quad H$.\n\n\n$$P(H|D) = \\frac{ P(H) \\cdot P(D|H) }{P(D)}$${#eq-bayes3}\n\n@eq-bayes3 fragt nach $P(H|D)$:\n\n>    Was ist die Wahrscheinlichkeit der Hypothese H, jetzt wo wir die Daten haben (und ein Modell?)\n\nUnd antwortet so (@eq-bayes3):\n\n>    Diese Wahrscheinlichkeit entspricht der Grundrate (Apriori-Wahrscheinlichkeit) der Hypothese mal der Plausibilität (Likelihood) der Daten unter Annahme (gegeben) der Hypothese. Aus Standardisierungsgründen dividiert man noch die totale Wahrscheinlichkeit der Daten über alle Hypothesen.\n\n\n### Zusammengesetzte Hypothesen\n\nDas ist vielleicht ein bisschen fancy,\naber man kann Bayes' Theorem auch nutzen, um die Wahrscheinlichkeit einer *zusammengesetzten Hypothese* zu berechnen: $H = H_1 \\cap H_2$. \nEin Beispiel wäre: \"Was ist die Wahrscheinlichkeit, dass es Regen ($R$) *und* Blitzeis ($B$) gibt, wenn es kalt ($K$) ist?\".\n\nDas sieht dann so aus, @eq-bayes4:\n\n$$\n\\begin{aligned}\nP(R \\cap B |K) &= \\frac{ P(R \\cap B) \\cdot P(K|R \\cap B) }{P(D)} \\\\\n&= \\frac{ P(R ) \\cdot P(B) \\cdot P(K|R \\cap B) }{P(D)}\n\\end{aligned}\n$${#eq-bayes4}\n\n\nHier haben wir $P(R \\cap B)$  aufgelöst in $P(R) \\cdot P(B)$,\ndas ist nur zulässig, wenn $R$ und $B$ unabhängig sind.\n\n#### Bayes-Video von 3b1b\n\nDas [Video zu Bayes von 3b1b](https://youtu.be/HZGCoVF3YvM) verdeutlicht das Vorgehen der Bayes-Methode auf einfache und anschauliche Weise.\n\n\n\n## Vertiefung\n\n\nBei @henze_stochastik_2019 findet sich eine anspruchsvollere Einführung in das Rechnen mit Wahrscheinlichkeit; dieses Kapitel behandelt ein Teil des Stoffes  der Kapitel 2 und 3 von @henze_stochastik_2019.\n\n\n\n## Aufgaben\n\n\nZusätzlich zu den Aufgaben im Buch:\n\n- [mtcars-abhaengig](https://datenwerk.netlify.app/posts/mtcars-abhaengig/mtcars-abhaengig.html)\n- [voll-normal](https://datenwerk.netlify.app/posts/voll-normal/voll-normal.html)\n- [corona-blutgruppe](https://datenwerk.netlify.app/posts/corona-blutgruppe/corona-blutgruppe.html)\n- [Bed-Wskt2](https://datenwerk.netlify.app/posts/bed-wskt2/bed-wskt2)\n- [Gem-Wskt1](https://datenwerk.netlify.app/posts/gem-wskt1/gem-wskt1)\n- [wuerfel01](https://datenwerk.netlify.app/posts/wuerfel01/wuerfel01.html)\n- [wuerfel02](https://datenwerk.netlify.app/posts/wuerfel02/wuerfel02.html)\n- [wuerfel03](https://datenwerk.netlify.app/posts/wuerfel03/wuerfel03.html)\n- [wuerfel04](https://datenwerk.netlify.app/posts/wuerfel04/wuerfel04.html)\n\n\n\n\n\n## ---\n\n\n\n![](img/outro-03.jpg){width=100%}\n\n\n\n\n\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<link href=\"site_libs/pagedtable-1.1/css/pagedtable.css\" rel=\"stylesheet\" />\n<script src=\"site_libs/pagedtable-1.1/js/pagedtable.js\"></script>\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}