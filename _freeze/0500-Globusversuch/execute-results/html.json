{
  "hash": "7f26660088c4b4d4ac4e0858e0cb6fed",
  "result": {
    "engine": "knitr",
    "markdown": "# Bayes-Globus \n\n\n\n\n\n![Bayes:Start](img/Golem_hex.png){width=5%}\n\n\n\n\n## Lernsteuerung\n\n### Position im Modulverlauf\n\n@fig-modulverlauf gibt einen √úberblick zum aktuellen Standort im Modulverlauf.\n\n\n\n\n### √úberblick\n\nIn diesem Kapitel √ºbersetzen wir eine Problemstellung (Forschungsfrage) in ein (mathematisches) Modell, das uns dann mit Hilfe der Bayes-Formel Antworten auf die Problemstellung gibt.\n\n\n### Lernziele\n\nNach Absolvieren des jeweiligen Kapitels sollen folgende Lernziele erreicht sein.\n\nSie k√∂nnen ...\n\n\n- Unterschiede zwischen Modellen und der Realit√§t erl√§utern\n- die Binomialverteilung heranziehen, um geeignete (einfache) Modelle zu erstellen (f√ºr binomial verteilte Zufallsvariablen)\n- die weite Einsetzbarkeit anhand mehrerer Beispiele exemplifizieren\n- das Bayes-Modell anhand bekannter Formeln herleiten\n- Post-Wahrscheinlichkeiten anhand der Bayesbox berechnen\n\n\n### Begleitliteratur\n\n\nDer Stoff dieses Kapitels deckt einen Teil aus @mcelreath2020, Kap. 2, ab. @mcelreath2020 stellt das Globusmodell mit mehr Erl√§uterung und etwas mehr theoretischem Hintergrund vor, als es in diesem Kapitel der Fall ist.\n\n\n\n\n### Vorbereitung im Eigenstudium\n\n- [Statistik 1, Kap. \"Daten Einlesen\"](https://statistik1.netlify.app/020-r)\n\n\n### Begleitvideos\n\n- üì∫ [Globusversuch](https://www.youtube.com/watch?v=fGlt9Ld4xzk&list=PLRR4REmBgpIGgz2Oe2Z9FcoLYBDnaWatN&index=6)\n\n\n### Ben√∂tigte R-Pakete\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(ggpubr)  # komfortable Visualisierung\n```\n:::\n\n\n\n\n\n\n\n\n\n\n\n\n\n## Von Welten und Golems\n\n### Kleine Welt, gro√üe Welt\n\nBekanntlich segelte Kolumbus 1492 los, und entdeckte Amerika^[wenn auch nicht als Erster]. Das war aber ein gl√ºcklicher Zufall, denn auf seinem Globus existierte Amerika gar nicht. Vielleicht sah sein Globus so aus wie der von Behaim, s. Abb @fig-behaim.\n\n![Behaims Globus: Kein Amerika](img/Behaim.jpg){#fig-behaim}\n\n[Quelle: Ernst Ravenstein, Wikimedia, Public Domain](https://commons.wikimedia.org/wiki/File:RavensteinBehaim.jpg)\n\nDie *kleine Welt des Modells* entsprach hier nicht *der gro√üen Welt, der echten Erdkugel*.\n\nDas ist ein Beispiel, das zeigt, wie Modellieren schiefgehen kann. Es ist aber auch ein Beispiel f√ºr, sagen wir, die Komplexit√§t wissenschaftlicher (und sonstiger) Erkenntnis. Einfach gesagt: Gl√ºck geh√∂rt halt auch dazu.\n\n\n::: callout-note\nBehaims Globus ist nicht gleich der Erde. Die kleine Welt von Behaims Globus ist nicht die gro√üe Welt, ist nicht die Erde.\n:::\n\nWas in der kleinen Welt funktioniert, muss nicht in der gro√üen Welt funktionieren. Modelle zeigen immer nur die kleine Welt: Vorsicht vor schnellen Schl√ºssen und vermeintlicher Gewissheit.\n\n\n:::{exr-modellno}\nüèã Nennen Sie ein Beispiel, in dem ein Modell nicht (exakt) der Wirklichkeit entspricht! $\\square$\n:::\n\n\n\n\n\n\n\n### Der Golem von Prag\n\n![Der Golem von Prag](img/170px-Golem_and_Loew.jpg){#fig-golem-prag width=\"33%\"}\n\n[Quelle](https://de.wikipedia.org/wiki/Golem)\n\n[Der Golem von Prag](http://www.prague.net/golem), die Legende einer vom Menschen geschaffene Kreatur mit gewaltiger Kraft, die Befehle w√∂rtlich ausf√ºhrt, s. @fig-golem-prag.\nDie Geschichte besagt, dass ein Rabbi mit Zauberkr√§ften den Golem aus Lehm erschuf, um die j√ºdische Bev√∂lkerung der Stadt zu sch√§tzen.\nBei kluger F√ºhrung kann ein Golem N√ºtzliches vollbringen.\nBei un√ºberlegter Verwendung wird er jedoch gro√üen Schaden anrichten.\n\n### Wissenschaftliche Modelle sind wie Golems\n\n\n\n![\"Yeah, ich bin ein Golem!\" - Bildquelle: Klara Schaumann](img/Golem_hex.png){width=25%}\n\n:::: {.columns}\n\n::: {.column width=\"50%\"}\n**Golem**\n\nEigenschaften des *Golems*:\n\n-   Besteht aus Lehm\n-   Belebt durch \"Wahrheit\"\n-   M√§chtig\n-   dumm\n-   F√ºhrt Befehle w√∂rtlich aus\n-   Missbrauch leicht m√∂glich\n-   M√§rchen\n:::\n\n::: {.column width=\"50%\"}\n**Modell**\n\nEigenschaften eines *Modells*:\n\n\n-   Besteht aus ~~Lehm~~Silikon\n-   Belebt durch Wahrheit (?)\n-   Manchmal m√§chtig\n-   simpler als die Realit√§t\n-   F√ºhrt Befehle w√∂rtlich aus\n-   Missbrauch leicht m√∂glich\n-   Nicht einmal falsch\n:::\n\n::::\n\n\n\n\n\n::: callout-note\nWir bauen Golems.\n:::\n\n@fig-xy stellt ein Sinnbild von Modellen dar.\n\n\nVergleichen wir die kleine Welt unserer Modellen (@tbl-klein-gross), wie z.B. Behaims Globus, mit der Gro√üen Welt, die Kolumbus und wir befahren.\n\n\n\n\n| Kleine Welt                                                | Gro√üe Welt                                 |\n|-----------------------------------------|-------------------------------|\n| Die Welt, wie sie der Golem sieht                          | Die Welt, wie sie in Wirklichkeit ist      |\n| ist das Modell, aber nicht (zwangsl√§ufig) die Wirklichkeit | entspricht nicht (zwangsl√§ufig) dem Modell |\n| Verwenden wir beim Modellieren                             | Ist das, was wir modellieren               |\n\n: Kleine Welt vs. gro√üe Welt {#tbl-klein-gross}\n\n\n\n\n\n\n<!-- ![So denkt unser Bayes-Golem](img/bayesupdate2.png){#fig-bayes1} -->\n\n\n\n:::{exm-bayes-lernen}\n### Die Bayes-Formel und Lernen\nüèã Bayes-Inferenz √§hnelt dem Lernen von Menschen. Geben Sie ein Beispiel von Lernen bei Menschen, das oben dargestelltem Prozess √§hnelt!$\\square$\n:::\n\n## Ein erster Versuch: Wir werfen den Globus\n\n\n\n\n\n### Welcher Anteil der Erdoberfl√§che ist mit Wasser bedeckt?\n\n\n:::{#exm-fofra1}\n### Wasseranteil auf der Erdoberfl√§che\nUnsere Forschungsfrage lautet, mit welchem Anteil die Erde wohl mit Wasser bedeckt ist (@fig-erde)? Um m√∂glichst wenig schreiben zu m√ºssen, schreiben wir f√ºr \"angenommener Wasseranteil auf der Erdoberfl√§che\" kurz $p$ oder $\\pi$ (p wie proportion, Anteil). $\\square$\n:::\n\n![Die Erde. Sch√∂n! Und mit viel Wasser, ca. 70% der Erdoberfl√§che sind mit Wasser bedeckt. \n[Quelle](https://pngimg.com/image/25340), Lizenz: CC 4.0 BY-NC](img/earth.png){#fig-erde width=\"10%\" fig-align=\"center\"}\n\n\nAnalog k√∂nnen wir uns vorstellen, 11 Wissenschaftlis haben jeweils eine andere Hypothese zum Wasseranteil, $\\pi$, der Erde. Die erste Person hat die Hypothese $\\pi_1 = 0$, die zweite Person geht von $\\pi_2 = 0.1$ aus ... die 11. Person von $\\pi_{11} = 1$.\n \nUm die Forschungsfage zu beantworten, werfen Sie einen Globus-Ball in die Luft und fangen in wieder auf. \nSie notieren dann, ob die Stelle unter Ihrem Zeigefinger Wasser zeigt (W) oder Land (L). Den Versuch wiederholen Sie, bis Sie den Globusball insgesamt 9 Mal geworfen haben.^[Warum gerade 9 Mal? Tja, dann hat das Handy geklingelt... Auch in wissenschaftlichen Versuchen ist (leider?) nicht immer alles genau geregelt.]\n\nSo sah *mein*^[*Ihr* Ergebnis kann anders aussehen, schlie√ülich ist es ja Zufall.] Ergebnis aus:\n\n$$W \\quad L \\quad W \\quad W \\quad W \\quad L \\quad W \\quad L \\quad W$$\n\n\nAlso $W=6$ (Wasser, d.h. \"Treffer\") und $L=3$ (Land) ($n=9$ Versuche).\n\n:::{#exr-globe1}\n### Spin the Globe\nüèãÔ∏èÔ∏è Besorgen Sie sich einen Globus (zur Not eine M√ºnze) und stellen Sie den Versuch nach!$\\square$\n:::\n\n\n\n\n\n\n### Bayes-Updates\n\n\nDer Golem denkt eigentlich ganz vern√ºnftig:\nZuerst hat er ein Vorwissen zum Wasseranteil, die dazugeh√∂rige Wahrscheinlichkeitsverteilung nennt man *Priori-Verteilung* (s. @def-priori).\nIn unserem Beispiel ist das Vorwissen recht bescheiden: Jeder Wasseranteil ist ihm gleich plausibel.\nAls n√§chstes beschaut sich der Golem die Daten und √ºberlegt,\nwie wahrscheinlich die Daten sind, wenn man von einer bestimmten Hypothese ausgeht, z.B. dass der Wasseranteil 50% betr√§gt.\nDie zugeh√∂rige Wahrscheinlichkeit der Daten unter Annahme einer Hypothese nennt man die^[oder den?] *Likelihood*^[zu Deutsch etwa: \"Mutma√ülichkeit\"], s. @def-L.\nAls letztes bildet sich der Golem eine abschlie√üende Meinung zur Wahrscheinlichkeit jeder Hypothese. Diese Wahrscheinlichkeitsverteilung nennt man *Posteriori-Verteilung*, s. @def-post1.\nSie berechnet als Gewichtung des Vorwissen mit den neuen Daten.\nAnders gesagt: Das Vorwissen wird anhand der Erkenntnisse (der Daten) aktualisiert oder \"geupdatet\", s. @fig-bayes-update.\n\n\n<!-- ![Updating mit Bayes](img/bayesupdate.png){#fig-bayes-update} -->\n\n\n\n\n\n\n```{mermaid}\n%%| fig-cap: Updating mit Bayes\n%%| label: fig-bayes-update\ngraph LR\nA[Priori-Vert.]-->B[Likelihood]-->C[Post-Vert.]-->A\n```\n\n\n\n\n\n\n\n\n\n\n:::{#def-priori}\n### Priori-Verteilung\nF√ºr jede Hypothese haben wir ein Vorab-Wissen, das die jeweilige Plausibilit√§t der Hypothese angibt: *Priori-Verteilung* (synonym: Apriori-Verteilung).$\\square$\n:::\n\n:::{#def-L}\n### Likelihood\nF√ºr jede Hypothese (d.h. jeden *Parameterwert* $\\pi$) m√∂chten wir wissen, \nwie wahrscheinlich die Daten sind (unter der Annahme, \ndass die Hypothese richtig ist). Kurz: Wir suchen die *Likelihood*. \nAnders gesagt: Die Likelihood sagt uns, \nwie gut die Daten zu einer bestimmten Hypothese passen.$\\square$\n:::\n\n:::{#def-post1}\n### Posteriori-Verteilung\nDann gewichten wir den Likelihood mit dem Vorabwissen, so dass wir die *Posteriori-Verteilung*^[ Anstatt von *Priori* liest man auch *Prior*; anstatt *Posteriori* auch *Posterior*] bekommen.$\\square$\n:::\n\n\n\n\n\n\n:::{#exr-l1}\n### Wie gut passen die Daten zur Hypothese, dass die Erde komplett trocken ist?\n\nWir haben in unseren Versuch $W=6$ und $L=3$ erzielt. Diese Daten passen *√ºberhaupt nicht* zur Hypothese, dass die Erdoberfl√§che komplett trocken ist.\nDie *Likelihood*, $L$ f√ºr $\\pi=0$ ist also Null.\nAnalog ist die Likelihood f√ºr $\\pi=1$ auch Null.$\\square$\n:::\n\n\n\n### Wie wahrscheinlich ist ein Wasseranteil von 90%?\n\nWie wahrscheinlich ist es, einen bestimmten Wasseranteil, z.B. 6 Treffer (bei 9 W√ºrfen) zu erhalten, wenn man eine bestimmte Hypothese (einen bestimmten Wasseranteil, z.B. 90%) annimmt?\nDiese Wahrscheinlichkeit nennt man die *Likelihood*, $L$ oder  $L$.\n\n<!-- Geht man von einer Binomialverteilng aus, ist die Likelihood einfach zu berechnen. -->\n\nWenn wir eine Binomialverteilung annehmen, \ndann gehen wir davon aus,  dass die Daten unabh√§ngig voneinander entstehen und sich der Parameterwert nicht zwischenzeitlich √§ndert\n^[Die sog. \"iid-Annahme\", *i*ndependently and *i*dentically distributed: Jeder Wurf der Globusballes ist eine Realisation der gleichen Zufallsvariablen. \nJeder Wurf ist unabh√§ngig von allen anderen: \nDas Ergebnis eines Wurfes hat keinen (stochastischen) Einfluss auf ein Ergebnis anderer W√ºrfe.\nDie Wahrscheinlichkeitsverteilung ist bei jedem Wurf identisch.].\nDer Wasseranteil der Erde bleibt w√§hrend des Versuchs gleich (durchaus plausibel).\n\nLassen Sie uns im Folgenden die Wahrscheinlichkeit ($Pr$), $W$ mal Wasser und $L$ mal Land zu beobachten, wenn die Wahrscheinlichkeit f√ºr Wasser $\\pi$ betr√§gt, so bezeichnen: $Pr(W,L | \\pi))$ oder auch (synonym) so: $Pr(W|\\pi, n)$.\nDiese Wahrscheinlichkeit, $Pr(W,L | \\pi)$, kann man im Fall des Globusversuchs mit der *Binomialverteilung* berechnen.\n\nM√∂chte man die Wahrscheinlichkeit ansprechen f√ºr das Ereignis \"6 mal Wasser und 3 mal Land, \nwenn wir von einem Wasseranteil von 70% ausgehen\", so w√ºrden wir kurz schreiben: $Pr(W=6, L=3 | \\pi=.7)$.\nOder man k√∂nnte (synonym) schreiben:  $Pr(W=6 | \\pi=.7, n=9)$.\n\n\nZur Erinnerung: Die Binomialverteilung zeigt die Verteilung der\nWahrscheinlichkeit der Ereignisse (z.B. 2 Mal Kopf) beim wiederholten M√ºnzwurf\n(und allen vergleichbaren Zufallsexperimenten): \n\"M√ºnzwurfverteilung\", s. Kap. @sec-bin-distrib.\n\n\n### Likelihood berechnen\n\n\n\n\nWas ist der Anteil der g√ºltigen Pfade in einem Baumdiagramm (d.h. die Wahrscheinlichkeit), \num 2 mal $W$ bei $n=W+L=3$ W√ºrfen zu bekommen, wenn wir von $\\pi=1/2$ ausgehen?\n^[Allgemeiner spricht man auch von 2 Treffern bei 3 W√ºrfen \n(d.h. 1 \"Nicht-Treffer\", den wir als \"Niete\" bezeichnen). \nTreffer werden oft mit `1`  und Nieten mit `0` bezeichnet], \ns. @lst-dbinom1, @fig-binom1a und @eq-dbinom1.\n\n\n\n\n\n\n::: {.cell}\n\n```{#lst-dbinom1 .r .cell-code  lst-cap=\"Binomialverteilung mit R f√ºr x=2, n=3, p=1/2\"}\nloesung <- dbinom(x = 2, size = 3, prob = 1/2)\nloesung\n## [1] 0.375\n```\n:::\n\n\n\n\n\nOder von Hand gerechnet:\n\n$$\\begin{aligned}\nPr(W=2 | \\pi=1/2, n=3) &=\\\\ \n\\tbinom{3}{2} \\cdot (1/2)^2 \\cdot (1/2)^1 &=\\\\\n\\frac{3!}{2!1!} \\cdot (1/2)^3 &= \\\\\n3 \\cdot 1/8 = 3/8 &= 0.375\n\\end{aligned}$${#eq-dbinom1}\n\nWenn man sich den entsprechenden Baum anschaut (s. @fig-binom1a): \nVon den 8 Endkonten bzw. Pfaden sind 3 g√ºnstig. \nDemnach ist die Wahrscheinlichkeit des gesuchten Ereignis (2 Treffer bei 3 W√ºrfen, binomialverteilt) gleich 3 von 8 (alle Pfade sind gleich wahrscheinlich);\n3/8 sind 0.375.\n\n\n\n\n\n\n\n\n\n\n```{mermaid}\n%%| echo: false\n%%| label: fig-binom1a\n%%| fig-cap: \"Wir werfen den Globus (oder eine M√ºnze) 3 Mal. Die Knoten sind der √úbersicht halber mit fortlaufenden Buchstaben (von A bis P) bezeichnet\"\nflowchart TD\n  A[A - Start] -. 1/2 .-> B[B - 0]\n  A -. 1/2 .-> C[C - 1]\n  B -. 1/2 .-> D[D - 0]\n  B -. 1/2 .-> E[E - 1]\n  C -. 1/2 .-> F[F - 0]\n  C -. 1/2 .-> G[G - 1]\n  D -. 1/2 .-> H[H - 0]\n  D -. 1/2 .-> J[I - 1]\n  E -. 1/2 .-> K[K - 0]\n  E -. 1/2 .-> L[L - 1]\n  F -. 1/2 .-> M[M - 0]\n  F -. 1/2 .-> N[N - 1]\n  G -. 1/2 .-> O[O - 0]\n  G -. 1/2 .-> P[P - 1]\n```\n\n\n\n\n\n\nAbb. @fig-binom1a stellt einen einfachen Baum f√ºr 3 Globusw√ºrfe mit je zwei m√∂glichen Ereignissen (W vs. L) dar.\nIn der ersten (obersten) Zeile (Knoten A; \"Start\") ist Ausgangspunkt dargestellt: Der Globus ruht wurfbereit in unserer Hand.\nJetzt Achtung: Sie werfen den Globusball hoch.\nDie Pfeile zeigen zu den (zwei) m√∂gliche Ergebnissen.\nDie zweite Zeile (Knoten B und C) stellt die beiden Ergebnisse des Wurfes dar. \nDie Ergebnisse sind hier mit `0` und `1` bezeichnet (das eine eine einfache und weiteinsetzbare Notation).\nDie dritte Zeile (Knoten D bis G) stellt die Ergebnisse des des zweiten Wurfes dar.\nDie vierte Zeile (Knoten H bis P)  stellt die Ergebnisse des des dritten Wurfes dar.\n\n\nF√ºr mehr W√ºrfe w√ºrde das Diagramm irgendwann un√ºbersichtlich werden.\n\n\n\n\n@fig-bin-klein zeigt die Binomialverteilung $X \\sim Bin(9, 1/2)$:\nDie jeweilige Wahrscheinlichkeit f√ºr $k=0,1,\\ldots, 9$ Treffer bei $n=9$ Versuchen mit\nTrefferwahrscheinlichkeit $\\pi=1/2$.\n\n\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![Ein Beispiel f√ºr eine Binomialverteilung mit Parametern N=9 und p=1/2.](0500-Globusversuch_files/figure-html/fig-bin-klein-1.png){#fig-bin-klein width=672}\n:::\n:::\n\n\n\n\n\n\n\nAbb @fig-binom2 ist ein vergeblicher Versuch, so einen gro√üen Baum ($n=9$) darzustellen.\n\n:::callout-note\nVisualisierungen wie Baumdiagramme sind eine praktische Hilfe zum Verst√§ndnis,\nkommen aber bei gr√∂√üeren Daten schnell an ihre Grenze.\n:::\n\n\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![Wir werfen den Globus (oder eine M√ºnze) 9 Mal, es resultieren 512 Endknoten. Nicht gerade √ºbersichtlich.](0500-Globusversuch_files/figure-html/fig-binom2-1.png){#fig-binom2 width=672}\n:::\n:::\n\n\n\n\n\n\nJetzt folgen einige Beispiele.\n\n\n\n:::{#exm-globus697-dbinom}\n### Globus mit 6 Treffern bei 9 W√ºrfen, p=1/2\nWas ist der Anteil der g√ºltigen Pfade in einem Baumdiagramm (Wahrscheinlichkeit), \num 6 mal $W$ bei $N=W+L=9$ W√ºrfen zu bekommen, wenn wir von $p=1/2$ ausgehen?\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndbinom(x = 6, size = 9, prob = 1/2)\n## [1] 0.1640625\n```\n:::\n\n\n\n\n\nOder, synonym, wenn man einen Taschenrechner (oder R als Taschenrechner) benutzt:\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nchoose(9, 6) * (1/2)^6 * (1/2)^3\n## [1] 0.1640625\n```\n:::\n\n\n\n\n\n$\\square$\n:::\n\n\n::: {#exm-globus2}\n\n### Globus mit 9 Treffern bei 9 W√ºrfen, p=1/2\n\nWas ist die Wahrscheinlichkeit,\ngegeben $W=9$ bei $n=9$ und $\\pi=1/2$?\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndbinom(x = 9, size = 9, prob = 1/2)\n## [1] 0.001953125\n```\n:::\n\n\n\n\n\nDas ist 1 g√ºnstiger Pfad von 512 Pfaden, also $Pr(W=9|\\pi=1/2, n=9)=1/512$.\n\n:::\n\n\n:::{#exm-globus697}\n\n### Globus mit 6 Treffern bei 9 W√ºrfen, p=70%\n\nWas ist die Wahrscheinlichkeit f√ºr $W=6$,\ngegeben $n=9$ und $p=.7$?\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndbinom(x = 6, size = 9, prob = .7)\n## [1] 0.2668279\n```\n:::\n\n\n\n\n\nMit Taschenrechner gerechnet:\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nchoose(9,6) * (.7)^6 * (.3)^3\n## [1] 0.2668279\n```\n:::\n\n\n\n\n\n(Fast) von Hand gerechnet:\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfactorial(9)/(factorial(6)*factorial(3)) * (.7)^6 * (.3)^3\n## [1] 0.2668279\n```\n:::\n\n\n\n\n\n\n$\\square$\n:::\n\n\n\nZur Erinnerung: Die Funktion `dbinom` gibt uns die Wahrscheinlichkeit von `x` Treffern, \nbei `size` Versuchen zur√ºck, \nwobei eine Binomialverteilung angenommen wird mit Trefferwahrscheinlichkeit `prob`.\n\n\n\n### Unser Modell ist geboren\n\nEin Modell (in der Bayes-Statistik) besteht aus mind. zwei Komponenten:\n\n1. Die *Likelihood* (die Wahrscheinlichkeit der Daten unter Annahme der Hypothese), s. @eq-globus1\n2. Die *Priori-Verteilung(en)* (die Wahrscheinlichkeit der Hypothese vor den Daten), a. @eq-prior-unif-globus\n3. Die *Posteriori-Verteilung* (die Wahrscheinlichkeit der Hypothese nach den Daten), s. @eq-post-globus1\n\n\n### Likelihood\n\nIm Globusversuch verwenden wir die Binomialverteilung zur Berechnung der *Likelihood*, s. @eq-globus1.\n\n$$W \\sim \\text{Bin}(n,\\pi)$${#eq-globus1}\n\nLies: \"W ist *bin*omial verteilt mit den Parametern $n$ und $\\pi$\". $n$ gibt die Anzahl der Globusw√ºrfe an: $n=W+L$.\n\n\nMit einem konkretes Beispiel: $W \\sim \\text{Bin}(9, 0.7)$ bedeutet, \ndass wir von 9 W√ºrfen ausgehen und eine Wahrscheinlichkeit f√ºr Wasser von 70% annehmen.\n\n\nDie Verwendung der Binomialvertielung ist an einige Annahmen gekn√ºpft:\n\n1.  Die Z√ºge sind unabh√§ngig voneinander (Die W√ºrfe des Globusballs beeinflussen sich einander nicht).\n2.  Der Parameterwert $\\pi$ bleibt konstant (Der Wasseranteil der Erde √§ndert sich nicht w√§hrend des Versuchs).\n\n\n:::{#exr-annahmen1}\nüèã Welche Annahmen w√ºrden Sie √§ndern? \nWelche k√∂nnte man wegnehmen? Welche hinzuf√ºgen? \nWas w√§ren die Konsequenzen?$\\square$\n:::\n\n\n### Priori-Verteilung\n\nUnser Vorab-  bzw. *Apriori*-Wissen zu $p$ sei, dass uns alle Werte gleich (\"uniform\") plausibel erscheinen, s. @eq-prior-unif-globus.\n\n$$\\pi \\sim \\text{Unif}(0,1).$${#eq-prior-unif-globus}\n\nLies: \"$\\pi$ ist gleich (uniform) verteilt mit der Untergrenze 0 und der Obergrenze 1\".\n\nMan k√∂nnte auch sagen: Wir haben praktisch kein Vorwissen, wir sind erstmal (aprior) indifferent,\njeder Parameterwert erscheint uns erstmal gleich wahrscheinlich, \ns. @fig-unif01.\n\n\n\n\n\n\n\n\n\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![Gleichverteilung mit Parametern min=0 und max=1](0500-Globusversuch_files/figure-html/fig-unif01-1.png){#fig-unif01 width=50%}\n:::\n:::\n\n\n\n\n\n\n\n\n### Posteriori-Verteilung\n\ndie Posteriori-Verteilung quantifiziert unser Wissen nach Kenntnis der Daten, aufbauend auf unserem Vorwissen (Priori-Wissen).\nDie Posteriori-Verteilung ist das Ergebnis des Bayes-Updates, s. @eq-post-globus1.\n\nDie Wahrscheinlichkeit bestimmter Hypothesen nennt man Posteriori-Wahrscheinlichkeit und bezeichnet sie kurz mit $Pr(H|D)$.\nLies: \"Die Wahrscheinlichkeit der Hypothese H gegeben der Daten D\".\nDabei nimmt man stillschweigend an, dass die Daten anhand eines gewissen Modells generiert wurden.\n\n\n\n\n\n\n\n\n\n\n\n\n\n## Bayes' Theorem\n\n### Wozu wird Bayes in der Praxis genutzt?\n\n\n\n\nIn der Praxis nutzt man Bayes h√§ufig,\nwenn man Daten $D$ gesammelt hat,\nund wissen m√∂chte,\nwie wahrscheinlich eine Hypothese $H$ ist,\nim Lichte dieser gesammelten Daten, s. @eq-bayes3.\n\n<!-- $$D \\quad \\underrightarrow{Bayes} \\quad H$$ -->\n\n\n$$Pr(H|D) = \\frac{ Pr(H) \\cdot Pr(D|H) }{Pr(D)}$${#eq-bayes3}\n\n@eq-bayes3 fragt nach $Pr(H|D)$:\n\n>    Was ist die Wahrscheinlichkeit der Hypothese H, jetzt wo wir die Daten haben (und ein Modell?)\n\nUnd antwortet so (@eq-bayes3):\n\n>    Diese Wahrscheinlichkeit entspricht der Grundrate (Apriori-Wahrscheinlichkeit) der Hypothese mal der Plausibilit√§t (Likelihood) der Daten unter Annahme (gegeben) der Hypothese. Aus Standardisierungsgr√ºnden dividiert man noch die totale Wahrscheinlichkeit der Daten √ºber alle Hypothesen.\n\n\nF√ºr unser Globusbeispiel:\n\n\n>    Wie wahrscheinlich ist denn jetzt ein bestimmter Wasseranteil auf der Erde, $\\pi$, (gegeben den Daten, $W=6$ und $L=3$)? Also, wie wahrscheinlich ist z.B. ein Wasseranteil von 70% oder von 50%?\n\n\n\n\n\n### Bayes als bedingte Wahrscheinlichkeit\n\nBayes' Theorem wird h√§ufig verwendet,\num die Wahrscheinlichkeit einer Hypothese,\ngegeben einer bestimmten Datenlage, zu berechnen,\nalso $Pr(H|D)$.\nDabei ist Bayes' Theorem nichts anderes als eine normale bedingte Wahrscheinlichkeit.\n\n\n\n$Pr(H|D) = \\frac{\\overbrace{ Pr(H\\cap D)}^\\text{umformen}}{Pr(D)}$\n\n$Pr(H| D)$ kann man  umformen (vgl. @eq-pr-cond und @def-gem-wskt-abh),\ndann erh√§lt man Bayes' Theorem, s. @eq-bayes1:\n\n$$\\begin{aligned}\nPr(H|D) &=\\frac{Pr(D\\cap H)}{Pr(D)}  \\\\  &= \\frac{\\overbrace{Pr(H)}^\\text{Apriori-Wahrscheinlichkeit} \\cdot \\overbrace{Pr(D|H)}^\\text{Likelihood}}{\\underbrace{Pr(D)}_\\text{Evidenz}}\n\\end{aligned}$$ {#eq-bayes1}\n\n\n\n\n\n:::{#def-evidenz}\n### Evidenz\n$Pr(D)$ nennt man die *Evidenz*. \nDie Evidenz berechnet sich als Summe der Likelihoods √ºber alle Auspr√§gungen von $D$, d.h. als die totale Wahrscheinlichkeit von $D$, s. @eq-evidenz, vgl. auch @def-totwskt:\n\n$$Pr(D) = Pr(D|H) \\cdot Pr(H) + Pr(D| \\neg H) \\cdot Pr(\\neg H)$${#eq-evidenz}\n\n\nDie Aufgabe der Evidenz ist nur daf√ºr zu sorgen, dass der Bruch insgesamt nur Werte zwischen 0 und 1 annehmen kann. $\\square$\n:::\n\n:::{#exm-evidenz1}\nIn @exm-bayes1 betrug der Wert der Evidenz $0.03 + 0.002 + 0.012 = 0.044$,\nalso ca. 4%. $\\square$\n:::\n\n\n\n<!-- :::{#exm-regen-bayes} -->\n<!-- ### Bayes im kalten Regen -->\n<!-- Erinnerung wir uns (@exm-kalt-regen): Die Wahrscheinlichkeit f√ºr *Regen* und *kalt* ist gleich der Wahrscheinlichkeit von *Regen*, *gegeben kalt* mal der Wahrscheinlichkeit von *kalt*; das ist die Kettenregel (@def-kettenregel). $\\square$ -->\n<!-- ::: -->\n\n\n\n\n\n\n\n### Bayes' Theorem als Formel\n\n<!-- Gesucht ist die Wahrscheinlichkeit einer Hypothese gegeben einer bestimmten Datenlage, $Pr(H|D)$, s. @eq-bayes5 -->\n\n<!-- $$Pr(H|D) = \\frac{Pr(D|H) \\cdot Pr(H)}{Pr(D)} = \\frac{\\text{Likelihood}  \\cdot \\text{Priori-Wahrscheinlichkeit}}{\\text{Evidenz}}$${#eq-bayes5} -->\n\nSchauen wir uns die Bestandteile von Bayes' Theorem (@eq-bayes1) noch etwas n√§her an:\n\n-   (standardisierte) Posteriori-Wahrscheinlichkeit: $Pr_{Post} := Pr(H|D)$\n\n-   Likelihood: $L := Pr(D|H)$\n\n-   Apriori-Wahrscheinlichkeit: $Pr_{Priori} := Pr(H)$\n\n-   Evidenz: $E := Pr(D)$\n\n\nBayes' Theorem gibt die $Pr_{Post}$ an, wenn man die Gleichung\nmit der $Pr_{Priori}$ und dem $L$ f√ºttert.\nBayes' Theorem wird verwendet, um die $Pr_{Post}$ zu quantifizieren.\nDie $Pr_{Post}$ ist proportional zu $L \\times Pr_{Priori}$.\n\n\n\n\n\n### Posteriori als Produkt von Priori und Likelihood\n\nDie *unstandardisierte* Post-Wahrscheinlichkeit $Pr_{\\text{unPost}}$ ist einfach das Produkt von \nLikelihood und Priori, s. @eq-unpost.\n\n$$Pr_{\\text{unPost}} = L \\times \\text{Priori}$${#eq-unpost}\n\n\n\nAbb. @fig-post3 visualisiert, dass die Post-Verteilung eine Gewichtung von Priori und Likelihood ist.\nMathematisch gesprochen beruht diese Gewichtung auf einer einfachen Multiplikationen der beiden genannten Terme.\n\n\n\n![Prior mal Likelihood = Post](img/img241.png){#fig-post3}\n\n\nStandardisiert man die unstandardisierte Post-Verteilung,\nso erh√§lt man die standardisierte Post-Verteilung.\nDas Standardisieren dient nur dazu, einen Wert zwischen 0 und 1 zu erhalten. \nDies erreichen wir, indem wir durch die Summe aller Post-Wahrscheinlichkeiten dividieren.\nDie Summe der Post-Wahrscheinlichkeiten bezeichnet man (auch) als Evidenz, vgl. Gleichung @eq-post.\n\n\n$$\\text{Posteriori} = \\frac{\\text{Likelihood} \\times \\text{Priori}}{\\text{Evidenz}}$${#eq-post}\n\n\n\n\n\n\n### Wissen updaten: Wir f√ºttern Daten in das Modell\n\nGolems k√∂nnen lernen?! @fig-lernen-golem zeigt die Post-Verteilung, nach $n=1, 2, ...,n=9$ \nDatenpunkten, d.h. W√ºrfen mit dem Globusball.\nMan sieht: Am Anfang, apriori, also bevor die Daten haben, \nvor dem ersten Wurf also, ist jeder Parameterwert gleich wahrscheinlich f√ºr den Golem (das Modell).\nJe nach Ergebnis des Wurfes ver√§ndert sich die Wahrscheinlichkeit der Parameterwerte,\nkurz gesagt, die Post-Verteilung ver√§ndert sich in Abh√§ngigkeit von den Daten.\n\n\n![Unser Golem lernt](img/img221.png){#fig-lernen-golem}\n\n\nInsofern kann man sagen: Unser Golem (das Modell) lernt. \nOb das Modell n√ºtzlich ist (pr√§zise Vorhersagen liefert), \nsteht auf einem anderen Blatt.\n\n\n\n## Bayes berechnen mit mit der Bayes-Box\n\nWir erstellen uns eine kleine Tabelle, die man \"Bayes-Box\" nennen k√∂nnte.^[Auch Gitter-Methode oder Grid-Methode genannt.]\nDazu gehen wir so vor:\n\n### Die Idee der Bayes-Box\n\n1.  Teile den Wertebereich des Parameters in ein \"Gitter\" auf, z.B. $0.1, 0.2, ..., 0.9, 1$.\n2.  W√§hle den Priori-Wert des Parameters f√ºr jeden Parameterwert, z.B. 1/11 bei einer Gleichverteilung von 0 bis 1.\n3.  Berechne den Likelihood f√ºr jeden Parameterwert.\n4.  Berechne den unstandardisierten Posteriori-Wert f√ºr jeden Parameterwert (Produkt von Priori und Likelihood).\n5.  Standardisiere den Posteriori-Wert durch teilen anhand der Summe alle unstand. Posteriori-Werte.\n\n\nF√ºr jeden Parameterwert berechnen wir eine (Post-)Wahrscheinlichkeit.^[Ein Parameterwert ist eine m√∂gliche Auspr√§gung des Parameters.]\nH√§ufig entspricht eine Hypothese einem Parameterwert, \netwa wenn man sagt: \"Ich glaube, die M√ºnze ist fair\", was auf einen Parameterwert von 50% herausl√§uft.\nDazu geben wir an, f√ºr wie wahrscheinlich wie apriori^[synonym: priori] \n-- also bevor wir irgendwelche Daten erheben -- jeden einzelnen Parameterwert halten.\nWir machen es uns hier einfach und halten jeden Parameterwert f√ºr gleich wahrscheinlich. \nTats√§chlich ist der konkrete Wert hier egal, entscheidend ist das Verh√§ltnis der Apriori-Werte zueinander: \nGeben wir einigen Parameterwerten den Wert 2, aber anderen den Wert 1, \nso halten wir Erstere f√ºr (apriori) doppelt so plausibel wie Letztere.\nDer Likelihood wird in diesem Beispiel mit der Binomialverteilung berechnet \n(da wir ein bin√§res Ereignis, $W$ oder $L$, haben).\nDer Likelihood gibt an, \nwie wahrscheinlich ein Parameterwert ist gegeben einem bestimmten apriori gew√§hlten Parameterwert.\nDie \"End-Wahrscheinlichkeit\", die unstandardisierte Post-Wahrscheinlichkeit, \ndie \"hinten rauskommt\" ist das Produkt von Priori-Wert und Likelihood.\nAnschaulich gesprochen: Die Priori-Werte werden mit den Likelihoodwerten gewichtet^[synonym: Die Likelihoodwerte werden mit den Apriori-Werten gewichtet.].\nDa wir letztlich eine Wahrscheinlichkeitverteilung bekommen m√∂chten, \nteilen wir jeden Posteriori-Wert durch die Summe aller Posteriori-Werte. \nDadurch ist gerantiert, dass sich die Posteriori-Werte zu eins aufaddieren. \nDamit haben wir dann die Anspr√ºche an eine Wahrscheinlichkeitsverteilung erf√ºllt (vgl. @sec-kolmogorov).\n\n\n### Bayes-Box in R berechnen\n\nLegen wir uns ein Gitter mit Parameterwerten ($\\pi$) an, um deren Posteriori-Wahrscheinlichkeit zu berechnen.\nKonkret gesprochen: Wir listen jeden f√ºr uns interessanten Wasseranteil ($\\pi$) auf,\nalso $\\pi=0, 0.1, 0.2, ..., 1$.\nDiese Parameterwerte sind die Hypothesen, die wir testen wollen,\ns. @lst-wasseranteile.\n\n\n\n\n\n\n::: {.cell}\n\n```{#lst-wasseranteile .r .cell-code  lst-cap=\"Parameterwerte (Gitter) f√ºr Wasseranteile: 0, 0.1, 0.2, ..., 1\"}\nwasseranteile <- seq(from = 0, to = 1, by = 0.1)  # Parameterwerte\nwasseranteile\n##  [1] 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0\n```\n:::\n\n\n\n\n\n\nDann berechnen wir schon mal die Wahrscheinlichkeit der Daten (6 W bei 9 W√ºrfen) gegeben jeweils eines Wasseranteils:\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nLikelihood <- dbinom(6, size = 9, prob = wasseranteile)\nLikelihood\n##  [1] 0.000000000 0.000061236 0.002752512 0.021003948 0.074317824 0.164062500\n##  [7] 0.250822656 0.266827932 0.176160768 0.044641044 0.000000000\n```\n:::\n\n\n\n\n\nDann packen wir das alles in eine Tabelle, die \"Bayes-Box\", s. @tbl-globus und @lst-gitter1.\n\n\n\n\n\n::: {.cell}\n\n```{#lst-gitter1 .r .cell-code  lst-cap=\"Wir basteln uns eine Bayes-Box\"}\nd <-\n  tibble(\n    # definiere die Hypothesen (die Parameterwerte, p): \n    p = wasseranteile,\n    # Lege den Priori-Wert fest:\n    Priori  = 1/11) |> \n    mutate(\n      # berechne Likelihood f√ºr jeden Wasseranteil (Parameterwert):\n      Likelihood = Likelihood,\n      # berechne unstand. Posteriori-Werte:\n      unstd_Post = Likelihood * Priori,\n      # berechne Evidenz, d.i. die Summe aller unstand. Post-Werte:\n      Evidenz = sum(unstd_Post),\n      # berechne stand. Posteriori-Werte (summiert zu 1):\n      Post = unstd_Post / Evidenz)  \n```\n:::\n\n\n\n\n\nDie Bayes-Box (@tbl-globus) zeigt, wie sich die Post-Verteilung berechnet.\n\n\n\n\n\n::: {#tbl-globus .cell tbl-cap='Die Bayes-Box f√ºr den Globusversuch, k=6 Treffer, n=9 Versuche, Apriori-Wahrscheinlichkeit Pr(H)=9%, und Wasseranteile p von 0 bis 1'}\n::: {.cell-output-display}\n\n\n| id|   p| Priori| Likelihood| unstd_Post| Evidenz|  Post|\n|--:|---:|------:|----------:|----------:|-------:|-----:|\n|  1| 0.0|  0.091|      0.000|      0.000|   0.091| 0.000|\n|  2| 0.1|  0.091|      0.000|      0.000|   0.091| 0.000|\n|  3| 0.2|  0.091|      0.003|      0.000|   0.091| 0.003|\n|  4| 0.3|  0.091|      0.021|      0.002|   0.091| 0.021|\n|  5| 0.4|  0.091|      0.074|      0.007|   0.091| 0.074|\n|  6| 0.5|  0.091|      0.164|      0.015|   0.091| 0.164|\n|  7| 0.6|  0.091|      0.251|      0.023|   0.091| 0.251|\n|  8| 0.7|  0.091|      0.267|      0.024|   0.091| 0.267|\n|  9| 0.8|  0.091|      0.176|      0.016|   0.091| 0.176|\n| 10| 0.9|  0.091|      0.045|      0.004|   0.091| 0.045|\n| 11| 1.0|  0.091|      0.000|      0.000|   0.091| 0.000|\n\n\n:::\n:::\n\n\n\n\n\n\nF√ºr jede Hypothese (Spalte `id`) berechnen wir die *unstandardisierte*\nPosteriori-Wahrscheinlichkeit als Produkt von Priori und Likelihood, s. @eq-postunstand.    \n\n$$\\text{Post}_{\\text{unstand}} = \\text{Priori} \\cdot \\text{Likelihood}$${#eq-postunstand}\n\nUm zur *standardisierten* Posteriori-Wahrscheinlichkeit zu gelangten,\nteilen wir in jeder Zeile der Bayesbox (also f√ºr jede Hypothese) \ndie unstandardisierte Post-Wahrscheinlichkeit durch die Summe \nder unstandardisierten Post-Wahrscheinlichkeiten, s. @eq-poststand-allgemein.\n\n\n$$\\text{Post} = \\frac{\\text{Post}_{\\text{unstand}}}{\\text{Evidenz}} = \\frac{Pr(H) \\cdot Pr(H|D)}{Pr(D)}$${#eq-poststand-allgemein}\n\nDabei haben wir die Priori-Wahrscheinlihkeit f√ºr alle Parameterwerte als gleich angenommen, da wir keinerlei Vorwissen hatten, $Pr(H_i) = 1/11$.\nDie Evidenz berechnet sich als Summe der unstandardisierten Post-Wahrscheinlichkeiten, $Pr(D) = 0.09$.\n\n:::callout-note\nWenn der Priori-Wert f√ºr jeden Parameterwert gleich ist, dann ist der Likelihood gleich der unstandardisierten Post-Wahrscheinlichkeit.$\\square$\n:::\n\n\n:::{#exm-globus697-post}\n### Post-Wahrscheinlichkeit im Globusversuch f√ºr p=.7\nIn @exm-globus697 haben wir die Wahrscheinlichkeit f√ºr 6 Treffer bei 9 W√ºrfen gegeben einer Trefferwahrscheinlichkeit von $\\pi = .7$ berechnet.\nDamit haben wir die Likelihood $L = Pr(D|H) =.25$ berechnet.\n\n\nAuf dieser Basis k√∂nnen wir die Posteriori-Wahrscheinlichkeit $Pr_{Post}$ berechnen, zun√§chst die unstandardisierte.\nDazu haben wir die Priori-Wahrscheinlichkeit mit der Likelihood multipliziert, s. @eq-postunstand697:\n\n$$\\text{Post}_{\\text{unstand}} = Pr(H) \\cdot Pr(D|H) = 0.09 \\cdot 0.25 = 0.025$${#eq-postunstand697}\n\nJetzt standardisieren wir die unstandardisierte Post-Wahrscheinlichkeit, indem wir durch die Evidenz dividieren, s. @eq-poststand697.\n\n$$\\text{Post} = \\frac{\\text{Post}_{\\text{unstand}}}{\\text{Evidenz}} = \\frac{0.025}{0.1} =0.25$${#eq-poststand697}\n\n\n@eq-bayes-globus697 fasst die Schritte der Berechnung zusammen.\n\n$$\\begin{aligned}\nPr(H_{\\pi=0.7}|D) =\n\\frac{Pr(D|H) \\cdot Pr(H)}{Pr(D)} &= \\\\\n\\frac{\\text{Likelihood}  \\cdot \\text{Priori-Wahrscheinlichkeit}}{\\text{Evidenz}} &= \\\\\n\\frac{0.25 \\cdot 0.1}{0.1} &= 0.25\n\\end{aligned}$${#eq-bayes-globus697} $\\square$\n\nFazit: Nach dem Versuch, d.h. nachdem wir die Daten in Betracht gezogen haben,\nhat sich unsere Meinung √ºber den Wasseranteil geupdatet von 0.1 auf 0.25.$\\square$\n:::\n\n\n\n\n\n\n:::{#exr-priori-change}\nüèãÔ∏è Was wohl mit *Post* passiert, wenn wir *Priori* √§ndern?$\\square$\n:::\n\n@fig-post1 zeigt eine Visualisierung der Post-Verteilung mit Hilfe der Funktion `ggline(x, y)` aus dem Paket `ggpubr`.\nWie man sieht, ist die Post-Wahrscheinlichkeit am h√∂chsten bei $\\pi=0.7$.\nWobei der Bereich von 0.6 bis 0.8 auch recht wahrscheinlich ist.\n\n\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![Die Post-Verteilung visualisiert. Die Post-Wahrscheinlichkeit ist am h√∂chsten bei p=0.7](0500-Globusversuch_files/figure-html/p-post1-1.png){width=70%}\n:::\n:::\n\n\n\n\n\n\n\n\n\n### Was sagt die Post?\n\nDie Posteriori-Verteilung (Kurz: \"Post-Verteilung\", oder \"Post\"), $Pr_{Post}$, zeigt, wie plausibel wir jeden Wert von $p$ halten, \njetzt, nachdem wir die Daten des Versuchs kennen.\nDie Post-Wahrscheinlichkeit updatet unser Apriori-Wissen mit dem Wissen, \ndas wir durch die Daten erhalten haben.\n\n\n\n@fig-gitter zeigt die Post-Wahrscheinlichkeit f√ºr 5, 10 und 20 Parameterwerte. Das mittlere Teilbild (10 Gitterwerte) entspricht unserer Tabelle oben.\nMan sieht: Je mehr Parameterwerte, desto genauer wird die Verteilung wiedergegeben.\n\n\n![Je mehr Parameterwerte, desto genauer wird die Verteilung wiedergegeben.](img/img242.png){#fig-gitter}\n\n\n:::{.callout-note}\nUnter sonst gleichen Umst√§nden gilt:\n\n- Mehr Gitterwerte gl√§tten die Ann√§herung.\n- Je gr√∂√üer die Stichprobe ($N$), desto zuverl√§ssiger wird unsere Berechnung. $\\square$\n:::\n\n\n\n\n\n\n:::{#callout-important}\nDie Post-Verteilung ist sowas wie das Ziel all Ihrer Tr√§ume (falls Sie es noch nicht gewusst haben):\nAus der Post-Verteilung k√∂nnen Sie ablesen,\nwie wahrscheinlich Ihre Hypothese (Ihr Lieblings-Parameterwert) ist. Und noch einiges mehr, aber das ist Thema des n√§chsten Kapitels. $\\square$\n:::\n\n\n\n\n\n## Abschluss\n\n### Zusammenfassung\n\n\nüì∫ [√úbung zum Globusversuch](https://www.youtube.com/watch?v=YJEZiQvCBgs&list=PLRR4REmBgpIGgz2Oe2Z9FcoLYBDnaWatN&index=7)\n\n-   In unserem Modell haben wir Annahmen zu $Pr_{Priori}$ und $L$ getroffen.\n\n-   Auf dieser Basis hat der Golem sein Wissen geupdated zu $Pr_{Post}$.\n\n-   Mit der Gitter-Methode haben wir viele Hypothesen (Parameterwerte) untersucht und jeweils die $Pr_{Post}$ berechnet.\n\n-   Unser Modell bildet die kleine Welt ab; ob es in der gro√üen Welt n√ºtzlich ist, steht auf einem anderen Blatt.\n\n:::{#exr-tipp-p}\nüèãÔ∏è Wenn Sie auf einen Prozentwert f√ºr $W$ tippen m√ºssten, welchen w√ºrden Sie nehmen, laut dem Modell (und gegeben der Daten)? $\\square$\n:::\n\n\n### Der Globusversuch als Modell f√ºr zweiwertige Zufallsversuche\n\nDer Globusversuch ist kein prototypisches Beispiel f√ºr Statistik in der Praxis, zumindest nicht auf dem ersten Blick. \nEr hat aber aber den Vorteil, dass es ein einfaches, gut greifbares Beispiel ist, und damit zum Lernen gut geeignet ist.\nBei n√§herer Betrachtung ist der Globusversuch prototypisch f√ºr ganz viele Fragestellungen:\n\n- Von einem neuen Produkt von von $n$ Exemplaren $k$ verkauft. Auf welchen Wert $p$ kann die Akzeptanzrate dieses Produkts gesch√§tzt werden?\n- Ein Chat-Bot hat von $n$ Fragen $k$ richtig beantwortet. Wie hoch kann die Verst√§ndnisrate $p$ dieses Programms gesch√§tzt werden?\n- Eine neue Krebstherapie hat von $n$ \"austherapierten\" Patientis $k$ geheilt. Auf wie hoch kann die Erfolgsrate dieser Therapie gesch√§tzt werden?\n\n\n\nKurz: Der Globusversuch ist ein Muster f√ºr zweiwertige Zufallsversuche. Und solche sind h√§ufig im Leben, im Business und in der Wissenschaft.\n\n\n\n\n\n## Vertiefung\n\n\n### Bayes-Video von 3b1b\n\nDas [\"Bayes-Paradox-Video\" von 3b1b](https://youtu.be/lG4VkPoG3ko) pr√§sentiert eine gut verst√§ndliche Darstellung des Bayes-Theorem aus einer zwar nicht gleichen, \naber √§hnlichen Darstellung wie in diesem Kapitel.\n\n\n\n### Bayes als Baum\n\nBayes' Theorem kann man sich als als Baumdiagramm vor Augen f√ºhren,\n@fig-tot-wskt2.\n\nGesucht sei $Pr(M_1|A)$,\nalso: die Wahrscheinlichkeit,\ndass das Teil von Maschine 1 produziert wurde, gegeben, dass es Ausschuss ist.\nGegeben sind die Wahrscheinlichkeiten, dass Machine $i$ das Teil produziert hat, $Pr(M_i)$. Au√üerdem sind die Wahrscheinlichkeiten, dass das Teil Ausschuss ist, $Pr(A|M_i)$, bekannt.\n\nDas Diagramm l√∂st die Aufgabe f√ºr uns;\nes zeigt damit die Anwendung von Bayes' Theorem auf.\n\nUm $Pr(M_1|A)$ zu erhalten,\nsetzt man die Wahrscheinlichkeit\ndes  *g√ºnstigen* Asts ins Verh√§ltnis zur Wahrscheinlichkeit \n*aller relevanten* √Ñste, $Pr(A)$.\n\n:::{#exm-bayes1}\n### Maschine produziert Ausschuss\n\nDie drei Maschinen $M_1, M_2, M_3$ produzieren den gleichen Artikel. Ihr jeweiliger Anteil, an der Produktion liegt bei 60%, 10% bzw. 30%. \nDie jeweilige Ausschussquote liegt bei 5, 2, bzw. 4%, s. @fig-tot-wskt2.\n\n*Aufgabe*: Wie gro√ü ist die Wahrscheinlichkeit, dass ein defektes Teil von Maschine 1 produziert wurde? Berechnen Sie diese Wahrscheinlichkeit.$\\square$\n:::\n\n\n\nDer g√ºnstige (gesuchte) Ast, $Pr(M1 \\cap A)$, ist hier fett gedruckt, s. @fig-tot-wskt2. \nIn @fig-tot-wskt2 zeigen die runden K√§stchen am Ende der Pfade die Wahrscheinlichkeiten des jeweiligen Pfades an.\n\n\n\n\n\n\n\n\n\n```{mermaid}\n%%| fig-cap: G√ºnstige Pfade\n%%| label: fig-tot-wskt2\nflowchart LR\n  A[Start] ==>|0.60|B[M1]\n  A --->|0.10|C[M2]\n  A --->|0.30|D[M3]\n  B ==>|0.05|E[A]\n  B -->|0.95|F[Nicht-A]\n  C --->|0.02|G[A]\n  C --->|0.98|H[Nicht-A]\n  D --->|0.04|I[A]\n  D --->|0.96|J[Nicht-A]\n  E --- K((0.030))\n  F --- L((0.570))\n  G --- M((0.002))\n  H --- N((0.098))\n  I --- O((0.012))\n  J --- P((0.288))\n```\n\n\n\n\n\n\n\n$$Pr(M1|A) = \\frac{Pr(M1 \\cap A)}{Pr(A)} = \\frac{0.6 \\cdot 0.05}{0.03 + 0.002 + 0.012} = \\frac{0.03}{0.044} \\approx 0.68$$\n\n\n$Pr(M1|A)$ betr√§gt also ca. 68%.\n\nZur Erinnerung: $Pr(A)$ ist die totale Wahrscheinlichkeit\n(dass ein produziertes Teil Ausschuss ist).\n\n\n\n\n### Weitere Herleitung der Bayes-Formel\n\nMan kann sich Bayes' Theorem  auch wie folgt herleiten:\n\n\n$Pr(D\\cap H) = Pr(D \\cap H) = Pr(D) \\cdot Pr(H|D) = Pr(H) \\cdot Pr(D|H)$\n\nDann l√∂sen wir nach P$(H|D)$ auf, s. @eq-bayes2.\n\n\n$$Pr(H|D) = \\frac{\\overbrace{Pr(H)}^\\text{Apriori-Wahrscheinlichkeit} \\cdot \\overbrace{Pr(D|H)}^\\text{Likelihood}}{\\underbrace{Pr(D)}_\\text{Evidenz}}$${#eq-bayes2}\n\n\n\n\n### Zusammengesetzte Hypothesen\n\nDas ist vielleicht ein bisschen fancy,\naber man kann Bayes' Theorem auch nutzen, um die Wahrscheinlichkeit einer *zusammengesetzten Hypothese* zu berechnen: $H = H_1 \\cap H_2$. \nEin Beispiel w√§re: \"Was ist die Wahrscheinlichkeit, dass es Regen ($R$) *und* Blitzeis ($B$) gibt, wenn es kalt ($K$) ist?\".\n\nDas sieht dann so aus, @eq-bayes4:\n\n$$\n\\begin{aligned}\nPr(R \\cap B |K) &= \\frac{ Pr(R \\cap B) \\cdot Pr(K|R \\cap B) }{Pr(D)} \\\\\n&= \\frac{ Pr(R ) \\cdot Pr(B) \\cdot Pr(K|R \\cap B) }{Pr(D)}\n\\end{aligned}\n$${#eq-bayes4}\n\n\nHier haben wir $Pr(R \\cap B)$  aufgel√∂st in $Pr(R) \\cdot Pr(B)$,\ndas ist nur zul√§ssig, wenn $R$ und $B$ unabh√§ngig sind.\n\n\n\n\n\n\n## Aufgaben\n\n:::{.callout-tip}\nEinige der folgenden Aufgaben sind in englischer Sprache.\nWenn Ihnen eine andere Sprache (z.B. Deutsch) lieber ist, nutzen\nSie einfach die √úbersetzungsfunktion Ihres Browsers. Das sind meist nur zwei Klicks. $\\square$\n:::\n\n### Papier-und-Bleistift-Aufgaben\n\n1. [Verteilungen-Quiz-01](https://datenwerk.netlify.app/posts/verteilungen-quiz-01/verteilungen-quiz-01)\n2. [globus1](https://datenwerk.netlify.app/posts/globus1/index.html)\n2. [globus2](https://datenwerk.netlify.app/posts/globus2/index.html)\n3. [globus3](https://datenwerk.netlify.app/posts/globus3/index.html)\n4. [globus-bin](https://datenwerk.netlify.app/posts/globus-bin/index.html)\n4. [globus-bin2](https://datenwerk.netlify.app/posts/globus-bin2/index.html)\n3. [Krebs1](https://datenwerk.netlify.app/posts/krebs1/krebs1)\n1. [Lose-Nieten-Binomial-Grid](https://datenwerk.netlify.app/posts/lose-nieten-binomial-grid/lose-nieten-binomial-grid)\n2. [kekse01](https://datenwerk.netlify.app/posts/kekse01/kekse01.html)\n2. [bayes2](https://datenwerk.netlify.app/posts/bayes2/bayes2)\n3. [Bayes-Theorem1](https://datenwerk.netlify.app/posts/bayes-theorem1/bayes-theorem1) \n5. [bayes-ziel1](https://datenwerk.netlify.app/posts/bayes-ziel1/bayes-ziel1)\n6. [totale-wskt1](https://datenwerk.netlify.app/posts/totale-wskt1/totale-wskt1)\n7. [wskt-quiz13](https://datenwerk.netlify.app/posts/wskt-quiz13/wskt-quiz13)\n8. [wskt-quiz12](https://datenwerk.netlify.app/posts/wskt-quiz12/wskt-quiz12)\n8. [wskt-quiz15](https://datenwerk.netlify.app/posts/wskt-quiz15/wskt-quiz15)\n\n\n### Aufgaben, f√ºr die man einen Computer braucht\n\n<!-- 2. [Rethink2E4](https://datenwerk.netlify.app/posts/Rethink2e4/Rethink2e4) -->\n2. [Rethink2m1](https://datenwerk.netlify.app/posts/Rethink2m1/Rethink2m1)\n2. [Rethink2m2](https://datenwerk.netlify.app/posts/Rethink2m2/Rethink2m2)\n2. [Rethink2m3](https://datenwerk.netlify.app/posts/Rethink2m3/Rethink2m3)\n<!-- 2. [Rethink2m4](https://datenwerk.netlify.app/posts/Rethink2m4/Rethink2m4) -->\n<!-- 2. [Rethink2m5](https://datenwerk.netlify.app/posts/Rethink2m5/Rethink2m5) -->\n<!-- 2. [Rethink2m6](https://datenwerk.netlify.app/posts/Rethink2m6/Rethink2m6) -->\n<!-- 2. [Rethink2m7](https://datenwerk.netlify.app/posts/Rethink2m7/Rethink2m7) -->\n2. [kekse02](https://datenwerk.netlify.app/posts/kekse02/kekse02.html)\n2. [euro-bayes](https://datenwerk.netlify.app/posts/euro-bayes/euro-bayes.html)\n3. [bath42](https://datenwerk.netlify.app/posts/bath42/bath42)\n4. [Kaefer2](https://datenwerk.netlify.app/posts/kaefer2/kaefer2)\n5. [rethink3m1](https://datenwerk.netlify.app/posts/rethink3m1/rethink3m1)\n\n\n\n\n\n\n\n\n## ---\n\n\n\n![](img/outro-05.jpg){width=100%}\n\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<link href=\"site_libs/pagedtable-1.1/css/pagedtable.css\" rel=\"stylesheet\" />\n<script src=\"site_libs/pagedtable-1.1/js/pagedtable.js\"></script>\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}