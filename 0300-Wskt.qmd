
# Wahrscheinlichkeit

```{r}
#| include: false
library(tidyverse)
library(easystats)
library(lubridate)
library(gmp)
library(titanic)
theme_set(theme_modern())
```



```{r}
#| include: false
theme_set(theme_modern())
```


```{r define-plots-16}
#| echo: false
#| fig-width: 7

plot16a <-
  ggplot(data.frame(A = c(0, 1),
                    B = c(0, 1))) +
  aes(x = A, y = B) +
  annotate("rect", xmin = 0, xmax = 1, ymin = 0, ymax = .5,
           color = "yellow", alpha = .7, fill = NA) +
  annotate("rect", xmin = 0, xmax = 1, ymin = 0.5, ymax = 1,
           color = NA, alpha = .5, fill = "yellow") +
  annotate("rect", xmin = 0, xmax =0.5, ymin = 0, ymax = 1,
           color = "blue", alpha = .7, fill = NA) +
  annotate("rect", xmin = 0.5, xmax = 1, ymin = 0, ymax = 1,
           color = "blue", alpha = .5, fill = "blue") +
  scale_x_continuous(breaks = c(0.25, 0.75), labels = c("¬A", "A")) +
  scale_y_continuous(breaks = c(0.25, 0.75), labels = c("¬B", "B"))


plot16a1 <-
  ggplot(data.frame(A = c(0, 1),
                    B = c(0, 1))) +
  aes(x = A, y = B) +
  annotate("rect", xmin = 0, xmax = 1, ymin = 0, ymax = .5,
           color = "yellow", alpha = .7, fill = NA) +
  annotate("rect", xmin = 0, xmax = 1, ymin = 0.5, ymax = 1,
           color = NA, alpha = .5, fill = "yellow") +
  scale_x_continuous(breaks = c(0.25, 0.75), labels = c("¬A", "A")) +
  scale_y_continuous(breaks = c(0.25, 0.75), labels = c("¬B", "B")) +
  annotate("label", x = .5, y = .75, label = "Pr(B) = 50%")

plot16a2 <-
  ggplot(data.frame(A = c(0, 1),
                    B = c(0, 1))) +
  aes(x = A, y = B) +
  annotate("rect", xmin = 0, xmax =0.5, ymin = 0, ymax = 1,
           color = "blue", alpha = .7, fill = NA) +
  annotate("rect", xmin = 0.5, xmax = 1, ymin = 0, ymax = 1,
           color = "blue", alpha = .5, fill = "blue") +
  scale_x_continuous(breaks = c(0.25, 0.75), labels = c("¬A", "A")) +
  scale_y_continuous(breaks = c(0.25, 0.75), labels = c("¬B", "B")) +
  annotate("label", x = .75, y = .5, label = "Pr(A) = 50%")


plot16b <-
ggplot(data.frame(A = c(0, 1),
                  B = c(0, 1))) +
  aes(x = A, y = B) +
  annotate("rect", xmin = 0, xmax = 1, ymin = 0, ymax = .5,
           color = "yellow", alpha = .7, fill = NA) +
  annotate("rect", xmin = 0, xmax = 1, ymin = 0.5, ymax = 1,
           color = NA, alpha = .5, fill = "yellow") +
  annotate("rect", xmin = 0, xmax =0.5, ymin = 0, ymax = 1,
           color = "blue", alpha = .7, fill = NA) +
  annotate("rect", xmin = 0.5, xmax = 1, ymin = 0, ymax = 1,
           color = "blue", alpha = .5, fill = "blue") +
  annotate("rect", xmin = 0.5, xmax = 1, ymin = 0.5, ymax = 1,
           color = "green", alpha = .7, fill = NA) +
  scale_x_continuous(breaks = c(0.25, 0.75), labels = c("¬A", "A")) +
  scale_y_continuous(breaks = c(0.25, 0.75), labels = c("¬B", "B")) +
  annotate(geom = "label", x = .75, y = 0.75, label = "A,B")





plot16c <-
  ggplot(data.frame(A = c(0, 1),
                    B = c(0, 1))) +
  aes(x = A, y = B) +
  annotate("rect", xmin = 0, xmax = 1, ymin = 0, ymax = .5,
           color = "yellow", alpha = .7, fill = NA) +
  annotate("rect", xmin = 0, xmax = 1, ymin = 0.5, ymax = 1,
           color = NA, alpha = .5, fill = "NA") +
  annotate("rect", xmin = 0, xmax =0.5, ymin = 0, ymax = 1,
           color = "blue", alpha = .7, fill = NA) +
  annotate("rect", xmin = 0.5, xmax = 1, ymin = 0, ymax = 1,
           color = "blue", alpha = .5, fill = "blue", linewidth = 2) +
  annotate("rect", xmin = 0.5, xmax = 1, ymin = 0.5, ymax = 1,
           color = "green", alpha = .3, fill = "green") +
  scale_x_continuous(breaks = c(0.25, 0.75), labels = c("¬A", "A")) +
  scale_y_continuous(breaks = c(0.25, 0.75), labels = c("¬B", "B")) +
  annotate(geom = "label", x = .75, y = 0.75, label = "B|A")


```

## Lernsteuerung


### Lernziele

Nach Absolvieren des jeweiligen Kapitels sollen folgende Lernziele erreicht sein.

Sie können ...


- die Grundbegriffe der Wahrscheinlichkeitsrechnung erläuternd definieren
- die drei Arten der direkten Ermittlung von Wahrscheinlichkeit erläutern
- typische Relationen (Operationen) von Ereignissen anhand von Beispielen veranschaulichen
- mit Wahrscheinlichkeiten rechnen



### Eigenstudium


:::{.callout-note}
Dieses Kapitel ist selbständig im Eigenstudium vorzubereiten vor dem Unterricht.
Lesen Sie dazu die angegebene Literatur.$\square$
:::


### Prüfungsrelevanter Stoff

Lesen Sie dazu @bourier_2018, Kap. 2-4. 
Weitere Übungsaufgaben finden Sie im dazugehörigen Übungsbuch, @bourier_statistik-ubungen_2022.







### Zentrale Begriffe


#### Grundbegriffe

- Zufallsvorgang (Zufallsexperiment)
- Elementarereignis
- Ereignisraum
- Zufallsereignis (zufälliges Ereignis)
- Sicheres Ereignis
- Unmögliches Ereignis


#### Wahrscheinlichkeitsbegriffe

- Klassische Wahrscheinlichkeit (LaPlace'sche Wahrscheinlichkeit)
- Statistische (empirische) Wahrscheinlichkeitsermittlung
- Subjektive (Bayes) Wahrscheinlichkeitsermittlung


#### Wahrscheinlichkeitsrelationen

- Vereinigung von Ereignissen
- Schnitt(menge) von Ereignissen
- Komplementärereignis
- Vollständiges Ereignissystem
- Kolmogorovs Definition von Wahrscheinlichkeit


#### Wahrscheinlichkeitsrechnung

- Allgemeiner Additionsssatz
- Disjunkte Ereignisse
- Additionssatz für disjunkte Ereignisse
- Bedingte Wahrscheinlichkeit
- (Stochastische) Unabhängigkeit
- Baumdiagramm für gemeinsame Wahrscheinlichkeit
- Allgemeiner Multiplikationssatz
- Multiplikationssatz für unabhängige Ereignisse
- Totale Wahrscheinlichkeit
- Satz von Bayes


### Begleitvideos


- [Video zum Thema Wahrscheinlichkeit](https://youtu.be/rR6NspapEyo)




## Unterstützung: Wahrscheinlichkeit in Bildern

Wahrscheinlichkeit in Bildern: zur einfachen Erschließung des Materials,
ein Unterstützungsangebot.


Im Folgenden sind einige Schlüsselbegriffe und -regeln in (ver-)einfach(t)er Form schematisch bzw. visuell dargestellt mit dem Ziel, den Stoff einfacher zu erschließen.


### Zufall

Werfen Sie eine Münze!

Diese hier zum Beispiel:

![](img/1024px-Coin-155597.svg.png){width=10% fig-align="center"}

[Quelle: By OpenClipartVectors, CC0]( https://pixabay.com/pt/moeda-euro-europa-fran%C3%A7a-dinheiro-155597)

Wiederholen Sie den Versuch 10, nein, 100, nein 1000, nein: $10^6$ Mal.

Notieren Sie das Ergebnis!

Oder probieren Sie die [App der Brown University](https://seeing-theory.brown.edu/basic-probability/index.html#section1).



### Relationen von Mengen

Venn-Diagramme eigenen sich, um typische Operationen (Relationen) auf Mengen zu visualisieren.



#### Überblick

Die folgenden Diagramme stammen von [Wikipedia (En)](https://en.wikipedia.org/wiki/Venn_diagram).

Wir gehen von Ereignisraum $\Omega$ aus, mit dem Ereignis $A$ als Teilmenge: $A \subset B$.


::: {#fig-sets layout-ncol=2}




![$A \cap B$](img/Venn0001.svg.png){width=5%}




![$A \cup B$](img/Venn0111.svg.png){width=5%}




![$\bar{A}$](img/2560px-Venn1010.svg.png){width=5%}






![$A \setminus B$](img/Venn0100.svg.png){width=5%}

Typische Mengenoperationen
:::



#### Disjunkte Ereignisse


(Engl. disjoint events)

$A= \{1,2,3\}; B= \{4,5,6\}$

$A$ und $B$ sind disjunkt: ihre Schnittmenge ist leer: $A \cap B = \emptyset$,
s. @fig-disjunkt





![Zwei disunkte Ereignisse, dargestellt noch überlappungsfreie Kreise](img/2880px-Disjunkte_Mengen.svg.png){#fig-disjunkt width="25%" fig-align="center"}





#### Eselsbrücke zur Vereinigungs- und Schnittmenge

Das Zeichen für eine Vereinigung zweier Mengen kann man leicht mit dem Zeichen für einen Schnitt zweier Mengen leicht verwechseln; daher kommt eine Eselbrücke gelesen, s. @fig-esel.

![Eselsbrücke für Vereinigungs- und Schnittmenge](img/ven_cup_cap.jpeg){#fig-esel width=55%}

[Quelle: rither.de](http://www.rither.de/a/mathematik/stochastik/mengentheorie-und-venn-diagramme/)


#### Animationen


[Animation zu Mengenoperationen](https://seeing-theory.brown.edu/compound-probability/index.html)



[Animation zur Vereinigung von Mengen](https://www.geogebra.org/m/GEZV4xXc#material/cmXR8fHN)

[Quelle](Geogebra, J. Merschhemke)


### Additionssatz 

Der Additionssatz wird verwendet, wenn wir an der Wahrscheinlichkeit interessiert sind, dass *mindestens eines der Ereignisse* eintritt.

#### Diskunkte Ereignisse

$\Omega = {1,2,3,4,5,6}$



$\boxed{1\; 2\; 3\; 4\; 5\; 6}$

Gesucht sei die Wahrscheinlichkeit des Ereignis $A=\{1,2\}$.


$\boxed{\boxed{1\; 2}\; \color{gray}{ 3\; 4\; 5\; 6}}$

$P(1 \cup 2) = \frac{1}{6} + \frac{1}{6} = \frac{2}{6}$


#### Allgemein (disjunkt oder nicht disjunkt)


Bei der Addition der Wahrscheinlichkeiten für $A$ und $B$ wird der Schnitt $A\cap B$ doppelt erfasst. Er muss daher noch abgezogen werden (vgl. @fig-sets2):



$$P(A \cup B) = P(A) + P(B) - P(A\cap B)$$



::: {#fig-sets2 layout-ncol=2}

![$A \cup B$](img/Venn0111.svg.png){width=5%}




![$A \cap B$](img/Venn0001.svg.png){width=5%}



Die Schnittmenge muss beim Vereinigen abgezogen werden,
damit sie nicht doppelt gezählt wird.


:::



### Bedingte Wahrscheinlichkeit


#### Animation


Schauen Sie sich mal diese [Wahnsinnsanimation von Victor Powell an](https://setosa.io/conditional/). Hammer!


#### Schema


Abb. @fig-schema-p illustriert gemeinsame Wahrscheinlichkeit, $P(A \cap B) und bedingte Wahrscheinlichkeit, $P(A|B)$.


```{r bed-w-schema}
#| echo: false
#| fig-cap: Illustration von gemeinsamer und bedingter Wahrscheinlichkeit
#| label: fig-schema-p
plots(plot16a1, plot16a2, plot16c)
```



Bedingte Wahrscheinlichkeit ist vergleichbar zu Filtern einer Tabelle:

```{r}
#| echo: true
d <- 
  tibble::tribble(
      ~id, ~A, ~B,
      "1", 0L, 0L,
      "2", 0L, 1L,
      "3", 1L, 0L,
      "4", 1L, 1L,
  "SUMME", 2L, 2L
  )

```

Es ergeben sich folgende Wahrscheinlichkeiten:

$P(A) = 2/4$

$P(B) = 2/4$

$P(A \cap B) = 1/4$

$P(A|B) = 1/2$






### (Un-)Abhängigkeit

Stochastische Unabhängigkeit ist ein Spezialfall von Abhängigkeit: Es gibt sehr viele Ausprägungen für Abhängigkeit, aber nur eine für Unabhängigkeit.
Können wir Unabhängigkeit nachweisen, haben wir also eine starke Aussage getätigt.






```{r}
#| echo: false
data("titanic_train")


plottitanic1 <-
titanic_train %>%
  select(Pclass, Survived) %>%
  mutate(Survived = factor(Survived)) %>%
  ggplot(aes(x = Pclass)) +
  geom_bar(aes(fill = Survived), position = "fill") +
  theme(legend.position = "bottom")


plottitanic2 <-
titanic_train %>%
  select(Survived, Embarked) %>%
  filter(Embarked %in% c("C", "Q", "S")) %>%
  mutate(Survived = factor(Survived)) %>%
  ggplot(aes(x = Embarked)) +
  geom_bar(aes(fill = Survived), position = "fill") +
  theme(legend.position = "bottom")





plottitanic3 <-
  titanic_train %>%
  select(Survived, Age) %>%
  mutate(Age_prime = isprime(Age),
         Age_prime = factor(Age_prime)) %>%
  mutate(Survived = factor(Survived)) %>%
  ggplot(aes(x = Age_prime)) +
  geom_bar(aes(fill = Survived), position = "fill") +
  theme(legend.position = "bottom") +
  scale_x_discrete(breaks = c(0, 2),
                     labels = c("Nicht Prim", "Prim"))

# plottitanic3

```



*Abhängig*, s. @fig-abh, links: Überleben auf der Titanic ist offenbar *abhängig* von der Passagierklasse.
Auf der anderen Seite: Das Ereignis *Überleben* auf der Titanic ist *un*abhängig vom Ereignis *Alter ist eine Primzahl*, s. @fig-abh, rechts.





```{r QM2-Thema1-WasistInferenz-31, out.width="100%"}
#| fig-cap: "Abhängigkeit und Unabhängigkeit zweier Ereignisse"
#| label: fig-abh
#| echo: false
plots(plottitanic1, plottitanic3,
      n_rows = 1,
      title = c("Abhängigkeit zweier Ereignisse", "Un-Abhängigkeit zweier Ereignisse"))
```








Zur Ab- bzw. Un-Abhängigkeit zweier Variablen, an Beispielen illustriert.




:::{#exm-covid}

## Zusammenhang von Covidsterblichkeit und Impfquote


Sind die Ereignisse *Tod durch Covid*  bzw. *Impfquote* ($A$) und *Land*^[hier mit den zwei Ausprägungen *DEU* und *USA*] ($B$) voneinander abhängig (Abb. @fig-covid1)?

```{r covid1}
#| message: false
#| echo: false
#| cache: true
#| fig-cap: Impfquote und Sterblichkeit sind voneinander abhängig (bezogen auf Covid, auf Basis vorliegender Daten)
#| label: fig-covid1


# source: https://ourworldindata.org/covid-vaccinations
# access date: 2021-09-24
# licence: https://ourworldindata.org/covid-vaccinations#licence



dfile <- "data/owid-covid-data.csv"



d <- read_csv(dfile)

d2<-
  d %>%
  filter(iso_code %in% c("DEU", "USA")) %>%
  mutate(date = as_date(date)) %>%
  rename(Land = iso_code) %>%
  select(date,
         Land,
         #total_deaths,
         #new_deaths,
         people_fully_vaccinated_per_hundred,
         total_deaths_per_million,
         #new_vaccinations,
         total_vaccinations) %>%
  filter(date == "2021-09-23") %>%
  group_by(Land)


# d2 %>%
#   ungroup() %>%
#   count(people_fully_vaccinated_per_hundred)

plot_covid1 <-
  d2 %>%
  ggplot(aes(x = Land,
             y = people_fully_vaccinated_per_hundred)) +
  geom_col() +
  labs(title = "Anteil komplett geimpfter Personen",
       subtitle = "2021-09-23")




plot_covid2 <-
  d2 %>%
  ggplot(aes(x = Land,
             y = total_deaths_per_million)) +
  geom_col()+
  labs(title = "Corona-Tote pro Million",
       subtitle = "2021-09-23")


plots(plot_covid1, plot_covid2)
```

Ja, da in beiden Diagrammen gilt: $P(A|B) \ne Pr(A) \ne Pr(A|\neg B)$.


Daten von [Our World in Data](https://ourworldindata.org/covid-deaths).


:::








### Multiplikationssatz

Der Multiplikationssatz wird verwendet, wenn wir an der Wahrscheinlichkeit interessiert sind, dass *alle Ereignisse* eintreten.


#### Unabhängige Ereignisse


Wir werfen eine faire Münze *zwei* Mal (Abb. @fig-2muenzen).


![Wir werfen 2 faire Münzen](img/muenz1.png){#fig-2muenzen}

Abb. @fig-2muenzen zeigt ein *Baumdiagramm*. Jeder *Kasten* (Knoten) zeigt ein *Ergebnis.* 
Die Pfeile (Kanten) symbolisieren die Abfolge des Experiments: Vom "Start" (schwarzer Kreis) 
führen zwei mögliche Ergebniss ab, jeweils mit Wahrscheinlichkeit 1/2.
Die untersten Knoten nennt man auch *Blätter* (Endknoten), sie zeigen das Endresultat des (in diesem Fall) zweifachen Münzwurfs.
Der Weg vom Start zu einem bestimmten Blatt nennt man *Pfad*. 
Die Anzahl der Pfade entspricht der Anzahl der Blätter.
In diesen Diagramm gibt es vier Pfade (und Blätter).





```{r}
#| echo: false
tibble::tribble(
  ~Ereignis,               ~Pr,
       "0K", "1/2 * 1/2 = 1/4",
       "1K", "1/4 + 1/4 = 1/2",
       "2K", "1/2 * 1/2 = 1/4"
  )
```

Wir werfen eine faire Münze *drei* Mal (Abb. @fig-3muenzen)

![Wir werfen drei faire Münzen](img/muenz2.png){#fig-3muenzen}



```{r QM2-Thema1-WasistInferenz-27}
#| echo: false
tibble::tribble(
  ~Ereignis,                     ~Pr,
       "0K", "1/2 * 1/2 * 1/2 = 1/8",
       "1K", "1/8 + 1/8 + 1/8 = 3/8",
       "2K",          "3 * 1/8 = 3/8",
       "3K", "1/2 * 1/2 * 1/2 = 1/8"
  ) 
```




$Pr(AB) = Pr(A) \cdot Pr(B) = 50\% \cdot 50\% = 25\%$

```{r plot-unabh-ereignisse}
#| echo: false
#| fig-cap: Unabhängige Ereignisse visualisiert
#| label: fig-unabh-e
plots(plot16a1, plot16a2, plot16c)
```


Abb. @fig-unabh-e zeigt, dass gilt: $P(A\cap B) = P(A) \cdot P(B) = P(B) \cdot P(A)$.


#### Kalt und Regen

Von @mcelreath_statistical_2020 stammt diese Verdeutlichung der gmeinsamen Wahrscheinlichkeit:

Was ist die Wahrscheinlichkeit für *kalt ❄ und Regen ⛈️*?

Die Wahrscheinlichkeit für kalt und Regen ist die Wahrscheinlichkeit von *Regen* ⛈, wenn's *kalt* ❄ ist mal die Wahrscheinlichkeit von *Kälte* ❄.

Ebenfalls gilt:

Die Wahrscheinlichkeit für kalt und Regen ist die Wahrscheinlichkeit von *Kälte* ❄, wenn's *regnet* ⛈️ mal die Wahrscheinlichkeit von *Regen* ⛈️.

Das Gesagte als Emoji-Gleichung:

$P(❄️ und ⛈️) = P(⛈️ |❄️ ) \cdot P(❄️) =  P(❄️ |⛈️ ) \cdot P(⛈️) = P(⛈️ und  ❄️)$



Allgemein:

$P(A\cap B) = P(A) \cdot P(B|A) = P(B) \cdot P(A|B)$


Man kann also die "Gleichung drehen".

#### Abhängige Ereignisse



Ein Baumdiagramm bietet sich zur Visualisierung abhängiger Ereignisse an, s. Abb. @fig-baum-abh. Für unabhängige Ereignisse übrigens auch.


In einer Urne befinden sich fünf Kugeln, von denen vier rot sind und eine blau ist.

wie groß ist die Wahrscheinlichkeit, dass bei zwei Ziehungen ohne Zurücklegen (*ZOZ*) *zwei rote Kugeln* gezogen werden [@bourier_2018], S. 47.


Hier ist unsere Urne:

$$\boxed{\color{red}{R, R, R, R}, \color{blue}B}$$

Und jetzt ziehen wir. Hier ist das Baumdiagramm, s. Abb. @fig-baum-abh.


```{mermaid}
%%| fig-cap: "Baumdiagramm für ein ein zweistufiges Zufallsereignis, wobei der 2. Zug (Stufe) abhängig ist vom 1. Zug."
%%| label: fig-baum-abh
flowchart LR
  A[Start] -->|4/5|B[1. Zug: R]
  A -->|1/5|C[1. Zug: B]
  B -->|3/4|D[2. Zug: R]
  B -->|1/4|E[2. Zug: B]
  D --- H[Fazit: RR:  12/20]
  E --- I[Fazit: RB: 4/20]
  C -->|4/4|F[2. Zug: R]
  C -->|0/4|G[2. Zug: B]
  F --- J[Fazit: BR: 4/20]
  G --- K[Fazit: BB: 0/20]
```



Es gilt also: $P(A\cap B) = P(A) \cdot P(B|A)$.



### Totale Wahrscheinlichkeit




@fig-tot-wskt zeigt das Baumdiagramm für die Aufgabe @bourier_2018, S. 56.

```{mermaid}
%%| fig-cap: Totale Wahrscheinlichkeit
%%| label: fig-tot-wskt
flowchart LR
  A[Start] -->|0.6|B[A1]
  A -->|0.1|C[A2]
  A -->|0.3|D[A3]
  B -->|0.05|E[B]
  B -.->|0.95|F[Nicht-B]
  C -->|0.02|G[B]
  C -.->|0.98|H[Nicht-B]
  D -->|0.04|I[B]
  D -.->|0.96|J[Nicht-B]
```



Gesucht ist die Wahrscheinlichkeit $P(B)$.

Dazu addieren wir die Warhscheinlichkeiten der relevanten Äste.

```{r}
W_total <- 0.6 * 0.05 + 0.1 * 0.02 + 0.3 * 0.04
W_total
```

Die totale Wahrscheinlichkeit beträgt also $P(B) = 4.4\%$.

Einfacher noch ist es, wenn man anstelle von Wahrscheinlichkeiten absolute Häufigkeiten verwendet.


### Bayes

#### Bayes als Baum

Gesucht sei $P(A_1|B)$.

Für Bayes' Formel setzt man die Wahrscheinlichkeit des  *günstigen* Ast zur Wahrscheinlichkeit aller relevanten Äste, $P(B)$.

Der günstige Ast ist hier schwarz gedruckt, die übrigen Äste gestrichelt, s. @fig-tot-wskt2.

```{mermaid}
%%| fig-cap: Günstige Pfade
%%| label: fig-tot-wskt2
flowchart LR
  A[Start] -->|0.6|B[A1]
  A -.->|0.1|C[A2]
  A -.->|0.3|D[A3]
  B --->|0.05|E[B]
  B -.->|0.95|F[Nicht-B]
  C -.->|0.02|G[B]
  C -.->|0.98|H[Nicht-B]
  D -.->|0.04|I[B]
  D -.->|0.96|J[Nicht-B]
```



$$P(A|B) = \frac{P(A1 \cap B)}{P(B)} = \frac{0.6 \cdot 0.05}{0.03 + 0.002 + 0.012} = \frac{0.03}{0.044} \approx 0.68$$

$P(A|B)$ beträgt also ca. 68%.

Zur Erinnerung: $P(B)$ ist die totale Wahrscheinlichkeit.


## Bayes' Theorem

### Bayes als bedingte Wahrscheinlichkeit


Bayes' Theorem ist auch nur eine normale bedingte Wahrscheinlichkeit:


$P(A|B) = \frac{\overbrace{ P(A\cap B)}^\text{umformen}}{P(B)}$

$P(A\cap B)$ kann man  umformen, s. @eq-bayes1:

$$P(A|B) =\frac{P(A\cap B)}{P(B)} = \frac{P(B|A) \cdot P(A)}{P(B)}$${#eq-bayes1}

Man kann sich Bayes' Theorem  auch wie folgt herleiten:



$P(A\cap B) = P(B \cap A) = P(A) \cdot P(B|A) = P(B) \cdot P(A|B)$

Dann lösen wir nach P$(A|B)$ auf:


$P(A|B) = \frac{P(A) \cdot P(B|A)}{P(B)}$


### Wozu wird Bayes in der Praxis genutzt?


In der Praxis nutzt man Bayes häufig, wenn man Daten zu einer Wirkung $W$ hat,
und auf die Ursache $U$ zurückschließen möchte, sinngemäß:

$W \quad \underrightarrow{Bayes} \quad U$.

Dann kann man @eq-bayes1 so schreiben, s. @eq-bayes2:

$$P(U|W) = \frac{ P(U) \cdot P(W|U) }{P(W)}$${#eq-bayes2}

Eine ähnliche Situation, die in der Praxis häufig ist,
dass man Daten $D$ hat und auf die Wahrscheinlichkeit einer Hypothese $H$ schließen möchte, s. @eq-bayes3.

$D \quad \underrightarrow{Bayes} \quad H$.


$$P(H|D) = \frac{ P(H) \cdot P(D|H) }{P(D)}$${#eq-bayes3}

@eq-bayes3 fragt nach $P(H|D)$:

>    Was ist die Wahrscheinlichkeit der Hypothese H, jetzt wo wir die Daten haben (und ein Modell?)

Und antwortet so (@eq-bayes3):

>    Diese Wahrscheinlichkeit entspricht der Grundrate (Apriori-Wahrscheinlichkeit) der Hypothese mal der Plausibilität (Likelihood) der Daten unter Annahme (gegeben) der Hypothese. Aus Standardisierungsgründen dividiert man noch die totale Wahrscheinlichkeit der Daten über alle Hypothesen.


### Zusammengesetzte Hypothesen

Das ist vielleicht ein bisschen fancy,
aber man kann Bayes' Theorem auch nutzen, um die Wahrscheinlichkeit einer *zusammengesetzten Hypothese* zu berechnen: $H = H_1 \cap H_2$. 
Ein Beispiel wäre: "Was ist die Wahrscheinlichkeit, dass es Regen ($R$) *und* Blitzeis ($B$) gibt, wenn es kalt ($K$) ist?".

Das sieht dann so aus, @eq-bayes4:

$$
\begin{aligned}
P(R \cap B |K) &= \frac{ P(R \cap B) \cdot P(K|R \cap B) }{P(D)} \\
&= \frac{ P(R ) \cdot P(B) \cdot P(K|R \cap B) }{P(D)}
\end{aligned}
$${#eq-bayes4}


Hier haben wir $P(R \cap B)$  aufgelöst in $P(R) \cdot P(B)$,
das ist nur zulässig, wenn $R$ und $B$ unabhängig sind.

#### Bayes-Video von 3b1b

Das [Video zu Bayes von 3b1b](https://youtu.be/HZGCoVF3YvM) verdeutlicht das Vorgehen der Bayes-Methode auf einfache und anschauliche Weise.






## Aufgaben


Zusätzlich zu den Aufgaben im Buch:

- [mtcars-abhaengig](https://datenwerk.netlify.app/posts/mtcars-abhaengig/mtcars-abhaengig.html)
- [voll-normal](https://datenwerk.netlify.app/posts/voll-normal/voll-normal.html)
- [corona-blutgruppe](https://datenwerk.netlify.app/posts/corona-blutgruppe/corona-blutgruppe.html)
- [Bed-Wskt2](https://datenwerk.netlify.app/posts/bed-wskt2/bed-wskt2)
- [Gem-Wskt1](https://datenwerk.netlify.app/posts/gem-wskt1/gem-wskt1)
- [wuerfel01](https://datenwerk.netlify.app/posts/wuerfel01/wuerfel01.html)
- [wuerfel02](https://datenwerk.netlify.app/posts/wuerfel02/wuerfel02.html)
- [wuerfel03](https://datenwerk.netlify.app/posts/wuerfel03/wuerfel03.html)
- [wuerfel04](https://datenwerk.netlify.app/posts/wuerfel04/wuerfel04.html)





## ---



![](img/outro-03.jpg){width=100%}






