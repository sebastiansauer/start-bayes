
# Wahrscheinlichkeit

```{r}
#| include: false
library(tidyverse)
library(easystats)
library(lubridate)
library(gmp)
library(titanic)
```




```{r r-setup}
#| echo: false
#| message: false
theme_set(theme_minimal())
#scale_color_okabeito()
scale_colour_discrete <- function(...) 
  scale_color_okabeito()
```


```{r define-plots-16}
#| echo: false
#| fig-width: 7

plot16a <-
  ggplot(data.frame(A = c(0, 1),
                    B = c(0, 1))) +
  aes(x = A, y = B) +
  annotate("rect", xmin = 0, xmax = 1, ymin = 0, ymax = .5,
           color = "yellow", alpha = .7, fill = NA) +
  annotate("rect", xmin = 0, xmax = 1, ymin = 0.5, ymax = 1,
           color = NA, alpha = .5, fill = "yellow") +
  annotate("rect", xmin = 0, xmax =0.5, ymin = 0, ymax = 1,
           color = "blue", alpha = .7, fill = NA) +
  annotate("rect", xmin = 0.5, xmax = 1, ymin = 0, ymax = 1,
           color = "blue", alpha = .5, fill = "blue") +
  scale_x_continuous(breaks = c(0.25, 0.75), labels = c("¬A", "A")) +
  scale_y_continuous(breaks = c(0.25, 0.75), labels = c("¬B", "B"))


plot16a1 <-
  ggplot(data.frame(A = c(0, 1),
                    B = c(0, 1))) +
  aes(x = A, y = B) +
  annotate("rect", xmin = 0, xmax = 1, ymin = 0, ymax = .5,
           color = "yellow", alpha = .7, fill = NA) +
  annotate("rect", xmin = 0, xmax = 1, ymin = 0.5, ymax = 1,
           color = NA, alpha = .5, fill = "yellow") +
  scale_x_continuous(breaks = c(0.25, 0.75), labels = c("¬A", "A")) +
  scale_y_continuous(breaks = c(0.25, 0.75), labels = c("¬B", "B")) +
  annotate("label", x = .5, y = .75, label = "Pr(B) = 50%")

plot16a2 <-
  ggplot(data.frame(A = c(0, 1),
                    B = c(0, 1))) +
  aes(x = A, y = B) +
  annotate("rect", xmin = 0, xmax =0.5, ymin = 0, ymax = 1,
           color = "blue", alpha = .7, fill = NA) +
  annotate("rect", xmin = 0.5, xmax = 1, ymin = 0, ymax = 1,
           color = "blue", alpha = .5, fill = "blue") +
  scale_x_continuous(breaks = c(0.25, 0.75), labels = c("¬A", "A")) +
  scale_y_continuous(breaks = c(0.25, 0.75), labels = c("¬B", "B")) +
  annotate("label", x = .75, y = .5, label = "Pr(A) = 50%")


plot16b <-
ggplot(data.frame(A = c(0, 1),
                  B = c(0, 1))) +
  aes(x = A, y = B) +
  annotate("rect", xmin = 0, xmax = 1, ymin = 0, ymax = .5,
           color = "yellow", alpha = .7, fill = NA) +
  annotate("rect", xmin = 0, xmax = 1, ymin = 0.5, ymax = 1,
           color = NA, alpha = .5, fill = "yellow") +
  annotate("rect", xmin = 0, xmax =0.5, ymin = 0, ymax = 1,
           color = "blue", alpha = .7, fill = NA) +
  annotate("rect", xmin = 0.5, xmax = 1, ymin = 0, ymax = 1,
           color = "blue", alpha = .5, fill = "blue") +
  annotate("rect", xmin = 0.5, xmax = 1, ymin = 0.5, ymax = 1,
           color = "green", alpha = .7, fill = NA) +
  scale_x_continuous(breaks = c(0.25, 0.75), labels = c("¬A", "A")) +
  scale_y_continuous(breaks = c(0.25, 0.75), labels = c("¬B", "B")) +
  annotate(geom = "label", x = .75, y = 0.75, label = "A,B")





plot16c <-
  ggplot(data.frame(A = c(0, 1),
                    B = c(0, 1))) +
  aes(x = A, y = B) +
  annotate("rect", xmin = 0, xmax = 1, ymin = 0, ymax = .5,
           color = "yellow", alpha = .7, fill = NA) +
  annotate("rect", xmin = 0, xmax = 1, ymin = 0.5, ymax = 1,
           color = NA, alpha = .5, fill = "NA") +
  annotate("rect", xmin = 0, xmax =0.5, ymin = 0, ymax = 1,
           color = "blue", alpha = .7, fill = NA) +
  annotate("rect", xmin = 0.5, xmax = 1, ymin = 0, ymax = 1,
           color = "blue", alpha = .5, fill = "blue", linewidth = 2) +
  annotate("rect", xmin = 0.5, xmax = 1, ymin = 0.5, ymax = 1,
           color = "green", alpha = .3, fill = "green") +
  scale_x_continuous(breaks = c(0.25, 0.75), labels = c("¬A", "A")) +
  scale_y_continuous(breaks = c(0.25, 0.75), labels = c("¬B", "B")) +
  annotate(geom = "label", x = .75, y = 0.75, label = "B|A")


```

## Lernsteuerung


### Lernziele

Nach Absolvieren des jeweiligen Kapitels sollen folgende Lernziele erreicht sein.

Sie können ...


- die Grundbegriffe der Wahrscheinlichkeitsrechnung erläuternd definieren
- die drei Arten der direkten Ermittlung von Wahrscheinlichkeit erläutern
- typische Relationen (Operationen) von Ereignissen anhand von Beispielen veranschaulichen
- mit Wahrscheinlichkeiten rechnen



### Eigenstudium


:::{.callout-note}
Dieses Kapitel ist selbständig im Eigenstudium vorzubereiten vor dem Unterricht.
Lesen Sie dazu die angegebene Literatur.$\square$
:::


### Prüfungsrelevanter Stoff

Lesen Sie dazu @bourier_2018, Kap. 2-4. 
Weitere Übungsaufgaben finden Sie im dazugehörigen Übungsbuch, @bourier_statistik-ubungen_2022.







### Zentrale Begriffe


#### Grundbegriffe

- Zufallsvorgang (Zufallsexperiment)
- Elementarereignis
- Ereignisraum
- Zufallsereignis (zufälliges Ereignis)
- Sicheres Ereignis
- Unmögliches Ereignis


#### Wahrscheinlichkeitsbegriffe

- Klassische Wahrscheinlichkeit (LaPlace'sche Wahrscheinlichkeit)
- Statistische (empirische) Wahrscheinlichkeitsermittlung
- Subjektive (Bayes) Wahrscheinlichkeitsermittlung


#### Wahrscheinlichkeitsrelationen

- Vereinigung von Ereignissen
- Schnitt(menge) von Ereignissen
- Komplementärereignis
- Vollständiges Ereignissystem
- Kolmogorovs Definition von Wahrscheinlichkeit


#### Wahrscheinlichkeitsrechnung

- Allgemeiner Additionsssatz
- Disjunkte Ereignisse
- Additionssatz für disjunkte Ereignisse
- Bedingte Wahrscheinlichkeit
- (Stochastische) Unabhängigkeit
- Baumdiagramm für gemeinsame Wahrscheinlichkeit
- Allgemeiner Multiplikationssatz
- Multiplikationssatz für unabhängige Ereignisse
- Totale Wahrscheinlichkeit
- Satz von Bayes


### Begleitvideos


- [Video zum Thema Wahrscheinlichkeit](https://youtu.be/rR6NspapEyo)




<!-- ## Unterstützung: Wahrscheinlichkeit in Bildern -->

<!-- Wahrscheinlichkeit in Bildern: zur einfachen Erschließung des Materials, -->
<!-- ein Unterstützungsangebot. -->


<!-- Im Folgenden sind einige Schlüsselbegriffe und -regeln in (ver-)einfach(t)er Form schematisch bzw. visuell dargestellt mit dem Ziel, den Stoff einfacher zu erschließen. -->


## Grundbegriffe





:::{#exm-muenz}
Klassisches Beispiel für einen Zufallsvorgang ist das (einmalige oder mehrmalige) Werfen einer Münze.$\square$
:::

::{#exr-muenze}
Werfen Sie eine Münze!
Diese hier zum Beispiel:

![](img/1024px-Coin-155597.svg.png){width=10% fig-align="center"}

[Quelle: By OpenClipartVectors, CC0]( https://pixabay.com/pt/moeda-euro-europa-fran%C3%A7a-dinheiro-155597)

Wiederholen Sie den Versuch 10, nein, 100, nein 1000, nein: $10^6$ Mal.

Notieren Sie das Ergebnis!

Oder probieren Sie die [App der Brown University](https://seeing-theory.brown.edu/basic-probability/index.html#section1).


:::{#def-zufall}
### Zufallsvorgang
Ein Zufallsvorgang oder Zufallsexperiment ist eine einigermaßen klar beschriebene Tätigkeit, deren Ergebnis nicht bekannt ist. Allerdings ist die Menge möglicher Ergebnisse sicher und die Wahrscheinlichkeit für die Ergebnisse kann quantifiziert werden.$\square$
:::


:::{#exr-zufall2}
Nennen Sie Beispiele für Zufallsvorgänge!\square^[Beispiele für Zufallsexperimente sind das Werfen einer Münze, das Ziehen einer Karte aus einem Kartenspiel, das Messen eines Umweltphänomens wie der Temperatur oder die Anzahl der Kunden, die einen Laden betreten. In jedem dieser Fälle sind die möglichen Ergebnisse nicht im Voraus bekannt und hängen von nicht komplett bekannten Faktoren ab.]
:::


:::callout-caution
Zufall heißt nicht, dass ein Vorgang keine Ursachen hätte. So gehorcht der Fall einer Münze komplett den Gesetzen der Gravitation. Würden wir diese Gesetze und die Ausgangsbedingungen (Luftdruck, Fallhöhe, Oberflächenbeschaffenheit, Gewichtsverteilungen, ...) exakt kennen, könnten wir theoretisch sehr genaue Vorhersagen machen. Der "Zufall" würde aus dem Münzwurf verschwinden. Man sollte "Zufall" also besser verstehen als "unbekannt".$\square$
::::




:::{#def-ereignisraum}
### Grundraum
Die möglichen Ergebnisse eines Zufallvorgangs fasst man als Menge mit dem Namen *Ereignisraum*[synonym: *Ereignisraum* oder *Grundraum*] zusammen. Man verwendet den griechischen Buchstaben $\Omega$ für diese Menge.
Die Elemente $\omega$ (kleines Omega) von $\Omega$ nennt man *Ergebnisse*.$\square$
:::

:::{#exm-grundraum}
Beobachtet man beim Würfelwurf die oben liegende Augenzahl, so ist 

$$\Omega = \{ 1,2,3,4,5,6 \}$$

ein natürlicher Grundraum [@henze_stochastik_2019].$\square$
:::

:::{#def-ereignis}
### Ereignis
Jede Teilmenge^[$A$ ist eine Teilmenge von $B$, wenn alle Elemente von $A$ auch Teil von $B$ sind.] von $\Omega$ heißt *Ereignis*; $A \subseteq B$ .$\square$
:::



:::{#exm-ereignis}
Beim Mensch-ärger-dich-nicht Spielen habe ich eine 6 geworfen.^[Schon wieder.]
Das Nennen wir das Ereignis $A$: "Augenzahl 6 liegt oben"-$\square$

$A= \{6\}$
:::


:::{#def-unm-sich}
### Unmögliches und sicheres Ereignis
Die leere Menge $\varnothing$ heißt das *umögliche*, der Grundraum $\Omega$ heißt das *sichere Ereignis*. $\square$
:::

:::{#exm-unm}
### Unmögliches Ereignis
Alois behauptet, er habe mit seinem Würfel eine 7 geworfen.
Schorsch ergänt, sein Würfel liege auf einer Ecke, so dass keine Augenzahl oben liegt.
Draco hat seinen Würfel runtergeschluckt.$\square$
:::


:::{#exm-sicher}
Nach dem der Würfel geworfen wurde, liegt eine Augenzahl zwischen 1 und 6 oben.$\square$
:::


:::{#def-elementarereignis}
### Elementarereignis

Jede einelementige Teilmenge $\{\omega\}$ von $\Omega$ heißt *Elementarereignis* (häufig mit $A$ bezeichnet).$\square$
:::



## Direkte Ermittlung von Wahrscheinlichkeiten

### Epistemologische Wahrscheinlichkeit


Vor uns liegt ein Würfel. Schlicht, ruhig, unbesonders.
Wir haben keinen Grund anzunehmen, dass eine seiner $n=6$ Seiten bevorzugt nach oben zu liegen kommt. 
Jedes der sechs Elementarereignisse ist uns gleich plausibel;
der Würfel erscheint uns fair.
In Ermangelung weiteres Wissens zu unserem Würfel gehen wir schlicht davon aus, dass jedes der $n$ Elementarereignis gleich wahrscheinlich ist.
Es gibt keinerlei Notwendigkeit, den Würfel in die Hand zu nehmen,
um zu einer Wahrscheinlichkeitsaussage auf diesem Weg zu kommen.
Natürlich *könnten* wir unsere Auffassung eines fairen Würfels testen,
aber auch ohne das Testen können wir eine stringente Aussage (basierend auf unserer Annahme der Indifferenz der $n$ Elementarereignisse) zur Wahrscheinlichkeit eines bestimmten Ereignisses $A$ kommen [@briggs_uncertainty:_2016].


$$Pr(A) = 1/n$$

:::{#exm-briggs}
Sei $A$ = "Der Würfel wird beim nächsten Wurf eine 6 zeigen."
Die Wahrscheinlichkeit für $A$ ist $1/6$.$square$
:::



### Frequentistische Wahrscheinlichkeit

In Ermangelung einer Theorie zum Verhalten eines (uns) unbekannten Zufallsvorgangs und unter der Vermutung, dass die Elementarereignisse nicht gleichwahrscheinlich sind, bleibt uns ein einfacher (aber aufwändiger) Ausweg: Ausprobieren.

Angenommen, ein Statistik-Dozent, bekannt für seine Vorliebe zum Glücksspiel und scheinbar endlosen Glückssträhnen, er wirft andauernd eine 6, hat seinen Lieblingswürfel versehentlich liegen gelassen. Das ist *die* Gelegenheit!
Sie greifen sich den Würfel, und ... Ja, was jetzt?
Nach kurzer Überlegung kommen Sie zum Entschluss, den Würfel einen "Praxistest" zu unterziehen: Sie werfen ihn 1000 Mal (Puh!) und zählen den Anteil der `6`.
Falls der Würfel fair ist, müsste gelten $Pr(A=6)=1/6\approx .17$. Schauen wir mal!



```{r}
#| echo: false
n <- 1e3

set.seed(42)
wuerfel_oft <- 
  sample(x = 1:6, size = n, replace = TRUE) 


wuerfel_tab <-
  tibble(
    id = 1:n,
    x = wuerfel_oft,
    ist_6 = ifelse(x == 6, 1, 0),
    ist_6_cumsum = cumsum(ist_6) / id
  )

```


Und hier der Anteil von  `6` im Verlauf unserer Würfe, s. @fig-wuerfel.


```{r}
#| label: fig-wuerfel
#| fig-cap: "Das Gesetz der großen Zahl am Beispiel der Stabilisierung des Trefferanteils beim wiederholten Würfelwurf"
#| fig-asp: 0.5

wuerfel_tab %>% 
  slice_head(n = 1e3) %>% 
  ggplot() +
  aes(x = id, y = ist_6_cumsum) +
  geom_hline(yintercept = 1/6, color = "grey80", size = 3) +
  geom_line()
```

Hm, leider ist auf den ersten Blick kein Anzeichen für Schummeln bzw. einen gezinkten Würfel zu finden (zumindest nicht zu Gunsten des Zwielichten Dozenten).



### Subjektive Wahrscheinlichkeit

Um subjektiv zu einer Wahrscheinlichkeit zu kommen, sagt man einfach seine Meinung.
Das hört sich natürlich total plump an. 
Und tatsächlich besteht die Gefahr, dass die so ermittelten Wahrscheinlichkeiten aus der Luft gegriffen, also haltlos, sind.

Allerdings kann diese Art von Wahrscheinlichkeitsermittlung auch sehr wertvoll sein.
In komplizierten Situation im echten Leben kommt man oft in die Situation, dass weder die epistemologische noch die frequentistische Variante verwendet werden kann.
Dann muss man auf Schätzungen, Vorwissen, Erfahrung, theoretischen Überlengungen etc. zurückgreifen.

## Indirekte Ermittlung von Wahrscheinlichkeiten

Die indirekte Ermittlung von Wahrscheinlichkeiten meint das Ableiten von Wahrscheinlichkeitsaussagen, wenn man schon etwas über die Wahrscheinlichkeiten des Grundraums weiß. 
Dazu greift man auf Rechenregeln der Stochastik zurück.
Das hört sich vielleicht wild an, ist aber oft ganz einfach.

:::{exm-ind}
### Gezinkter Würfel
Ein gezinkter Würfel hat eine erhöhte Wahrscheinlichkeit für das Ereignis $A=$"6 liegt oben", und zwar gelte $Pr(A)=1/3$.
Was ist die Wahrscheinlichkeit, *keine*  `6` zu würfeln?$\square$^[Die Wahrscheinlichkeit, keine `6` zu würfeln, liegt bei $2/3$.]
:::

Für das Rechnen mit Wahrscheinlichkeiten ist es hilfreich, ein paar Werkzeuge zu kennen, die wir uns im Folgenden anschauen.


## Relationen von Mengen





Wir gehen von Grundraum $\Omega$ aus, mit dem Ereignis $A$ als Teilmenge: $A \subset B$.


Da wir Ereignisse als Mengen auffassen, verwenden wir im Folgenden die beiden Begriffe synonym.


Dabei nutzen wir u.a. Venn-Diagramme.
Venn-Diagramme eigenen sich, um typische Operationen (Relationen) auf Mengen zu visualisieren. Die folgenden Venn-Diagramme stammen von [Wikipedia (En)](https://en.wikipedia.org/wiki/Venn_diagram).

:::callout-note
### Wozu sind die Venn-Diagramme gut? Warum soll ich die lernen?
Venn-Diagramme zeigen Kreise und ihre überlappenden Teile;
daraus lassen sich Rückschlüsse auf Rechenregeln für Wahrscheinlichkeiten ableiten.
Viele Menschen tun sich leichter, Rechenregeln visuell aufzufassen als mit Formeln und Zahlen alleine. Aber entscheiden Sie selbst!$\square$
:::


### Vereinigung von Ereignissen

:::{#def-mengen-verein}
### Vereinigung von Ereignissen
Vereinigt man zwei Ereignisse $A$ und $B$, dann besteht das neue Ereignis $C$ genau aus den Elementarereignissen der vereinigten Ereignisse.
Man schreibt $C = A \cup B$, lies: "C ist A vereinigt mit B".$\square$
:::

@fig-cup zeigt ein Venn-Diagramm zur Verdeutlichung der Vereinigung von Ereignissen.

![$A \cup B$](img/Venn0111.svg.png){@fig-cup width=25%}

:::{#exm-mengen-verein}

Um einen (hohen!) Geldpreis zu gewinnen, muss bei ihrem nächsten Wurf mindestens eines der beiden Ereignisse $A$ oder $B$ eintreten.

\begin{aligned}
A = \{1,2\} \qquad \boxed{\boxed{1\; 2}\; \color{gray}{ 3\; 4\; 5\; 6}} \\
B = \{2,3\} \qquad  \boxed{1\; \boxed{2\; 3}\; \color{gray}{ 4\; 5\; 6}} \\
\newline
\hline \\
A \cup B = \{1,2,3\} \qquad \boxed{\boxed{1\; 2\; 3}\; \color{gray}{4\; 5\; 6}}
\end{aligned}
:::



Zur besseren Verbildlichung betrachten Sie mal diese
[Animation zur Vereinigung von Mengen](https://www.geogebra.org/m/GEZV4xXc#material/cmXR8fHN); [Quelle](Geogebra, J. Merschhemke).


### (Durch-)Schnitt von Ereignissen


:::{#def-mengen-schnitt}
### Schnittmenge von Ereignissen
Die Schnittmenge zweier Ereignisse $A$ und $B$ umfasst genau die Elementarereignisse, die Teil beider Ereignisse sind.$\square$
:::

@fig-cap zeigt ein Sinnbild zur Schnittmenge zweier Ereignisse.



![$A \cap B$](img/Venn0001.svg.png){@fig-cap width=25%}


:::{#exm-mengen-schnitt}

Um einen (hohen!) Geldpreis zu gewinnen, muss bei ihrem nächsten Wurf sowohl das Ereignis $A$ = "gerade Augenzahl" als auch $B$ = "Augenzahl größer 4".


\begin{align}
& A = \{2,4,6\} \qquad \hfill \boxed{\color{gray}{1}\; \boxed{\color{black}{2}}\; \color{gray}{3}\; \boxed{\color{black}{4}}\; \color{gray}{5}\; \boxed{\color{black}{6}}\;} \\
& B = \{5,6\} \qquad \qquad \hfill  \boxed{ \color{gray}{1\; 2\; 3\; 4\;} \boxed{\color{black}{5\; 6}}} \\
\newline
\hline \\
& A \cap B = \{6\} \qquad \qquad \hfill  \boxed{\color{gray}{1\; 2\; 3\; 4\; 5\;} \color{black}{6}}
\end{align}
:::



:::callout-note
#### Eselsbrücke zur Vereinigungs- und Schnittmenge

Das Zeichen für eine Vereinigung zweier Mengen kann man leicht mit dem Zeichen für einen Schnitt zweier Mengen leicht verwechseln; daher kommt eine Eselbrücke gelesen, s. @fig-esel.

![Eselsbrücke für Vereinigungs- und Schnittmenge](img/ven_cup_cap.jpeg){#fig-esel width=55%}
:::


### Komplementärereignis


:::{#def-menge-komplement}
### Komplementärereignis
Ein Ereignis $A$ ist genau dann ein Komplementärereignis zu $B$, wenn es genau die Elementarereignisse von $\Omega$ umfasst, die nicht Elementarereignis des anderen Ereignisses sind, s. @fig-neg.$\square$
:::


Man schreibt für das Komplementärereignis von $A$ oft $\bar{A}$ oder $\neg A$; lies "Nicht-A$ oder "A-quer".


:::{#exm-mengen-komplement}

Beim normalen Würfelwurf sei $A$ das Ereignis "gerade Augenzahl"; 
das Komplementärereignis ist dann $\neg A$ "ungerade Augenzahl".


\begin{align}
A = \{2,4,6\} \qquad \hfill \boxed{\color{gray}{1}\; \boxed{\color{black}{2}}\; \color{gray}{3}\; \boxed{\color{black}{4}}\; \color{gray}{5}\; \boxed{\color{black}{6}}\;} \\
\hline \\
\neg B = \{1,3,5\} \qquad  \hfill \boxed{\boxed{\color{black}{1}}\; \color{gray}{2}\; \boxed{\color{black}{3}}\; \color{gray}{4}\; \boxed{\color{black}{5}}\; \color{gray}{6}\; } \\
\end{align}
:::


![$\bar{A}$](img/2560px-Venn1010.svg.png){#fig-neg width=25%}


### Logische Differenz

:::{#def-mengen-diff}
### Logische Differenz
Die logische Differenz der Ereignisse $A$ und $B$ ist das Ereignis, 
das genau aus den Elementarereignissen besteht von $A$ besteht, die nicht zugleich Elementarereignis von $B$ sind, s. @fig-setminus.$\square$
:::

Die logische Differenz von $A$ zu $B$ schreibt man häufig so: $A \setminus B$; lies "A minus B".


![$A \setminus B$](img/Venn0100.svg.png){#fig-setminus width=25%}

:::{#exm-mengen-setminus}

Sei $A$ die Menge "große Zahlen" mit $A = \{4,5,6 \}$.
Sei $B$ die Menge "gerade Zahlen".
Wir suchen die logische Differenz, $A \setminus B$.


\begin{align}
A = \{4,5, 6\} \qquad \hfill \boxed{4\; 5\; 6} \\
B = \{2,4,6\} \qquad  \hfill \boxed{2\; 4\; 6} \\
\hline \\
A \setminus B \qquad \hfill \boxed{5}
\end{align}
:::


In R gibt es die Funktion `setdiff()`, die eine Mengendifferenz ausgibt.

```{r}
A <- c(4, 5, 6)
B <- c(2, 4, 6)

setdiff(A, B)
```
:::callout-caution
$A \setminus B \ne B \setminus A$.
:::


```{r}
setdiff(B, A)
```


### Disjunkte Ereignisse




Seien $A= \{1,2,3\}; B= \{4,5,6\}$.

$A$ und $B$ sind disjunkt^[engl. disjoint]: ihre Schnittmenge ist leer: $A \cap B = \emptyset$,
s. @fig-disjunkt




![Zwei disjunkte Ereignisse, dargestellt noch überlappungsfreie Kreise](img/2880px-Disjunkte_Mengen.svg.png){#fig-disjunkt width="25%" fig-align="center"}





[Quelle: rither.de](http://www.rither.de/a/mathematik/stochastik/mengentheorie-und-venn-diagramme/)


### Vertiefung


[Animation zu Mengenoperationen](https://seeing-theory.brown.edu/compound-probability/index.html)


## Eigenschaften von Wahrscheinlichkeiten

Wir richten eine Reihe von Forderungen an das Rechnen mit Wahrscheinlichkeiten, die direkt plausibel erscheinen:

1. *Nichtnegativität*: Die Wahrscheinlichkeit eines Ereignisses kann nicht negativ sein.
2. *Normierung*: Das sichere Ereignis hat die Wahrscheinlichkeit 1 bzw. 100%: $Pr(\Omega)=1$; das unmögliche Ereignis hat die Wahrscheinlichkeit 0: $Pr(\emptyset)=0$.
3. Sind $A$ und $B$ disjunkt, dann ist die Wahrscheinlichkeit von $A\cup B$ die Summe der beiden Einzelwahrscheinlichkeiten von $A$ und $B$.


## Rechnen mit Wahrscheinlichkeiten

### Additionssatz 

Der Additionssatz wird verwendet, wenn wir an der Wahrscheinlichkeit interessiert sind, dass *mindestens eines der Ereignisse* eintritt.

#### Disjunkte Ereignisse

$\Omega = {1,2,3,4,5,6}$



$\boxed{1\; 2\; 3\; 4\; 5\; 6}$

Gesucht sei die Wahrscheinlichkeit des Ereignis $A=\{1,2\}$.


$\boxed{\boxed{1\; 2}\; \color{gray}{ 3\; 4\; 5\; 6}}$

$P(1 \cup 2) = \frac{1}{6} + \frac{1}{6} = \frac{2}{6}$


#### Allgemein (disjunkt oder nicht disjunkt)


Bei der Addition der Wahrscheinlichkeiten für $A$ und $B$ wird der Schnitt $A\cap B$ doppelt erfasst. Er muss daher noch abgezogen werden (vgl. @fig-sets2):



$$P(A \cup B) = P(A) + P(B) - P(A\cap B)$$



::: {#fig-sets2 layout-ncol=2}

![$A \cup B$](img/Venn0111.svg.png){width=20%}




![$A \cap B$](img/Venn0001.svg.png){width=20%}



Die Schnittmenge muss beim Vereinigen abgezogen werden,
damit sie nicht doppelt gezählt wird.


:::



### Bedingte Wahrscheinlichkeit

:::{#def-pr-cond}
### Bedingte Wahrscheinlichkeit
Die Bedingte Wahrscheinlichkeit ist die Wahrscheinlichkeit, dass $A$ eintritt, gegeben dass $B$ schon eingetreten ist. $\square$

Man schreibt: $Pr(A|B).$ Lies: "A gegeben B".

Schauen Sie sich mal diese [Wahnsinnsanimation von Victor Powell an](https://setosa.io/conditional/) zu bedingten Wahrscheinlichkeiten. 
Hammer!


#### Schema


@fig-schema-p illustriert gemeinsame Wahrscheinlichkeit, $P(A \cap B)$ und bedingte Wahrscheinlichkeit, $P(A|B)$.


```{r bed-w-schema}
#| echo: false
#| fig-cap: Illustration von gemeinsamer und bedingter Wahrscheinlichkeit
#| label: fig-schema-p
plots(plot16a1, plot16a2, plot16c)
```


:::{#exm-bed-p}
### Bedingte Wahrscheinlichkeit
Sei $A$ "Schönes Wetter" und $B$ "Klausur steht an".
Dann meint $Pr(A|B)$ die Wahrscheinlichkeit, dass das Wetter schön ist, wenn gerade eine Klausur ansteht.$square$
:::


:::{#exm-papst}
### Von Päpsten und Männern

Man(n) beachte, dass die Wahrscheinlichkeit, Papst $P$ zu sein, wenn man Mann $M$ ist *etwas anderes* ist, als die Wahrscheinlichkeit, Mann zu sein, wenn man Papst ist:
$Pr(P|M) \ne Pr(M|P)$. Das hört sich erst verwirrend an, aber wenn man darüber nachdenkt, wird es sehr plausibel.$\square$
:::


:::{#exm-kalt-regen}
### Kalt und Regen
Die Wahrscheinlichkeit, dass es kalt ist, wenn es regnet, ist gleich der Wahrscheinlichkeit, dass es gleichzeitig kalt ist und regnet geteilt durch die Wahrscheinlichkeit, dass es regnet.


Bedingte Wahrscheinlichkeit, $Pr(A|B)$, ist vergleichbar zum Filtern einer Tabelle:

```{r}
#| echo: false
d <- 
  tibble::tribble(
      ~id, ~A, ~B,
      "1", 0L, 0L,
      "2", 0L, 1L,
      "3", 1L, 0L,
      "4", 1L, 1L,
  "SUMME", 2L, 2L
  )

knitr::kable(d)

```

Es ergeben sich folgende Wahrscheinlichkeiten:

$P(A) = 2/4$

$P(B) = 2/4$

$P(A \cap B) = 1/4$

$P(A|B) = 1/2$


#### Rechenregel

Die Wahrscheinlichkeit für $A$, wenn $B$ schon eingetreten ist, berechnet sich so, s. @eq-pr-cond.

$$Pr(A|B) = \frac{Pr(A \cap B)}{Pr(B)}$${#eq-pr-cond}




### Stochastische (Un-)Abhängigkeit

Stochastische Unabhängigkeit ist ein Spezialfall von Abhängigkeit: Es gibt sehr viele Ausprägungen für Abhängigkeit, aber nur eine für Unabhängigkeit.
Können wir Unabhängigkeit nachweisen, haben wir also eine starke Aussage getätigt.


:::{#def-indep}
### Stochastische Unabhängigkeit
Zwei Ereignisse sind (stochastisch) unabhängig voneinander, wenn die Wahrscheinlichkeit von $A$ nicht davon abhängt, ob $B$ der Fall ist.$\square$
:::


:::{#exm-indep1}
### Augenfarbe und Statistikliebe
Ich vermute, dass die Ereignisse "Augenfarbe ist blau" und "Ich liebe Statistik" voneinander unabhängig sind.$\square$^[Wer Daten dazu hat oder eine Theorie, der melde sich bitte bei mir.]
:::





```{r}
#| echo: false
data("titanic_train")


plottitanic1 <-
titanic_train %>%
  select(Pclass, Survived) %>%
  mutate(Survived = factor(Survived)) %>%
  ggplot(aes(x = Pclass)) +
  geom_bar(aes(fill = Survived), position = "fill") +
  theme(legend.position = "bottom")


plottitanic2 <-
titanic_train %>%
  select(Survived, Embarked) %>%
  filter(Embarked %in% c("C", "Q", "S")) %>%
  mutate(Survived = factor(Survived)) %>%
  ggplot(aes(x = Embarked)) +
  geom_bar(aes(fill = Survived), position = "fill") +
  theme(legend.position = "bottom")





plottitanic3 <-
  titanic_train %>%
  select(Survived, Age) %>%
  mutate(Age_prime = isprime(Age),
         Age_prime = factor(Age_prime)) %>%
  mutate(Survived = factor(Survived)) %>%
  ggplot(aes(x = Age_prime)) +
  geom_bar(aes(fill = Survived), position = "fill") +
  theme(legend.position = "bottom") +
  scale_x_discrete(breaks = c(0, 2),
                     labels = c("Nicht Prim", "Prim"))

# plottitanic3

```



:::{#exm-titanic}
### Überleben auf der Titanic
*Abhängig*, s. @fig-abh, links: Überleben auf der Titanic ist offenbar *abhängig* von der Passagierklasse.
Auf der anderen Seite: Das Ereignis *Überleben* auf der Titanic ist *un*abhängig vom Ereignis *Alter ist eine Primzahl*, s. @fig-abh, rechts.$\square$
:::





```{r QM2-Thema1-WasistInferenz-31, out.width="100%"}
#| fig-cap: "Abhängigkeit und Unabhängigkeit zweier Ereignisse"
#| label: fig-abh
#| echo: false
plots(plottitanic1, plottitanic3,
      n_rows = 1,
      title = c("Abhängigkeit zweier Ereignisse", "Un-Abhängigkeit zweier Ereignisse"))
```








Zur Ab- bzw. Un-Abhängigkeit zweier Variablen, an Beispielen illustriert.




:::{#exm-covid}

## Zusammenhang von Covidsterblichkeit und Impfquote


Sind die Ereignisse *Tod durch Covid*  bzw. *Impfquote* ($A$) und *Land*^[hier mit den zwei Ausprägungen *DEU* und *USA*] ($B$) voneinander abhängig (Abb. @fig-covid1)?

```{r covid1}
#| message: false
#| echo: false
#| cache: true
#| fig-cap: Impfquote und Sterblichkeit sind voneinander abhängig (bezogen auf Covid, auf Basis vorliegender Daten)
#| label: fig-covid1


# source: https://ourworldindata.org/covid-vaccinations
# access date: 2021-09-24
# licence: https://ourworldindata.org/covid-vaccinations#licence



dfile <- "data/owid-covid-data.csv"



d <- read_csv(dfile)

d2<-
  d %>%
  filter(iso_code %in% c("DEU", "USA")) %>%
  mutate(date = as_date(date)) %>%
  rename(Land = iso_code) %>%
  select(date,
         Land,
         #total_deaths,
         #new_deaths,
         people_fully_vaccinated_per_hundred,
         total_deaths_per_million,
         #new_vaccinations,
         total_vaccinations) %>%
  filter(date == "2021-09-23") %>%
  group_by(Land)


# d2 %>%
#   ungroup() %>%
#   count(people_fully_vaccinated_per_hundred)

plot_covid1 <-
  d2 %>%
  ggplot(aes(x = Land,
             y = people_fully_vaccinated_per_hundred)) +
  geom_col() +
  labs(title = "Anteil komplett geimpfter Personen",
       subtitle = "2021-09-23")




plot_covid2 <-
  d2 %>%
  ggplot(aes(x = Land,
             y = total_deaths_per_million)) +
  geom_col()+
  labs(title = "Corona-Tote pro Million",
       subtitle = "2021-09-23")


plots(plot_covid1, plot_covid2)
```

Ja, da in beiden Diagrammen gilt: $P(A|B) \ne Pr(A) \ne Pr(A|\neg B)$.$\square$^[
Daten von [Our World in Data](https://ourworldindata.org/covid-deaths).]


:::








### Multiplikationssatz

Gegeben seien die Ereignisse $A$ und $B$. Der Multiplikationssatz wird verwendet, wenn wir an der Wahrscheinlichkeit interessiert sind, dass *beide Ereignisse* $A$ und $B$ eintreten.


#### Unabhängige Ereignisse


:::{#exm-indep2}
Wir werfen eine faire Münze *zwei* Mal (Abb. @fig-2muenzen).^[Oder zwei Münzen ein Mal.]
Wie groß ist die Wahrscheinlichkeit, 2 Mal *K*opf zu werfen?$\square$
:::


![Wir werfen zwei faire Münzen](img/muenz1.png){#fig-2muenzen width="50%"}

Abb. @fig-2muenzen zeigt ein *Baumdiagramm*. 
Jeder *Kasten* (Knoten) zeigt ein *Ergebnis.* 
Die Pfeile (Kanten) symbolisieren die Abfolge des Experiments: Vom "Start" (schwarzer Kreis) 
führen zwei mögliche Ergebniss ab, jeweils mit Wahrscheinlichkeit 1/2.
Die untersten Knoten nennt man auch *Blätter* (Endknoten), sie zeigen das Endresultat des (in diesem Fall) zweifachen Münzwurfs.
Der Weg vom Start zu einem bestimmten Blatt nennt man *Pfad*. 
Die Anzahl der Pfade entspricht der Anzahl der Blätter.
In diesen Diagramm gibt es vier Pfade (und Blätter).


Die Wahrscheinlichkeiten der resultierenden Ereignisse finden sich in @tbl-muenz2.


```{r}
#| echo: false
#| tbl-cap: Wahrscheinlichkeiten der Ereignisse im zweimaligen Münzwurf
tibble::tribble(
  ~Ereignis,               ~Pr,
       "0K", "1/2 * 1/2 = 1/4",
       "1K", "1/4 + 1/4 = 1/2",
       "2K", "1/2 * 1/2 = 1/4"
  )
```

Wir werfen eine faire Münze *drei* Mal (Abb. @fig-3muenzen)

![Wir werfen drei faire Münzen](img/muenz2.png){#fig-3muenzen}



```{r QM2-Thema1-WasistInferenz-27}
#| echo: false
tibble::tribble(
  ~Ereignis,                     ~Pr,
       "0K", "1/2 * 1/2 * 1/2 = 1/8",
       "1K", "1/8 + 1/8 + 1/8 = 3/8",
       "2K",          "3 * 1/8 = 3/8",
       "3K", "1/2 * 1/2 * 1/2 = 1/8"
  ) 
```




$Pr(AB) = Pr(A) \cdot Pr(B) = 50\% \cdot 50\% = 25\%$

```{r plot-unabh-ereignisse}
#| echo: false
#| fig-cap: Unabhängige Ereignisse visualisiert
#| label: fig-unabh-e
plots(plot16a1, plot16a2, plot16c)
```


Abb. @fig-unabh-e zeigt, dass gilt: $P(A\cap B) = P(A) \cdot P(B) = P(B) \cdot P(A)$.


#### Kalt und Regen

Von @mcelreath_statistical_2020 stammt diese Verdeutlichung der gmeinsamen Wahrscheinlichkeit:

Was ist die Wahrscheinlichkeit für *kalt ❄ und Regen ⛈️*?

Die Wahrscheinlichkeit für kalt und Regen ist die Wahrscheinlichkeit von *Regen* ⛈, wenn's *kalt* ❄ ist mal die Wahrscheinlichkeit von *Kälte* ❄.

Ebenfalls gilt:

Die Wahrscheinlichkeit für kalt und Regen ist die Wahrscheinlichkeit von *Kälte* ❄, wenn's *regnet* ⛈️ mal die Wahrscheinlichkeit von *Regen* ⛈️.

Das Gesagte als Emoji-Gleichung:

$P(❄️ und ⛈️) = P(⛈️ |❄️ ) \cdot P(❄️) =  P(❄️ |⛈️ ) \cdot P(⛈️) = P(⛈️ und  ❄️)$



Allgemein:

$P(A\cap B) = P(A) \cdot P(B|A) = P(B) \cdot P(A|B)$


Man kann also die "Gleichung drehen".

#### Abhängige Ereignisse



Ein Baumdiagramm bietet sich zur Visualisierung abhängiger Ereignisse an, s. Abb. @fig-baum-abh. Für unabhängige Ereignisse übrigens auch.


In einer Urne befinden sich fünf Kugeln, von denen vier rot sind und eine blau ist.

wie groß ist die Wahrscheinlichkeit, dass bei zwei Ziehungen ohne Zurücklegen (*ZOZ*) *zwei rote Kugeln* gezogen werden [@bourier_2018], S. 47.


Hier ist unsere Urne:

$$\boxed{\color{red}{R, R, R, R}, \color{blue}B}$$

Und jetzt ziehen wir. Hier ist das Baumdiagramm, s. Abb. @fig-baum-abh.


```{mermaid}
%%| fig-cap: "Baumdiagramm für ein ein zweistufiges Zufallsereignis, wobei der 2. Zug (Stufe) abhängig ist vom 1. Zug."
%%| label: fig-baum-abh
flowchart LR
  A[Start] -->|4/5|B[1. Zug: R]
  A -->|1/5|C[1. Zug: B]
  B -->|3/4|D[2. Zug: R]
  B -->|1/4|E[2. Zug: B]
  D --- H[Fazit: RR:  12/20]
  E --- I[Fazit: RB: 4/20]
  C -->|4/4|F[2. Zug: R]
  C -->|0/4|G[2. Zug: B]
  F --- J[Fazit: BR: 4/20]
  G --- K[Fazit: BB: 0/20]
```



Es gilt also: $P(A\cap B) = P(A) \cdot P(B|A)$.



### Totale Wahrscheinlichkeit




@fig-tot-wskt zeigt das Baumdiagramm für die Aufgabe @bourier_2018, S. 56.

```{mermaid}
%%| fig-cap: Totale Wahrscheinlichkeit
%%| label: fig-tot-wskt
flowchart LR
  A[Start] -->|0.6|B[A1]
  A -->|0.1|C[A2]
  A -->|0.3|D[A3]
  B -->|0.05|E[B]
  B -.->|0.95|F[Nicht-B]
  C -->|0.02|G[B]
  C -.->|0.98|H[Nicht-B]
  D -->|0.04|I[B]
  D -.->|0.96|J[Nicht-B]
```



Gesucht ist die Wahrscheinlichkeit $P(B)$.

Dazu addieren wir die Warhscheinlichkeiten der relevanten Äste.

```{r}
W_total <- 0.6 * 0.05 + 0.1 * 0.02 + 0.3 * 0.04
W_total
```

Die totale Wahrscheinlichkeit beträgt also $P(B) = 4.4\%$.

Einfacher noch ist es, wenn man anstelle von Wahrscheinlichkeiten absolute Häufigkeiten verwendet.


### Bayes

#### Bayes als Baum

Gesucht sei $P(A_1|B)$.

Für Bayes' Formel setzt man die Wahrscheinlichkeit des  *günstigen* Ast zur Wahrscheinlichkeit aller relevanten Äste, $P(B)$.

Der günstige Ast ist hier schwarz gedruckt, die übrigen Äste gestrichelt, s. @fig-tot-wskt2.

```{mermaid}
%%| fig-cap: Günstige Pfade
%%| label: fig-tot-wskt2
flowchart LR
  A[Start] -->|0.6|B[A1]
  A -.->|0.1|C[A2]
  A -.->|0.3|D[A3]
  B --->|0.05|E[B]
  B -.->|0.95|F[Nicht-B]
  C -.->|0.02|G[B]
  C -.->|0.98|H[Nicht-B]
  D -.->|0.04|I[B]
  D -.->|0.96|J[Nicht-B]
```



$$P(A|B) = \frac{P(A1 \cap B)}{P(B)} = \frac{0.6 \cdot 0.05}{0.03 + 0.002 + 0.012} = \frac{0.03}{0.044} \approx 0.68$$

$P(A|B)$ beträgt also ca. 68%.

Zur Erinnerung: $P(B)$ ist die totale Wahrscheinlichkeit.


## Bayes' Theorem

### Bayes als bedingte Wahrscheinlichkeit


Bayes' Theorem ist auch nur eine normale bedingte Wahrscheinlichkeit:


$P(A|B) = \frac{\overbrace{ P(A\cap B)}^\text{umformen}}{P(B)}$

$P(A\cap B)$ kann man  umformen, s. @eq-bayes1:

$$P(A|B) =\frac{P(A\cap B)}{P(B)} = \frac{P(B|A) \cdot P(A)}{P(B)}$${#eq-bayes1}

Man kann sich Bayes' Theorem  auch wie folgt herleiten:



$P(A\cap B) = P(B \cap A) = P(A) \cdot P(B|A) = P(B) \cdot P(A|B)$

Dann lösen wir nach P$(A|B)$ auf:


$P(A|B) = \frac{P(A) \cdot P(B|A)}{P(B)}$


### Wozu wird Bayes in der Praxis genutzt?


In der Praxis nutzt man Bayes häufig, wenn man Daten zu einer Wirkung $W$ hat,
und auf die Ursache $U$ zurückschließen möchte, sinngemäß:

$W \quad \underrightarrow{Bayes} \quad U$.

Dann kann man @eq-bayes1 so schreiben, s. @eq-bayes2:

$$P(U|W) = \frac{ P(U) \cdot P(W|U) }{P(W)}$${#eq-bayes2}

Eine ähnliche Situation, die in der Praxis häufig ist,
dass man Daten $D$ hat und auf die Wahrscheinlichkeit einer Hypothese $H$ schließen möchte, s. @eq-bayes3.

$D \quad \underrightarrow{Bayes} \quad H$.


$$P(H|D) = \frac{ P(H) \cdot P(D|H) }{P(D)}$${#eq-bayes3}

@eq-bayes3 fragt nach $P(H|D)$:

>    Was ist die Wahrscheinlichkeit der Hypothese H, jetzt wo wir die Daten haben (und ein Modell?)

Und antwortet so (@eq-bayes3):

>    Diese Wahrscheinlichkeit entspricht der Grundrate (Apriori-Wahrscheinlichkeit) der Hypothese mal der Plausibilität (Likelihood) der Daten unter Annahme (gegeben) der Hypothese. Aus Standardisierungsgründen dividiert man noch die totale Wahrscheinlichkeit der Daten über alle Hypothesen.


### Zusammengesetzte Hypothesen

Das ist vielleicht ein bisschen fancy,
aber man kann Bayes' Theorem auch nutzen, um die Wahrscheinlichkeit einer *zusammengesetzten Hypothese* zu berechnen: $H = H_1 \cap H_2$. 
Ein Beispiel wäre: "Was ist die Wahrscheinlichkeit, dass es Regen ($R$) *und* Blitzeis ($B$) gibt, wenn es kalt ($K$) ist?".

Das sieht dann so aus, @eq-bayes4:

$$
\begin{aligned}
P(R \cap B |K) &= \frac{ P(R \cap B) \cdot P(K|R \cap B) }{P(D)} \\
&= \frac{ P(R ) \cdot P(B) \cdot P(K|R \cap B) }{P(D)}
\end{aligned}
$${#eq-bayes4}


Hier haben wir $P(R \cap B)$  aufgelöst in $P(R) \cdot P(B)$,
das ist nur zulässig, wenn $R$ und $B$ unabhängig sind.

#### Bayes-Video von 3b1b

Das [Video zu Bayes von 3b1b](https://youtu.be/HZGCoVF3YvM) verdeutlicht das Vorgehen der Bayes-Methode auf einfache und anschauliche Weise.



## Vertiefung


Bei @henze_stochastik_2019 findet sich eine anspruchsvollere Einführung in das Rechnen mit Wahrscheinlichkeit; dieses Kapitel behandelt ein Teil des Stoffes  der Kapitel 2 und 3 von @henze_stochastik_2019.



## Aufgaben


Zusätzlich zu den Aufgaben im Buch:

- [mtcars-abhaengig](https://datenwerk.netlify.app/posts/mtcars-abhaengig/mtcars-abhaengig.html)
- [voll-normal](https://datenwerk.netlify.app/posts/voll-normal/voll-normal.html)
- [corona-blutgruppe](https://datenwerk.netlify.app/posts/corona-blutgruppe/corona-blutgruppe.html)
- [Bed-Wskt2](https://datenwerk.netlify.app/posts/bed-wskt2/bed-wskt2)
- [Gem-Wskt1](https://datenwerk.netlify.app/posts/gem-wskt1/gem-wskt1)
- [wuerfel01](https://datenwerk.netlify.app/posts/wuerfel01/wuerfel01.html)
- [wuerfel02](https://datenwerk.netlify.app/posts/wuerfel02/wuerfel02.html)
- [wuerfel03](https://datenwerk.netlify.app/posts/wuerfel03/wuerfel03.html)
- [wuerfel04](https://datenwerk.netlify.app/posts/wuerfel04/wuerfel04.html)





## ---



![](img/outro-03.jpg){width=100%}






