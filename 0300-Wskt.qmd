# Hallo, Wahrscheinlichkeit

## Lernsteuerung


### Position im Modulverlauf

@fig-modulverlauf gibt einen √úberblick zum aktuellen Standort im Modulverlauf.



![Bayes:Start!](img/Golem_hex.png){width=10%}



```{r}
#| include: false
library(tidyverse)
library(easystats)
library(gmp)  # function "isprime"
library(titanic)
library(knitr)
library(DT)
library(kableExtra)
library(ggraph)
library(DiagrammeR)
library(gt)


source("funs/uniformplot.R")
source("funs/binomial_plot.R")
```




```{r r-setup}
#| echo: false
#| message: false
theme_set(theme_minimal())
scale_colour_discrete <- function(...) 
  scale_color_okabeito()
```


```{r define-plots-16}
#| echo: false
#| fig-width: 7

plot16a <-
  ggplot(data.frame(A = c(0, 1),
                    B = c(0, 1))) +
  aes(x = A, y = B) +
  annotate("rect", xmin = 0, xmax = 1, ymin = 0, ymax = .5,
           color = "#009E73FF", alpha = .7, fill = NA) +
  annotate("rect", xmin = 0, xmax = 1, ymin = 0.5, ymax = 1,
           color = NA, alpha = .5, fill = "#009E73FF") +
  annotate("rect", xmin = 0, xmax =0.5, ymin = 0, ymax = 1,
           color = "#56B4E9FF", alpha = .7, fill = NA) +
  annotate("rect", xmin = 0.5, xmax = 1, ymin = 0, ymax = 1,
           color = "#56B4E9FF", alpha = .5, fill = "#56B4E9FF") +
  scale_x_continuous(breaks = c(0.25, 0.75), labels = c("¬¨A", "A")) +
  scale_y_continuous(breaks = c(0.25, 0.75), labels = c("¬¨B", "B")) +
  theme_minimal()  +
  theme(axis.text = element_text(size = 18)) +
  labs(x = "", y = "")


plot_pr_b <-
  ggplot(data.frame(A = c(0, 1),
                    B = c(0, 1))) +
  aes(x = A, y = B) +
  annotate("rect", xmin = 0, xmax = 1, ymin = 0, ymax = .5,
           color = "grey", alpha = .7, fill = NA) +
  annotate("rect", xmin = 0, xmax = 1, ymin = 0.5, ymax = 1,
           color = NA, alpha = .5, fill = "#009E73FF") +
  scale_x_continuous(breaks = c(0.25, 0.75), labels = c("¬¨A", "A")) +
  scale_y_continuous(breaks = c(0.25, 0.75), labels = c("¬¨B", "B")) +
  annotate("label", x = .5, y = .75, label = "Pr(B) = 50%") +
  theme_minimal() +
  theme(axis.text = element_text(size = 18)) +
  labs(x = "", y = "")

plot_pr_a <-
  ggplot(data.frame(A = c(0, 1),
                    B = c(0, 1))) +
  aes(x = A, y = B) +
  annotate("rect", xmin = 0, xmax =0.5, ymin = 0, ymax = 1,
           color = "grey", alpha = .7, fill = NA) +
  annotate("rect", xmin = 0.5, xmax = 1, ymin = 0, ymax = 1,
           color = "#56B4E9FF", alpha = .5, fill = "#56B4E9FF") +
  scale_x_continuous(breaks = c(0.25, 0.75), labels = c("¬¨A", "A")) +
  scale_y_continuous(breaks = c(0.25, 0.75), labels = c("¬¨B", "B")) +
  annotate("label", x = .75, y = .5, label = "Pr(A) = 50%") +
  theme_minimal()  +
  theme(axis.text = element_text(size = 18)) +
  labs(x = "", y = "")


plot_pr_ab <-
ggplot(data.frame(A = c(0, 1),
                  B = c(0, 1))) +
  aes(x = A, y = B) +
  annotate("rect", xmin = 0, xmax = 1, ymin = 0, ymax = .5,
           color = "#009E73FF", alpha = .7, fill = NA) +
  annotate("rect", xmin = 0, xmax = 1, ymin = 0.5, ymax = 1,
           color = NA, alpha = .5, fill = "#009E73FF") +
  annotate("rect", xmin = 0, xmax =0.5, ymin = 0, ymax = 1,
           color = "#56B4E9FF", alpha = .7, fill = NA) +
  annotate("rect", xmin = 0.5, xmax = 1, ymin = 0, ymax = 1,
           color = "#56B4E9FF", alpha = .5, fill = "#56B4E9FF") +
  annotate("rect", xmin = 0.5, xmax = 1, ymin = 0.5, ymax = 1,
           color = "#E69F00FF", alpha = .7, fill = NA, linewidth = 2) +
  scale_x_continuous(breaks = c(0.25, 0.75), labels = c("¬¨A", "A")) +
  scale_y_continuous(breaks = c(0.25, 0.75), labels = c("¬¨B", "B")) +
  annotate(geom = "label", x = .75, y = 0.75, label = "A,B", color = "#E69F00FF") +
  theme_minimal()  +
  theme(axis.text = element_text(size = 18)) +
  labs(x = "", y = "")






plot_pr_b_geg_a <-
  ggplot(data.frame(A = c(0, 1),
                    B = c(0, 1))) +
  aes(x = A, y = B) +
  annotate("rect", xmin = 0, xmax = 1, ymin = 0.5, ymax = 1,
           color = "#009E73FF", alpha = .3, fill = NA, linewidth = 2) +
  annotate("rect", xmin = 0, xmax = 1, ymin = 0.5, ymax = 1,
           color = NA, alpha = .5, fill = "NA") +
  # annotate("rect", xmin = 0, xmax =0.5, ymin = 0, ymax = 1,
  #          color = "#56B4E9FF", alpha = .7, fill = NA) +
  annotate("rect", xmin = 0.5, xmax = 1, ymin = 0, ymax = 1,
           color = "#56B4E9FF", alpha = .7, fill = "#56B4E9FF", linewidth = 2) +
  annotate("rect", xmin = 0.5, xmax = 1, ymin = 0.5, ymax = 1,
           color = "#E69F00FF", alpha = .3, fill = "#E69F00FF", linewidth = 2) +
  scale_x_continuous(breaks = c(0.25, 0.75), labels = c("¬¨A", "A")) +
  scale_y_continuous(breaks = c(0.25, 0.75), labels = c("¬¨B", "B")) +
  annotate(geom = "label", x = .75, y = 0.75, label = "B|A", color = "#E69F00FF")  +
  theme(axis.text = element_text(size = 18)) +
  labs(x = "", y = "")


```




### √úberblick

Dieses Kapitel hat die Wahrscheinlichkeitstheorie (synonym: Wahrscheinlichkeitsrechnung) bzw. das Konzept der Wahrscheinlichkeit zum Thema.^[Die Wahrscheinlichkeitstheorie bildet zusammen mit der Statistik das Fachgebiet der Stochastik.]
Es geht sozusagen um die Mathematik des Zufalls.



### Wozu brauche ich dieses Kapitel?

Im wirklichen Leben sind Aussagen (Behauptungen) so gut wie nie sicher.

- "Weil sie so schlau ist, ist sie erfolgreich."
- "In Elektroautos liegt die Zukunft."
- "Das klappt sicher, meine Meinung."
- "Der n√§chste Pr√§sident wird XYZ."





### Lernziele

Nach Absolvieren des jeweiligen Kapitels sollen folgende Lernziele erreicht sein.

Sie k√∂nnen ...


- die Grundbegriffe der Wahrscheinlichkeitstheorie erl√§uternd definieren
- die Definitionen von Wahrscheinlichkeit beschreiben
- typische Relationen (Operationen) von Ereignissen anhand von Beispielen veranschaulichen
- erl√§utern, was eine Zufallsvariable ist



### Begleitliteratur

Lesen Sie zur Begleitung dieses Kapitels @bourier2011, Kap. 2-4. 


### Eigenstudium


:::{.callout-important}
Dieses Kapitel ist selbst√§ndig im Eigenstudium vorzubereiten vor dem Unterricht.
Lesen Sie dazu die angegebene Literatur.$\square$
:::


### Pr√ºfungsrelevanter Stoff

Der Stoff dieses Kapitels deckt sich (weitgehend) mit @bourier2011, Kap. 2-4. 
Weitere √úbungsaufgaben finden Sie im dazugeh√∂rigen √úbungsbuch, @bourier2022.

:::callout-note
In Ihrer [Hochschul-Bibliothek kann das Buch als Ebook verf√ºgbar](https://fantp20.bib-bvb.de/TouchPoint/singleHit.do?methodToCall=showHit&curPos=3&identifier=2_SOLR_SERVER_1157422278) sein. 
Pr√ºfen Sie, ob Ihr Dozent Ihnen weitere Hilfen im [gesch√ºtzten Bereich (Moodle)](https://moodle.hs-ansbach.de/mod/resource/view.php?id=136047) eingestellt hat.$\square$
:::










### Begleitvideos


- [Video zum Thema Wahrscheinlichkeit](https://youtu.be/rR6NspapEyo)





## Grundbegriffe


### Zufallsvorgang


:::{#exm-muenz}
Klassisches Beispiel f√ºr einen Zufallsvorgang ist das (einmalige oder mehrmalige) Werfen einer M√ºnze.$\square$

Werfen Sie eine M√ºnze!
Diese hier zum Beispiel:

![](img/1024px-Coin-155597.svg.png){width=10% fig-align="center"}

[Quelle: By OpenClipartVectors, CC0]( https://pixabay.com/pt/moeda-euro-europa-fran%C3%A7a-dinheiro-155597)



Wiederholen Sie den Versuch 10 Mal.

Das reicht Ihnen nicht? Okay, wiederholen Sie den Versuch 100, nein 1000, nein: $10^6$ Mal.^[$10^6 = 1000000$]

Notieren Sie als Ergebnis, wie oft die Seite mit der Zahl oben liegen kommt ("Treffer").$\square$
:::


Oder probieren Sie die [App der Brown University](https://seeing-theory.brown.edu/basic-probability/index.html#section1), 
wenn Sie keine Sehnenscheidenentz√ºndung bekommen wollen.


:::{#def-zufall}
### Zufallsvorgang
Ein *Zufallsvorgang* oder *Zufallsexperiment* ist eine einigerma√üen klar beschriebene T√§tigkeit, deren Ergebnis nicht sicher ist. 
Allerdings ist die Menge m√∂glicher Ergebnisse bekannt und die Wahrscheinlichkeit f√ºr alle Ergebnisse kann quantifiziert werden. $\square$
:::

:::{#exm-zufallsvorgang}
### Typische Zufallsvorg√§nge

- *W√ºrfeln*: Das Werfen eines fairen W√ºrfels ist ein klassisches Beispiel. Der Ausgang (die Augenzahl) kann 1, 2, 3, 4, 5 oder 6 sein.
- *M√ºnzwurf*: Beim Werfen einer M√ºnze sind die m√∂glichen Ausg√§nge "Kopf" oder "Zahl". 
- *Lottoziehung*: Die Ziehung von 6 aus 49 Kugeln ist ein komplexeres Zufallsexperiment. Jeder Ausgang ist eine bestimmte Kombination von 6 Zahlen. - *Kartenziehen*: Das Ziehen einer Karte aus einem gut gemischten Kartendeck ist ein weiteres Beispiel.
- *Gl√ºcksrad*: Das Drehen eines Gl√ºcksrads mit verschiedenen Feldern (z.B. Farben oder Zahlen). Welches Feld am Ende stehenbleibt, ist zuf√§llig. $\square$
:::


:::{#exr-zufall2}
Nennen Sie Beispiele f√ºr Zufallsvorg√§nge!\square^[Beispiele f√ºr Zufallsexperimente 
das Messen eines Umweltph√§nomens wie der Temperatur oder die Anzahl der Kunden, 
die einen Laden betreten. 
In jedem dieser F√§lle sind die m√∂glichen Ergebnisse nicht im Voraus bekannt und h√§ngen von nicht komplett bekannten Faktoren ab.]
:::


:::callout-caution
Zufall hei√üt nicht, dass ein Vorgang keine Ursachen h√§tte. 
So gehorcht der Fall einer M√ºnze komplett den Gesetzen der Gravitation. 
W√ºrden wir diese Gesetze und die Ausgangsbedingungen (Luftdruck, Fallh√∂he, Oberfl√§chenbeschaffenheit, Gewichtsverteilungen, ...) exakt kennen, k√∂nnten wir theoretisch sehr genaue Vorhersagen machen. 
Der "Zufall" w√ºrde aus dem M√ºnzwurf verschwinden. Man sollte "Zufall" also besser verstehen als "unbekannt".$\square$
::::


:::{#exr-w√ºrfel-geo}
[Mit dieser App](https://www.geogebra.org/m/cbqee8h7) k√∂nnen Sie W√ºrfelw√ºrfe simulieren und die Ausg√§nge dieses Zufallsexperiments beobachten.$\square$
:::

### Ergebnisraum

:::{#def-Ergebnisraum}
### Ergebnisraum
Die m√∂glichen Ergebnisse eines Zufallvorgangs fasst man als Menge mit dem Namen *Ergebnisraum* zusammen. 
Man verwendet den griechischen Buchstaben $\Omega$ f√ºr diese Menge.
Die Elemente $\omega$ (kleines Omega) von $\Omega$ nennt man *Ergebnisse*.$\square$
:::

:::{#exm-grundraum}
Beobachtet man beim W√ºrfelwurf (s. @fig-wuerfel) die oben liegende Augenzahl, so ist 



$$\Omega = \{ 1,2,3,4,5,6 \} = \{‚öÄ, ‚öÅ, ‚öÇ, ‚öÉ, ‚öÑ, ‚öÖ\}$$

ein nat√ºrlicher Grundraum [@henze2019].$\square$
:::


:::{#exm-ergebnisraum2}

- *M√ºnzwurf*: $\Omega = \{ \text{Kopf, Zahl} \}$
- *Lotto*:  Bei der Ziehung von 6 aus 49 Kugeln ist der Grundraum die Menge aller m√∂glichen Kombinationen von sechs Zahlen, das sind ca. 14 Millionen.
- *Kartenziehen*: Wenn eine einzelne Karte aus einem 52er-Kartendeck gezogen wird, ist der Grundraum die Menge aller 52 Karten.
- *Gl√ºcksrad*: Wenn das Gl√ºcksrad in vier gleich gro√üe, farbige Felder unterteilt ist (Rot, Gr√ºn, Blau, Gelb), dann ist der Grundraum die Menge der m√∂glichen Farben: $\Omega = \{ \text{Rot, Gr√ºn, Blau, Gelg} \}$ $\square$
:::


Die Wahrscheinlichkeitsrechnung baut auf der Mengenlehre auf, daher wird die Notation  der Mengenlehre hier verwendet.



::: {.content-visible when-format="html"}
![Ein (sechsseitiger) W√ºrfel, Bildquelle: Peter Steinberg, Wikipedia, CC-BY-SA 3.0](img/120px-Hexahedron-slowturn.gif){#fig-wuerfel width=10%}
:::

::: {.content-visible unless-format="html"}


![Ein (sechsseitiger) W√ºrfel](img/Dice_2005.jpg){width=33%}

[Bildquelle: CC BY-SA 3.0](https://commons.wikimedia.org/w/index.php?curid=474827)
:::



### Ereignis



:::{#def-ereignis}
### Ereignis
Jede Teilmenge^[$A$ ist eine Teilmenge von $B$, 
wenn alle Elemente von $A$ auch Teil von $B$ sind.] von $\Omega$ hei√üt *Ereignis*; $A \subseteq \Omega$ .$\square$
:::



:::{#exm-ereignis}
Beim Mensch-√§rger-dich-nicht Spielen habe ich eine 6 geworfen.^[Schon wieder.]
Das Nennen wir das Ereignis $A$: "Augenzahl 6 liegt oben" und schreiben in Kurzform:

$A= \{6\}\square$
:::


:::{#exm-muenzwurf}
Sie werfen eine M√ºnze (Sie haben keinen Grund, an ihrer Fairness zu zweifeln). "Soll ich jetzt lernen f√ºr die Klausur (Kopf) oder lieber zur Party gehen (Zahl)?"

@fig-baummuenz1 zeigt die m√∂glichen Ausg√§nge (T wie Treffer (Party) und N  (Niete, Lernen)) dieses Zufallexperiments.

```{mermaid}
%%| label: fig-baummuenz1
%%| fig-cap: Sie werfen eine M√ºnze. Party oder Lernen???
flowchart LR
 M[Sie werfen die M√ºnze] --> T["T (Treffer) ü•≥"]
  M --> N["N (Niete) üìö"]
```

Das Ereignis *Zahl* ist eingetreten! Treffer! Gl√ºck gehabt!^[?]$\square$
:::


### Unm√∂gliches und sicheres Ereignis



:::{#def-unm-sich}
### Unm√∂gliches und sicheres Ereignis
Die leere Menge $\varnothing$ hei√üt das *um√∂gliche*, der Grundraum $\Omega$ hei√üt das *sichere Ereignis*. $\square$
:::

:::{#exm-unm}
### Unm√∂gliches Ereignis
Alois behauptet, er habe mit seinem W√ºrfel eine 7 geworfen.
Schorsch erg√§nzt, sein W√ºrfel liege auf einer Ecke, so dass keine Augenzahl oben liegt.
Draco hat seinen W√ºrfel runtergeschluckt. 
Dracos und Alois' Ereignisse sind *unm√∂gliche Ereignisse*, zumindest nach unserer Vorstellung des Zufallsexperiments.$\square$
:::


:::{#exm-sicher}
### Sicheres Ereignis
Nach dem der W√ºrfel geworfen wurde, liegt eine Augenzahl zwischen 1 und 6 oben.$\square$
:::


### Elementarereignis

:::{#def-defelementarereignis}
### Elementarereignis
Jede einelementige Teilmenge $\{\omega\}$ von $\Omega$ hei√üt *Elementarereignis* (h√§ufig mit $A$ bezeichnet).
^[Ein *Ergebnis* ist ein Element von $\Omega$. Elementarereignisse sind die einelementigen Teilmengen von $\Omega$. Konzeptionell sind die beiden Begriffe sehr √§hnlich, vgl. <https://de.wikipedia.org/wiki/Ergebnis_(Stochastik)>. Wir werden uns hier auf den Begriff *Elementarereignis* konzentrieren und den Begriff *Ergebnis* nicht weiter verwenden.] $\square$
:::


:::{#exm-exmelementarereignis}

- Sie spielen Mensch-√§rger-dich-nicht. Und brauchen dringend eine `6`. Sie w√ºrfeln. Das Ereignis $A = \{1\}$ tritt ein.^[Na toll.]
- Sie schreiben eine Statistik-Klausur. Irgendwie haben Sie das Gef√ºhl, das Ergebnis sei ein Zufallsexperiment... Jedenfalls k√∂nnen nach Adam Riese zwei Dinge passieren: $\Omega= \{\text{bestehen, nicht bestehen}\}$.
Das erste der beiden Elementarereignisse tritt ein. Yeah!
- Sie f√ºhren eine Studie durch zur Wirksamkeit einer Lern-App. Es ist nicht klar, ob die App wirklich was bringt f√ºr den Lernerfolg. Vereinfacht gesprochen ist der Grundraum dieses Experiments: $\Omega = \{\text{schadet, bringt nichts, n√ºtzt}\}$.
Die Daten sprechen f√ºr das Ereignis $A = \{\text{bringt nichts}\}$.
:::

:::{#exr-not-elementarereignisse}
Welche Ereignisse beim W√ºrfelwurf sind keine Elementarereignisse? $\square$
:::


### Vollst√§ndiges Ereignissystem


:::{#def-vollereignis}
### Vollst√§ndiges Ereignissystem
Wird der Grundraum $\Omega$ vollst√§ndig in paarweis disjunkte Ereignisse zerlegt, 
so bilden diese Ereignisse ein vollst√§ndiges Ereignissystem, s. @fig-vollereignis.$\square$
:::


![Zerlegung des Grundraums in ein vollst√§ndiges Ereignissystem](img/vollereignis.png){#fig-vollereignis width=50%}



:::{#exm-vollereig1}
Sei $\Omega$ der typische Ergebnisraum des W√ºrfelwurfs. Wir zerlegen den Grundraum in zwei Ereignisse, $A$ "gerade Zahlen", und $B$ "ungerade Zahlen". 
Damit haben wir ein vollst√§ndiges Ereignissystem erstellt, s. @fig-complete-event-system1.


:::{#fig-complete-event-system1}

::: {.figure-content}


\begin{align}
A = \{2,4,6\} \qquad \hfill \boxed{\color{gray}{1}\; \boxed{\color{black}{2}}\; \color{gray}{3}\; \boxed{\color{black}{4}}\; \color{gray}{5}\; \boxed{\color{black}{6}}\;} \\
B = \{1,3,5\} \qquad  \hfill \boxed{\boxed{\color{black}{1}}\; \color{gray}{2}\; \boxed{\color{black}{3}}\; \color{gray}{4}\; \boxed{\color{black}{5}}\; \color{gray}{6}\; } \\
\hline \\
\Omega = \{1,2,3,4,5,6\}  \qquad  \hfill \boxed{1\; 2\; 3\; 4\; 5\; 6 } 

\end{align}
:::

:::

Ein Beispiel f√ºr ein vollst√§ndiges Ereignissystem

:::


:::::{#exm-vollereig2}
Sei $\Omega$ der typische Ergebnisraum des W√ºrfelwurfs. Wir zerlegen den Grundraum in zwei Ereignisse, $A$ "1,2,3", und $B$ "4,5,6". 
Damit haben wir ein vollst√§ndiges Ereignissystem erstellt, s. @fig-complete-event-system1.

::::{#fig-fig-complete-event-system2}

::: {.figure-content}


\begin{align}
A = \{1,2,3\} \qquad \qquad \hfill  \boxed{\boxed{ \color{black}{1\; 2\; 3}}\; \color{gray}{4\; 5\; 6}} \\
B = \{4,5, 6\} \qquad \qquad  \hfill \boxed{\color{gray}{1 \; 2 \; 3}\; \boxed{\color{black}{4\; 5 \; 6}}} \\

\newline
\hline \\
\Omega = \{1,2,3,4,5,6\} \qquad \qquad \hfill  \boxed{1\; 2\; 3\; 4\; 5\;6}
\end{align}
:::

Noch ein Beispiel f√ºr ein vollst√§ndiges Ereignissystem

::::
:::::




### M√§chtigkeit

:::{#def-macht}
### M√§chtigkeit
Die Anzahl der Elementarereignisse eines Ereignismraums nennt man die M√§chtigkeit (des Ergebnisraums).^[Die Menge aller Teilmengen einer Menge $A$ nennt man die *Potenzmenge* $\mathcal{P}(A)$, vgl. [hier](https://de.wikipedia.org/wiki/Datei:Hasse_diagram_of_powerset_of_3.svg).]$\square$
:::


Die M√§chtigkeit von $\Omega$ bezeichnet man mit dem Symbol $|\Omega|$.

:::{#exm-macht}
Beim Wurf eines W√ºrfels mit $\Omega=\{1,2,3,4,5,6\}$ gibt es 6 Elementarereignisse. 
Die M√§chtigkeit ist also 6: $|\Omega|=6$.$\square$
:::



### Disjunkte Ereignisse




Seien $A= \{1,2,3\}; B= \{4,5,6\}$.

$A$ und $B$ sind disjunkt^[engl. disjoint]: ihre Schnittmenge ist leer: $A \cap B = \emptyset$,
s. @fig-disjunkt.




![Zwei disjunkte Ereignisse, dargestellt noch √ºberlappungsfreie Kreise](img/2880px-Disjunkte_Mengen.svg.png){#fig-disjunkt width="25%" fig-align="center"}





[Quelle: rither.de](http://www.rither.de/a/mathematik/stochastik/mengentheorie-und-venn-diagramme/)


::::: {#exm-disjunkt1}
Das Ereignis $A$ "Gerade Augenzahl beim W√ºrfelwurf", $A={2,4,6}$ und das Ereignis $B$ "Ungerade Augenzahl beim W√ºrfelwurf", $B={1,3,5}$ sind disjunkt, s. @fig-disjunkt1.


::::{#fig-disjunkt1}

::: {.figure-content}


\begin{align}
A = \{2,4, 6\} \qquad \hfill \boxed{2\; 4\; 6} \\
B = \{1,3,5\} \qquad  \hfill \boxed{1\; 3\; 5} \\
\hline \\
A \cap B = \qquad  \hfill  \emptyset
\end{align}
:::

Beispiel f√ºr disjunkte Ereignisse

::::


:::::


:::{exm-disjunkt2}
Die Ereignisse "normaler Wochentag" und "Sonntag" sind disjunkt. $\square$
:::






:::{#exr-elementarereignis}
### Peer Instruction: Elementarereignis
Welche der folgenden Ereignisse zeigt ein Elementarereignis des W√ºrfelwurfs (wobei die Augenzahl 1,2,...,6 die Ergebnisse sind).

A) Gerade Zahl gew√ºrfelt
B) Ungereade Zahl gew√ºrfelt
C) Keine *6* gew√ºrfelt
D) *1* gew√ºrfelt
E) Keine der genannten $\square$
:::



## Was ist Wahrscheinlichkeit?


Die "klassische" Logik der Wissenschaft beruht auf Schl√ºssen (Syllogismen) wie diesem: "Alle Schw√§ne sind wei√ü." $\rightarrow$ "Dies ist ein Schwan." $\rightarrow$ "Dieser Schwan ist wei√ü."

Mit dem Konzept von Wahrscheinlichkeit k√∂nnen wir jetzt auch folgende Logik anwenden:
"Die meisten Schw√§ne sind wei√ü." $\rightarrow$ "Dies ist ein Schwan." $\rightarrow$ "Dieser Schwan ist wahrscheinlich wei√ü."
Das ist ein gro√üer Fortschritt, der die Denkweise der Wissenschaft gut widerspiegelt und daf√ºr eine logisch-mathematische Grundlage bereitstellt,
 s. @fig-prob-logic.
 
 

:::{#def-wskt}
### Einfache Definition von Wahrscheinlichkeit
Die Wahrscheinlichkeit ist ein Ma√ü f√ºr die Plausibilit√§t einer Aussage $A$, gegeben gewisser Hintergrundinformationen (Daten, $D$): $Pr(A|D)$.
Die Wahrscheinlichkeit eines Ereignisses wird (sofern berechenbar) als Zahl zwischen 0 und 1 angegeben,
wobei 0 bedeutet, dass das Ereignis als unm√∂glich angesehen wird,
und 1 bedeutet, dass das Ereignis als sicher betrachtet wird.
Je n√§her die Wahrscheinlichkeit bei 1 (0) liegt, desto sicherer ist jemand,
dass das Ereignis (nicht) der Fall ist.$\square$
:::

Die Wahrscheinlichkeitsrechnung ist die typische Methode, um Ungewissheit zu pr√§zisieren, d.h. zu quantifizieren.


Wir haben schon mit @def-wskt eine erste Definition von Wahrscheinlichkeit versucht. 
Jetzt gehen wir die Sache noch etwas n√§her an und vergleichen verschiedene Ideen (Definitionen) von Wahrscheinlichkeit.


### Formallogische Definitition

#### Wahrscheinlichkeit als Erweiterung der Logik

Die formallogische Konzeption von Wahrscheinlichkeit sieht Wahrscheinlichkeit als Erweiterung der formalen Logik [@jaynes2003].
^[Manchmal wird diese Art der Wahrscheinlichkeit auch *epistemologische* Wahrscheinlichkeit genannt.] 
In der formalen Logik ist ein Ereignis entweder *falsch* oder *wahr*. 
In der formallogischen Konzeption wird der Platz zwischen "falsch" (0) und "richtig" 
(1)
als die Wahrscheinlichkeit $0<p<1$, gesehen [@briggs2016], s. @fig-prob-logic.
Gr√∂√üere Werte stehen f√ºr gr√∂√üere Wahrscheinlichkeit und umgekehrt.






![Wahrscheinlichkeit als Erweiterung der Logik](img/prob-logic.png){#fig-prob-logic}

Nach dieser "Wahrscheinlichkeitslogik" kann man ein Ereignis, 
von dessen Eintreten man "wenig √ºberzeugt" ist, z.B. mit 0.2 quantifizieren. 
Hingegen einem Ereignis, von man "recht sicher" ist, mit 0.8 quantifizieren, s. @fig-prob-logic2.


![Ein Ereignis von dessen Eintreten man gering bzw. stark √ºberzeugt ist](img/prob-logic2.png){#fig-prob-logic2}


#### Wahrscheinlichkeit als Ungewissheit



Wahrscheinlichkeit existiert nicht in dem Sinne, wie ein Stein oder ein Mensch existiert. Wahrscheinlichkeit ist stattdessen eine Art von Wissen.
Daher ist es (nach @jaynes2003) falsch, zu sagen:
"Die Wahrscheinlichkeit f√ºr Zahl bei dieser M√ºnze ist 50%."
Richtig w√§re f√ºr einen unbedarften Spieler: "Ich habe keinen Grund, nicht an der Fairness der M√ºnze zu glauben, also gehe ich von 50% Wahrscheinlichkeit f√ºr Zahl aus."
Der Betr√ºger, der diese M√ºnze in der Hand hat, geht allerdings von einer Wahrscheinlichkeit von 75% f√ºr Zahl aus (er kennt die M√ºnze besser als Sie).

Das Beispiel zeigt: Wahrscheinlichkeit ist kein Ding der Welt, sondern ein Wissenszustand.

Formaler ausgedr√ºckt: Sei $Pr$ die Wahrscheinlichkeit des Ereignis $Z$ (Zahl) angibt, *gegeben* meiner Annahme, dass die M√ºnze fair ist ($F$), also eine Wahrscheinlichkeit von 50% (1/2 = .5) hat f√ºr $Z$. Dann kann man kurz schreiben:

$$Pr(Z|F) = 0.5$$
Gegeben des Wissens des Betr√ºgers $B$ w√§re die Wahrscheinlichkeit f√ºr $Z$ anders:

$$Pr(Z|B) = 0.75$$

In Worten: "Die Wahrscheinlichkeit von Zahl ($Z$) gegeben das Wissen des Betr√ºgers √ºber die M√ºnze ($B$) liegt bei 75%.

Das Beispiel zeigt auch, dass die Wahrscheinlichkeit eines Ereignissens (wie $Z$) von Hintergrundinformationen (Annahmen, Daten, Evidenz, ...) abh√§ngt.
Ohne zu sagen, auf welche Hintergrundinterformationen wir uns beziehen (die des unbedarften Spielers oder die des Betr√ºgers), ist es sinnvoll, eine Wahrscheinlichkeit anzugeben.
Daher ist $Pr(Z) = 0.5$ unvollst√§ndig.
Allerdings wird die Hintergrundinformation oft weggelassen, wenn es klar ist, welche Hintergrundinformation vorliegt.
So wird h√§ufig vorausgesetzt, dass eine M√ºnze fair ist.

:::{#exm-bedingt}
### Morgen regnet's

Es ist daher unvollst√§ndig zu sagen: "Morgen wir es mit einer Wahrscheinlichkeit von 70% regnen."

Man m√ºsste Hintergrundinformation (*Evidenz*, $E$) erg√§nzen, z.B. "auf Basis des Wettermodell X".

Also: Anstelle von $Pr(\text{Regen}) = .7$ besser sagen: $Pr(\text{Regen|Wettermodell X})$. $\square$
:::


:::{#exm-bedingt2}
### Wahrscheinlichkeit f√ºr Krebs

Jemand beobachtet bei sich Symptome, die f√ºr eine eine Hautkrebserkrankung $K$ typisch sind. Die Person sagt sich: "Ach, die Wahrscheinlichkeit f√ºr Hautkrebs liegt bei einem Promill. Kein Grund f√ºr Sorge." Formal: $Pr(K) = 0.001$.
Aber wenn die Person relevante Symptome $S$ hat, gilt: $Pr(\text{K} \mid \text{S}) \gg 0.001$. $\square$
:::


:::callout-note
Sagt jemand: "Die Wahrscheinlichkeit von A ist x%", frag immer: "gegeben welcher Annahmen, welcher Evidenz?"
:::


:::{#def-wskt}
### Wahrscheinlichkeit unter formallogischer Sichtweise
Die Wahrscheinlichkeit ist ein Ma√ü f√ºr unsere Gewissheitt einer Aussage $A$, gegeben gewisser Hintergrundinformationen (Daten, $D$): $Pr(A|D)$.
Die Wahrscheinlichkeit eines Ereignisses wird (sofern berechenbar) als Zahl zwischen 0 und 1 angegeben,
wobei 0 bedeutet, dass das Ereignis als unm√∂glich angesehen wird,
und 1 bedeutet, dass das Ereignis als sicher betrachtet wird.
Je n√§her die Wahrscheinlichkeit bei 1 (0) liegt, desto sicherer ist jemand,
dass das Ereignis (nicht) der Fall ist.$\square$
:::


:::{#def-indifferenzprinzip}
### Indifferenzprinzip

Das Indifferenzprinzip (synonym: Prinzip des unzureichenden Grundes) besagt, 
dass in Abwesenheit jeglicher Informationen, die bestimmte Ereignisse bevorzugen oder benachteiligen w√ºrden, 
alle m√∂glichen Ereignisse als gleich wahrscheinlich angesehen werden sollten. $\square$
:::


Vor uns liegt ein W√ºrfel. Schlicht, ruhig, unbesonders.
Wir haben keinen Grund anzunehmen, dass eine seiner $n=6$ Seiten bevorzugt nach oben zu liegen kommt. 
Jedes der sechs Elementarereignisse ist uns gleich plausibel;
der W√ºrfel erscheint uns fair.
In Ermangelung weiteres Wissens zu unserem W√ºrfel gehen wir schlicht davon aus, dass jedes der $n$ Elementarereignis gleich wahrscheinlich ist.
Es gibt keinerlei Notwendigkeit, den W√ºrfel in die Hand zu nehmen,
um zu einer Wahrscheinlichkeitsaussage auf diesem Weg zu kommen.
Nat√ºrlich *k√∂nnten* wir unsere Auffassung eines fairen W√ºrfels testen,
aber auch ohne das Testen k√∂nnen wir eine stringente Aussage (basierend auf dem Indifferenzprinzip (s. @def-indifferenzprinzip) der $n$ Elementarereignisse) zur Wahrscheinlichkeit eines bestimmten (Elementar-)Ereignisses $A$ kommen [@briggs2016], s. @thm-briggs.




:::{#thm-briggs}

### Indifferenzprinzip

$$Pr(A) = \frac{1}{n}= \frac{1}{|\Omega|} \quad \square$$
:::

:::{#exm-briggs}
Sei $A$ = "Der W√ºrfel wird beim n√§chsten Wurf eine 6 zeigen."
Die Wahrscheinlichkeit f√ºr $A$ ist $1/6. \square$
:::





:::{#def-laplace}
### Laplace-Experimt
Ein Zufallsexperiment, bei dem alle Elementarereignisse dieselbe Wahrscheinlichkeit haben, nennt man man ein *Laplace-Experiment*, s. @thm-laplace. $\square$
:::

In Erweiterung von @thm-briggs k√∂nnen wir f√ºr ein Laplace-Experiment schreiben, s. @thm-laplace.

:::{#thm-laplace}

### Laplace-Experiment

$$Pr(A)=\frac{\text{Anzahl Treffer}}{\text{Anzahl m√∂glicher Ergebnisse}} \quad \square$$
:::





:::{#exm-prob}
### Kann Sophia (Einhorn) fliegen?

Sei $A$: "Sophia ist ein Einhorn". Und sei $B$: "Einh√∂rner mit Fl√ºgeln k√∂nnen fliegen und die H√§lfte aller Einh√∂rner hat Fl√ºgel". Dann ist $Pr(A|B) = 1/2$.
$\square$
:::




:::{#exm-prob3}
### Ich werfe Zahl beim n√§chsten M√ºnzwurf

$B$ "Ich habe keinen Grund an der Fairness der M√ºnze zu zweifeln". 
$A$: "Es wird *Zahl* geworfen beim n√§chsten Wurf dieser M√ºnze". 
Es gilt: $Pr(A|B) = 1/2$. $\square$
:::






### Frequentistische Definition

In Ermangelung einer Theorie zum Verhalten eines (uns) unbekannten Zufallsvorgangs und unter der Vermutung, dass die Elementarereignisse nicht gleichwahrscheinlich sind, bleibt uns ein einfacher (aber aufw√§ndiger und manchmal unm√∂glicher) Weg, um die Wahrscheinlichkeit eines Ereignisses zu bestimmen: Ausprobieren.

Angenommen, ein Statistik-Dozent, bekannt f√ºr seine Vorliebe zum Gl√ºcksspiel und 
mit scheinbar endlosen Gl√ºcksstr√§hnen (er wirft andauernd eine 6), 
hat seinen Lieblingsw√ºrfel versehentlich liegen gelassen. 
Das ist *die* Gelegenheit!
Sie greifen sich den W√ºrfel, und ... Ja, was jetzt?
Nach kurzer √úberlegung kommen Sie zum Entschluss, den W√ºrfel einem "Praxistest" zu unterziehen: 
Sie werfen ihn 1000 Mal (Puh!) und z√§hlen den Anteil der `6`.
Falls der W√ºrfel fair ist, m√ºsste gelten $Pr(A=6)=1/6\approx .17$. Schauen wir mal!



```{r}
#| echo: false
n <- 1e3

set.seed(42)
wuerfel_oft <- 
  sample(x = 1:6, size = n, replace = TRUE) 


wuerfel_tab <-
  tibble(
    id = 1:n,
    x = wuerfel_oft,
    ist_6 = ifelse(x == 6, 1, 0),
    ist_6_cumsum = cumsum(ist_6) / id
  )

```


Und hier der Anteil von  `6` im Verlauf unserer W√ºrfe, s. @fig-wuerfel.


```{r}
#| label: fig-wuerfel
#| fig-cap: "Das Gesetz der gro√üen Zahl am Beispiel der Stabilisierung des Trefferanteils beim wiederholten W√ºrfelwurf"
#| fig-asp: 0.5
#| echo: false

wuerfel_tab %>% 
  slice_head(n = 1e3) %>% 
  ggplot() +
  aes(x = id, y = ist_6_cumsum) +
  geom_hline(yintercept = 1/6, color = "grey80", size = 3) +
  geom_line() +
  labs(x = "Nummer des W√ºrfelwurfs",
       y = "Kummulierte H√§ufigkeit einer Sechs") +
  annotate("label", x = 1000, y = 1/6, label = "0.17")
```

Hm, auf den ersten Blick ist kein (starkes) Anzeichen f√ºr Schummeln bzw. einen gezinkten W√ºrfel zu finden.






### Kolmogorovs Definition {#sec-kolmogorov}

Kolmogorov richtet eine Reihe von Forderungen an eine Definition von bzw. an das Rechnen mit Wahrscheinlichkeiten, 
die direkt plausibel erscheinen:

1. *Nichtnegativit√§t*: Die Wahrscheinlichkeit eines Ereignisses kann nicht negativ sein.
2. *Normierung*: Das sichere Ereignis hat die Wahrscheinlichkeit 1 bzw. 100%: $Pr(\Omega)=1$; das unm√∂gliche Ereignis hat die Wahrscheinlichkeit 0: $Pr(\emptyset)=0$.
3. *Additivit√§t*. Sind $A$ und $B$ disjunkt, dann ist die Wahrscheinlichkeit, 
dass mindestens eines der beiden Ereignisse eintritt ($A\cup B$) gleich der Summe der beiden Einzelwahrscheinlichkeiten von $A$ und $B$.




:::{#exr-prob2}
### Peer Instruction: Ist der Schmockulator im Zustand alpha?

$B$: "Locuratoren und Schmockulatoren kommen in zwei Zust√§nden vor, alpha und beta. 
Und zwar gleich h√§ufig". 
Leider wissen wir nichts Weiteres √ºber Locuratoren und Schmockulatoren.
Vor Ihnen steht ein Schmockulator. 
Welche m√∂glichst pr√§zise Aussage k√∂nnen wir √ºber den Zustand des Schmockulator treffen?

A) Der Schmockulator ist sicher im Zustand alpha.
B) Der Schmockulator ist in einem der beiden Zust√§nde.
C) Der Schmockulator ist vermutlich im Zustand alpha.
D) Der Schmockulator ist m√∂glicherweise im Zustand alpha.
E) Der Schmockulator ist mit einer Wahrscheinlichkeit von 50% im Zustand alpha.
F) Keine der genannten. $\square$
:::









## Relationen von Ereignissen

F√ºr das Rechnen mit Wahrscheinlichkeiten ist es hilfreich, ein paar Werkzeuge zu kennen, die wir uns im Folgenden anschauen.

:::{#def-relation}
### Relation
Eine Relation (zweier Ereignisse) bezeichnet die Beziehung, 
in der die beiden Ereignisse zueinander stehen. $\square$
:::

Typische Relationen sind Gleichheit, Ungleichheit, Vereinigung, Schnitt.


### √úberblick


Wir gehen von Grundraum $\Omega$ aus, mit dem Ereignis $A$ als Teilmenge von $\Omega$: $A \subset \Omega$.


Da wir Ereignisse als Mengen auffassen, verwenden wir im Folgenden die beiden Begriffe synonym.


Dabei nutzen wir u.a. Venn-Diagramme.
Venn-Diagramme eigenen sich, um typische Operationen (Relationen) auf Mengen zu visualisieren. Die folgenden Venn-Diagramme stammen von [Wikipedia (En)](https://en.wikipedia.org/wiki/Venn_diagram).

:::callout-note
### Wozu sind die Venn-Diagramme gut? Warum soll ich die lernen?
Venn-Diagramme zeigen Kreise und ihre √ºberlappenden Teile;
daraus lassen sich R√ºckschl√ºsse auf Rechenregeln f√ºr Wahrscheinlichkeiten ableiten.
Viele Menschen tun sich leichter, 
Rechenregeln visuell aufzufassen als mit Formeln und Zahlen alleine. Aber entscheiden Sie selbst!$\square$
:::


[Diese App](https://www.geogebra.org/m/QZvCMSDs) versinnbildlicht das Rechnen mit Relationen von Ereignissen anhand von Venn-Diagrammen.^[<https://www.geogebra.org/m/QZvCMSDs>]



### Vereinigung von Ereignissen

:::{#def-mengen-verein}
### Vereinigung von Ereignissen
Vereinigt man zwei Ereignisse $A$ und $B$, dann besteht das neue Ereignis $C$ genau aus den Elementarereignissen der vereinigten Ereignisse.
Man schreibt $C = A \cup B$, lies: "C ist A vereinigt mit B".$\square$
:::

@fig-cup zeigt ein Venn-Diagramm zur Verdeutlichung der Vereinigung von Ereignissen.

![$A \cup B$: Vereinigung](img/Venn0111.svg.png){#fig-cup width=25%}

::::::{#exm-mengen-verein}

Um einen (hohen!) Geldpreis zu gewinnen, muss bei ihrem n√§chsten Wurf mindestens eines der beiden Ereignisse $A= {1,2}$ oder $B={2,3}$ eintreten, s. @fig-venn-mengen-verein.

:::::{#fig-venn-mengen-verein}

:::: {.figure-content}


\begin{aligned}
A = \{1,2\} \qquad \boxed{\boxed{1\; 2}\; \color{gray}{ 3\; 4\; 5\; 6}} \\
B = \{2,3\} \qquad  \boxed{1\; \boxed{2\; 3}\; \color{gray}{ 4\; 5\; 6}} \\
\newline
\hline \\
A \cup B = \{1,2,3\} \qquad \boxed{\boxed{1\; 2\; 3}\; \color{gray}{4\; 5\; \boxed{6}}}
\end{aligned}
::::


Beispiel zur Vereinigung zweier Mengen

:::::


::::::


Zur besseren Verbildlichung betrachten Sie mal diese
[Animation zur Vereinigung von Mengen](https://www.geogebra.org/m/GEZV4xXc#material/cmXR8fHN); [Quelle](Geogebra, J. Merschhemke).


In R hei√üt die Vereinigung von Mengen `union()`. Praktisch zum Ausprobieren:

```{r}
A <- c(1, 2)
B <- c(2, 3)

union(A, B)
```



### (Durch-)Schnitt von Ereignissen


:::{#def-mengen-schnitt}
### Schnittmenge von Ereignissen
Die Schnittmenge zweier Ereignisse $A$ und $B$ umfasst genau die Elementarereignisse, 
die Teil beider Ereignisse sind. Man schreibt: $A \cap B.$^[Synonym und k√ºrzer: $AB$ anstelle von $A \cap B$.] Lies: "A geschnitten B". $\square$
:::

@fig-cap zeigt ein Sinnbild zur Schnittmenge zweier Ereignisse.



![$A \cap B$: Schnitt zweier Mengen](img/Venn0001.svg.png){#fig-cap width=25%}


::::::{#exm-mengen-schnitt}

Um einen (hohen!) Geldpreis zu gewinnen, muss bei ihrem n√§chsten Wurf sowohl das Ereignis $A$ = "gerade Augenzahl" als auch $B$ = "Augenzahl gr√∂√üer 4", s. @fig-mengen-schnitt.

:::::{#fig-mengen-schnitt}

:::: {.figure-content}

\begin{align}
& A = \{2,4,6\} \qquad \hfill \boxed{\color{gray}{1}\; \boxed{\color{black}{2}}\; \color{gray}{3}\; \boxed{\color{black}{4}}\; \color{gray}{5}\; \boxed{\color{black}{6}}\;} \\
& B = \{5,6\} \qquad \qquad \hfill  \boxed{ \color{gray}{1\; 2\; 3\; 4\;} \boxed{\color{black}{5\; 6}}} \\
\newline
\hline \\
& A \cap B = \{6\} \qquad \qquad \hfill  \boxed{\color{gray}{1\; 2\; 3\; 4\; 5\;} \color{black}{6}}
\end{align}
::::

Beispiel zum Schnitt zweier Mengen

:::::
::::::


```{r}
A <- c(2, 4, 6)
B <- c(5, 6)
intersect(A, B)
```



:::callout-note
#### Eselsbr√ºcke zur Vereinigungs- und Schnittmenge

Das Zeichen f√ºr eine Vereinigung zweier Mengen kann man leicht mit dem Zeichen f√ºr einen Schnitt zweier Mengen leicht verwechseln; 
daher kommt eine Eselbr√ºcke gelesen, s. @fig-esel.

![Eselsbr√ºcke f√ºr Vereinigungs- und Schnittmenge](img/ven_cup_cap.jpeg){#fig-esel width=55%}
:::


### Komplement√§rereignis


:::{#def-menge-komplement}
### Komplement√§rereignis
Ein Ereignis $A$ ist genau dann ein Komplement√§rereignis zu $B$, 
wenn es genau die Elementarereignisse von $\Omega$ umfasst, 
die nicht Elementarereignis des anderen Ereignisses sind, s. @fig-neg.$\square$
:::


Man schreibt f√ºr das Komplement√§rereignis^[synonym: Komplement] von $A$ oft $\bar{A}$ oder $\neg A$^[manchmal auch $A^C$; *C* wie *c*omplementary event]; lies "Nicht-A" oder "A-quer".


:::::{#exm-mengen-komplement}

Beim normalen W√ºrfelwurf sei $A$ das Ereignis "gerade Augenzahl"; 
das Komplement√§rereignis^[das "Komplement", nicht zu verwechseln mit "Kompliment"] 
ist dann $\neg A$ "ungerade Augenzahl", s. @fig-mengen-komplement.


::::{#fig-mengen-komplement}

::: {.figure-content}


\begin{align}
A = \{2,4,6\} \qquad \hfill \boxed{\color{gray}{1}\; \boxed{\color{black}{2}}\; \color{gray}{3}\; \boxed{\color{black}{4}}\; \color{gray}{5}\; \boxed{\color{black}{6}}\;} \\
\hline \\
\neg A = \{1,3,5\} \qquad  \hfill \boxed{\boxed{\color{black}{1}}\; \color{gray}{2}\; \boxed{\color{black}{3}}\; \color{gray}{4}\; \boxed{\color{black}{5}}\; \color{gray}{6}\; } \\
\end{align}

:::
Ein Beispiel f√ºr ein Komplement

:::::

::::::


![$\bar{A}$: Komplement](img/2560px-Venn1010.svg.png){#fig-neg width=25%}


### Logische Differenz

:::{#def-mengen-diff}
### Logische Differenz
Die logische Differenz der Ereignisse $A$ und $B$ ist das Ereignis, 
das genau aus den Elementarereignissen besteht von $A$ besteht, 
die nicht zugleich Elementarereignis von $B$ sind, s. @fig-setminus.$\square$
:::

Die logische Differenz von $A$ zu $B$ schreibt man h√§ufig so: $A \setminus B$; lies "A minus B".


![$A \setminus B$](img/Venn0100.svg.png){#fig-setminus width=25%}

:::::{#exm-mengen-setminus}

Sei $A$ die Menge "gro√üe Zahlen" mit $A = \{4,5,6 \}$.
Sei $B$ die Menge "gerade Zahlen" mit $B = \{2,4,6\}$.
Wir suchen die logische Differenz, $A \setminus B$, s. @fig-mengen-setminus.

::::{#fig-mengen-setminus}

::: {.figure-content}


\begin{align}
A = \{4,5, 6\} \qquad \hfill \boxed{\color{red}{4}\; \color{green}{5}\; \color{red}{6}} \\
B = \{2,4,6\} \qquad  \hfill \boxed{\color{grey}{2}\; \color{red}{4}\; \color{red}{6}} \\
\hline \\
A \setminus B \qquad \hfill \boxed{\color{green}{5}}
\end{align}
:::

Beispiel f√ºr die logische Differenz

::::
:::::


In R gibt es die Funktion `setdiff()`, die eine Mengendifferenz ausgibt.

```{r}
A <- c(4, 5, 6)
B <- c(2, 4, 6)

setdiff(A, B)
```

ü§Ø Von der Menge $A$ die Menge $B$ abzuziehen, ist etwas anderes, als von $B$ die Menge $A$ abzuziehen.


:::callout-caution
$A \setminus B \ne B \setminus A$.
:::

```{r}
setdiff(B, A)
```


### Vertiefung


[Animation zu Mengenoperationen](https://seeing-theory.brown.edu/compound-probability/index.html)



:::{#exr-komplement}
### Peer Instruction: Das Komplement√§rereignis von der Bestnote

Sie haben eine Statistikklausur bestanden. Mit Bestnote.
Genauer gesagt, haben Sie 100 von 100 Punkten erzielt.
Was ist das Komplement√§rereignis dazu?

A) 99 Punkte
B) 0 Punkte
C) 0-99 Punkte
D) 0-100 Punkte
E) durchgefallen
F) keines der genannten $\square$
:::



## Zufallsvariable

### Grundlagen

:::{#exm-thesis}
Schorsch sucht eine Betreuerin f√ºr seine Abschlussarbeit.
An die ideale Betreuerin setzt er 4 Kriterien an: 
a) klare, schriftliche fixierte Rahmenbedingungen, 
b) viel Erfahrung, 
c) guten Ruf und 
d) interessante Forschungsinteressen.
Je mehr dieser 4 Kriterien erf√ºllt sind, desto besser. 
Schorsch geht davon aus, dass die 4 Kriterien voneinander unabh√§ngig sind (ob eines erf√ºllt ist oder nicht, √§ndert nichts an der Wahrscheinlichkeit, dass ein anderes Kriterium erf√ºllt ist).
Schorsch interessiert sich also f√ºr die *Anzahl* der erf√ºllten Kriterien, also eine Zahl von 0 bis 4.
Er sch√§tzt die Wahrscheinlichkeit f√ºr einen "Treffer" in jedem seiner 4 Kriterien auf 50%.
Viel Gl√ºck, Schorsch!
Sein Zufallsexperiment hat 16 Ausg√§nge (Knoten 16 bis 31), s. @fig-4muenzen und @tbl-schorsch-zufall. Ganz sch√∂n komplex.
Eigentlich w√ºrden ihm ja eine Darstellung mit 5 Ergebnissen, also der "Gutachter-Score" von 0 bis 4 ja reichen. 
Wie k√∂nnen wir die Sache √ºbersichtlicher f√ºr Schorsch machen? @fig-4muenzen ist ein Versuch. $\square$
:::


![Ein Baumdiagramm mit 16 Ausg√§ngen, analog zur 4 M√ºnzw√ºrfen. T: Treffer (Kriterium erf√ºllt). N: Niete (Kriterium nicht erf√ºllt). NNNT w√§re zum Beispiel eine Gutachterin ohne klar fixierte Rahmenbedingungen, ohne viel Erfahrung, mit schlechem Ruf aber mit interessanten Forschungsinteressen.](img/fig_4muenzen.png){#fig-4muenzen width=100%}


```{r}
#| echo: false
#| tbl-cap: Schorschs Zufallsexperiment, Auszug der Elementarereignisse (EE)
#| label: tbl-schorsch-zufall
d <- tibble::tribble(
   ~i, ~Elementarereignis, ~`Pr(EE)`, ~Trefferzahl,
  "1",             "NNNN",    "1/16",          "0",        
  "2",             "NNNT",    "1/16",          "1",             
  "3",             "NNTN",    "1/16",          "1",            
  "4",             "NTNN",    "1/16",          "1",        
  "5",             "TNNN",    "1/16",          "1",            
  "6",             "NNTT",    "1/16",          "2",            
  "‚Ä¶",                "‚Ä¶",       "‚Ä¶",          "‚Ä¶"         
  )


gt(d)
```


Schorsch braucht also eine √ºbersichtlichere Darstellung;
die Zahl der Treffer und ihre Wahrscheinlichkeit w√ºrde ihm ganz reichen.
In vielen Situationen ist man an der *Anzahl der Treffer* interessiert.
Die Wahrscheinlichkeit f√ºr eine bestimmte Trefferanzahl bekommt man einfach durch Addieren der Wahrscheinlichkeiten der zugeh√∂rigen Elementarereignisse, s. @tbl-schorsch-zufall.
Hier kommt die *Zufallsvariable* ins Spiel.
Wir nutzen sie, um die Anzahl der Treffer in einem Zufallsexperiment zu z√§hlen.


:::{#def-zufallsvariable}
### Zufallsvariable
Die Zuordnung der Elementarereignisse eines Zufallsexperiments zu genau einer Zahl 
$\in \mathbb{R}$ nennt man Zufallsvariable.$\square$
:::

Die den Elementarereignissen zugewiesenen Zahlen nennt man *Realisationen* oder *Auspr√§gungen* der Zufallsvariablen.

:::{#exm-lotto}
### Lotto
Ein Lottospiel hat ca. 14 Millionen Elementarereignisse. Die Zufallsvariable "Anzahl der Treffer" hat nur 7 Realisationen: 0,1,...,6.$\square$
:::

Es hat sich eingeb√ºrgert, Zufallszahlen mit $X$ zu bezeichnen (oder anderen Buchstaben weit hinten aus dem Alphabet).

Man schreibt f√ºr eine Zufallsvariable kurz: $X: \Omega \rightarrow \mathbb{R}$.
"X ist eine Zufallsvariable, die jedem Elementarereignis $\omega$ eine reelle Zahl zuordnet."
Um die Vorschrift der Zuordnung genauer zu bestimmen, kann man folgende Kurzschreibweise nutzen:


${\displaystyle X(\omega )={\begin{cases}1,&{\text{wenn }}\omega ={\text{Kopf}},\\[6pt]0,&{\text{wenn }}\omega ={\text{Zahl}}.\end{cases}}}$


@fig-zv stellt diese Abbildung dar.



![Eine Zufallsvariable ist eine Zuordnung der Ereignisse zu einer reellen Zahl (nach einer bestimmten Regel](img/zv_plot.png){#fig-zv width=33%}



Zufallsverteilungen kann im zwei Arten einteilen:

1. diskrete Zufallsvariablen
2. stetige Zufallsvariablen



### Diskrete Zufallsvariable

#### Grundlagen

Eine diskrete Zufallsvariable ist dadurch gekennzeichnet, dass nur bestimmte Realisationen m√∂glich sind, 
zumeist nat√ºrliche Zahlen, wie 0, 1, 2,..., .
@fig-zuv-disk versinnbildlicht die Zufallsvariable des "Gutachter-Scores", s. @exm-thesis.


```{r}
#| echo: false
#| fig-asp: 0.2
#| label: fig-zuv-disk
#| fig-cap: Sinnbild einer diskreten Zufallsvariablen X f√ºr Schorschs Suche nach einer Betreuerin seiner Abschlussarbeit. X gibt den Score der Gutachterin wider.
ggplot(data.frame(x=c(0:4), y = 0), aes(x,y)) +
  geom_point(size = 5, alpha = .7) +
  scale_y_continuous(limits = c(-1,1), breaks = NULL) +
  scale_x_continuous(breaks = 0:4, labels = 0:4) +
  theme_minimal() +
  labs(y = "", x = "")

```



:::{#exm-zv-disk}
### Diskrete Zufallsvariablen

- Anzahl der Bewerbungen bis zum ersten Job-Interview
- Anzahl Anl√§ufe bis zum Bestehen der Statistik-Klausur
- Anzahl der Absolventen an der HS Ansbach pro Jahr
- Anzahl Treffer beim Kauf von Losen
- Anzahl Betriebsunf√§lle
- Anzahl der Produkte in der Produktpalette$\square$
:::



:::{#exm-zweiwuerfel}
Der zweifache W√ºrfelwurf ist ein typisches Lehrbuchbeispiel f√ºr eine diskrete Zufallsvariable.
^[da einfach und deutlich]
Hier ist $S$^[S wie Summe] die Augen*s*umme des zweifachen W√ºrfelwurfs 
und $S$ ist eine Zahl zwischen 2 und 12.
F√ºr jede Realisation $X=x$ kann man die Wahrscheinlichkeit berechnen, 
@fig-zweiwuerfel-vert versinnbildlicht die Wahrscheinlichkeit f√ºr jede Realisation von $X$.$\square$
:::

![Augensumme des zweifachen W√ºrfelwurfs; f√ºr jede Realisation von S ist die zugeh√∂rige Wahrscheinlichkeit dargestellt. Bildquelle: Tim Stellmach, Wikipedia, PD](img/Dice_Distribution_(bar).svg.png){#fig-zweiwuerfel-vert width=50%}



*Wahrscheinlichkeitsverteilungen* dienen dazu, den Realisationen einer Zufallsvariablen eine Wahrscheinlichkeit zuzuordnen.








:::{#def-wvert-disk}
### Diskrete Wahrscheinlichkeitsverteilung
Eine *diskrete* Wahrscheinlichkeitsverteilung der (diskreten) Zufallsvariablen $X$ ordnet jeder der $k$ Auspr√§gungen $X=x$ eine Wahrscheinlichkeit $p$ zu.$\square$
:::

:::{#exm-babies}
### Wahrscheinlichkeit des Geschlechts bei der Geburt
So hat die Variable *Geschlecht eines Babies* die beiden Auspr√§gungen *M√§dchen* und *Junge* mit den Wahrscheinlichkeiten $p_M = 51.2\%$ bzw. $p_J = 48.8\%$, laut einer Studie [@gelman2021].$\square$
:::


Zwischen der deskriptiven Statistik und der Wahrscheinlichkeitstheorie bestehen enge Parallelen, 
@tbl-wkeit-desk stellt einige zentrale Konzepte gegen√ºber.
Bei einer guten Stichproben kann man die Kennwerte der deskriptiven Statistik
als Sch√§tzwerte f√ºr die zugrundeliegende Wahrscheinlichkeit verwenden.

```{r}
#| echo: false
#| label: tbl-wkeit-desk
#| tbl-cap: "Gegen√ºberstellung von Wahrscheinlichkeitstheorie und deskriptiver Statistik"
d <- tibble::tribble(
         ~Wahrscheinlichkeitstheorie,                      ~`Deskriptive Statistik`,
                   "Zufallsvariable",                                   "Merkmal",
                "Wahrscheinlichkeit",               "relative H√§ufigkeit, Anteil",
       "Wahrscheinlichkeitsverteilung",   "einfache relative H√§ufigkeitsverteilung",
               "Verteilungsfunktion", "kumulierte relative H√§ufigkeitsverteilung",
                    "Erwartungswert",                                "Mittelwert",
                           "Varianz",                                   "Varianz"
       )
gt::gt(d)
```




```{r}
#| echo: false
#| message: false
#| warning: false
dice_outcomes <- expand.grid(Die1 = 1:6, Die2 = 1:6)

# Calculate the sum of the two dice for each outcome
dice_outcomes$Sum <- dice_outcomes$Die1 + dice_outcomes$Die2

# Calculate the probability of each sum using the table function
sum_counts <- table(dice_outcomes$Sum)
total_outcomes <- sum(sum_counts)
probabilities <- sum_counts / total_outcomes

twodice <- tibble(
  Augensumme = 2:12,
  p = probabilities) |> 
  mutate(p_cum = cumsum(p))

p_twodice <- 
  ggplot(twodice, aes(x = Augensumme, y = p)) + 
  geom_col() +
  geom_label(aes(y = p, label = round(p, 2), nudge_y = .1)) +
  scale_x_continuous(breaks = 1:12)
```




```{r}
#| echo: false
#| message: false
#| warning: false
#| 
num_trials <- 1000  # You can change this to the desired number of trials

# Simulate repeated throws of two dice
results <- replicate(num_trials, {
  die1 <- sample(1:6, 1, replace = TRUE)  # Simulate the first die
  die2 <- sample(1:6, 1, replace = TRUE)  # Simulate the second die
  c(Die1 = die1, Die2 = die2)  # Return the results as a vector
}) |> 
  t() |> 
  as_tibble() |> 
  mutate(Augensumme  = Die1 + Die2)

# Display

results_count <-
  results |> 
  count(Augensumme) |> 
  mutate(prop = n/num_trials) |> 
  mutate(n_cum = cumsum(n),
         prop_cum = cumsum(prop))

p_sim2dice <-
  ggplot(results_count) +
  aes(x = Augensumme, y = n) +
  geom_col() +
  geom_label(aes(y = n, label = round(prop, 2))) +
  scale_x_continuous(breaks = 1:12)
```





Eine *Verteilung* zeigt, welche Auspr√§gungen eine Variable aufweist und wie h√§ufig bzw. wahrscheinlich diese sind. 
Einfach gesprochen veranschaulicht eine Balken- oder Histogramm eine Verteilung. Man unterscheidet H√§ufigkeitsverteilungen (s. Abb. @fig-2dice-sim) von Wahrscheinlichkeitsverteilungen (Abb. @fig-2dice-prob).




:::: {.columns}

::: {.column width="50%"}
```{r}
#| echo: false
#| fig-cap: Wahrscheinlichkeitsverteilung der Zufallsvariable "Augenzahl im zweifachen W√ºrfelwurf"
#| label: fig-2dice-prob
p_twodice
```

:::

::: {.column width="50%"}
```{r}
#| echo: false
#| fig-cap: (relative und absolute) H√§ufigkeiten des zweifachen W√ºrfelwurfs, 1000 Mal wiederholt
#| label: fig-2dice-sim
p_sim2dice
```
:::

::::



:::{#exm-wert-wuerfel}
### Wahrscheinlichkeitsverteilung eines W√ºrfels
@fig-w-wuerfel zeigt die Wahrscheinlichkeitsverteilung eines einfachen W√ºrfelwurfs.$\square$
:::

![Wahrscheinlichkeitsverteilung eines einfachen W√ºrfelwurfs, Bildrechte: Olex Alexandrov, Wikipedia, PD](img/220px-Fair_dice_probability_distribution.svg.png){#fig-w-wuerfel width=33%}





:::{#exm-w-fun}
Die Wahrscheinlichkeitsverteilung f√ºr $X$ "Augensumme im zweifachen W√ºrfelwurf" ist in @fig-2dice-prob visualisiert.$\square$ 
:::






#### Vertiefung: Verteilungsfunktion



:::{#def-vert-fun}
### Verteilungsfunktion
Die Verteilungsfunktion $F$ gibt die Wahrscheinlichkeit an, 
dass die diskrete Zufallsvariable $X$ eine Realisation annimmt, die kleiner oder gleich $x$ ist.$\square$
:::



Die Berechnung von $F(x)$ erfolgt, indem die Wahrscheinlichkeiten aller m√∂glichen Realisationen $x_i$, 
die kleiner oder gleich dem vorgegebenen Realisationswert $x$ sind, addiert werden:

$F(x) = \sum_{x_ \le x} Pr(X=x_i).$


```{r}
#| echo: false
p_F <- 
  ggplot(twodice, aes(x = Augensumme, y = p_cum)) + 
  geom_col() +
  geom_line() +
  geom_label(aes(label = round(p_cum, 2))) + 
  scale_x_continuous(breaks = 1:12) +
  labs(y = "Verteilungsfunktion F")


y_lab <- "empirische Verteilungsfunktion F emp."

p_F_emp <-
  ggplot(results_count) +
  aes(x = Augensumme, y = prop_cum) +
  geom_col() +
  geom_line() +
  geom_label(aes(y = prop_cum, label = round(prop_cum, 2))) +
  labs(y = y_lab) +
  scale_x_continuous(breaks = 2:12)
```


Die Verteilungsfunktion ist das Pendant zur *kumulierten H√§ufigkeitsverteilung*, vgl. @fig-kum-h-vert und @fig-kum-h-vert-emp:
Was die kumulierte H√§ufigkeitsverteilung f√ºr H√§ufigkeiten ist, ist die Verteilungsfunktion f√ºr Wahrscheinlichkeiten.


:::: {.columns}

::: {.column width="50%"}
```{r}
#| echo: false
#| fig-cap: Verteilungsfunktion $F(X \le x_i)$ f√ºr die Zufallsvariable "Augenzahl im zweifachen W√ºrfelwurf"
#| label: fig-kum-h-vert
p_F
```

:::

::: {.column width="50%"}
```{r}
#| echo: false
#| fig-cap: Empirische Verteilungsfunktion (kumulierte H√§ufigkeitsverteilung) $F(X \le x_i)$ von 1000 zweifachen M√ºnzw√ºrfen
#| label: fig-kum-h-vert-emp
p_F_emp 
```

:::

::::


### Stetige Zufallsvariablen

#### Grundlagen


üì∫ [Verteilungen metrischer Zufallsvariablen](https://www.youtube.com/watch?v=7GqIE4sKDs4&list=PLRR4REmBgpIGgz2Oe2Z9FcoLYBDnaWatN&index=4)

@fig-zv-stetig-groesse versinnbildlicht die stetige Zufallsvariable "K√∂rpergr√∂√üe", die (theoretisch, in Ann√§herung) jeden beliebigen Wert zwischen 0 und (vielleicht) 2 Meter annehmen kann.

```{r echo = FALSE}
#| fig-cap: Sinnbild f√ºr eine stetige Zufallsvariable X "K√∂rpergr√∂√üe"
#| label: fig-zv-stetig-groesse
#| fig-asp: 0.2
 
ggplot(data.frame(x=0, y = 0), aes(x,y)) +
  #geom_point(size = 5, alpha = .7) +
  scale_y_continuous(limits = c(-1,1), breaks = NULL) +
  scale_x_continuous(breaks = c(0, 50, 100, 150, 200)) +
  annotate("segment", x = 0, xend = 200, y = 0, yend = 0, color = "red")  +
  theme_minimal() +
  annotate("label", x = 200, y = 0, label = "...") +
  labs(y = "", x = "")
```


:::{#def-zv-stetig}
### Stetige Zufallsvariable
Eine stetige Zufallsvariable gleicht einer diskreten, nur dass alle Werte im Intervall erlaubt sind.$\square$
:::


:::{#exm-zu-stetig}
- Spritverbrauch
- K√∂rpergewicht von Professoren
- Schnabell√§ngen von Pinguinen
- Geschwindigkeit beim Geblitztwerden$\square$
:::


:::{#exr-bus-42}
### Warten auf den Bus, 42 Sekunden
Sie stehen an der Bushaltestellen und warten auf den Bus.
Langweilig.
Da kommt Ihnen ein Gedanken in den Sinn: 
Wie hoch ist wohl die Wahrscheinlichkeit, dass Sie *exakt* 42 Sekunden auf den Bus warten m√ºssen, s. @fig-p42?
Weiterhin √ºberlegen Sie, dass davon auszugehen ist, dass jede Wartezeit zwischen 0 und 10 Minuten gleich wahrscheinlich ist.
Sp√§testens nach 10 Minuten kommt der Bus, so ist die Taktung (extrem zuverl√§ssig).
Exakt hei√üt *exakt*, also nicht 42.1s, nicht 42.01s, nicht 42.001s, etc. bis zur x-ten Dezimale.$\square$
:::


Nicht so einfach (?). Hingegen ist die Frage, wie hoch die Wahrscheinlichkeit ist, zwischen 0 und 5 Minuten auf den Bus zu warten ($0<x<5$), einfach: Sie betr√§gt 50%, wie man in @fig-bus gut sehen kann.




:::: {.columns}

::: {.column width="50%"}

![Wie gro√ü ist die Wahrscheinlichkeit, zwischen 0 und 5 Minuten auf den Bus zu warten? 50 Prozent!](img/p_bus2.png){#fig-bus}


:::

::: {.column width="50%"}
```{r}
#| echo: false
#| fig-cap: "Wie gro√ü ist die Wahrscheinlichkeit, genau 42 Sekunden auf den Bus zu warten? Hm."
#| label: fig-p42
#| warning: false
#| out-width: 100%

p_bus1 <- 
  uniform_Plot(0, 10) + 
  geom_vline(xintercept = .42, color = "#56B4E9FF", size = 1) +
  annotate("label", x = .42, y = .05, hjust = 0, label = "Pr(X=0.42)=?", color="#56B4E9FF") +
  annotate("point", x = .42, y = 0, size = 5, color = "#56B4E9FF", alpha = .7) +
  annotate("label", x= 5, y = 0.1, label = "f(x) = 1/10", color = "#009E73FF", size = 10)

p_bus1
```
:::

::::

Vergleicht man @fig-p42 und @fig-bus kommt man (vielleicht) zu dem Schluss, dass die Wahrscheinlichkeit exakt 42s auf den Bus zu warten, praktisch Null ist.
Der Grund ist, dass die Fl√§che des Intervalls gegen Null geht, wenn das Intervall immer schm√§ler wird.
Aus diesem Grund kann man bei stetigen Zufallszahlen nicht von einer Wahrscheinlichkeit eines bestimmten Punktes $X=x$ sprechen.
F√ºr einen bestimmten Punkt $X=x$ kann man aber die *Dichte* der Wahrscheinlichkeit angeben.

Was  gleich ist in beiden Situationen ($Pr(X=.42)$ und $Pr(0<x<0.5)$) ist die *Wahrscheinlichkeitsdichte*, $f$.
In @fig-p42 und @fig-bus ist die Wahrscheinlichkeitsdichte gleich, $f=1/10=0.1$.

:::{#def-wdichte}
### Wahrscheinlichkeitsdichte
Die Wahrscheinlichkeitsdichte $f(x)$ gibt an, wie viel Wahrscheinlichkeitsmasse pro Einheit von $X$ an an der Stelle $x$ ist.$\square$
:::


Die Wahrscheinlichkeitsdichte zeigt an, an welchen Stellen $x$ die Wahrscheinlichkeit besonders "geballt" oder "dicht" sind, s. @fig-wdichte-sinnbild.

![Die Wahrscheinlichkeit, dass eine Zufallsvariable einen Wert zwischen und annimmt, entspricht dem Inhalt der Fl√§che unter dem Graph der Wahrscheinlichkeitsdichtefunktion. Bildrechte: 4C, Wikipedia, CC-BY-SA .](img/260px-Integral_as_region_under_curve.svg.png){#fig-wdichte-sinnbild width=33%}



Bei *stetigen* Zufallsvariablen $X$ geht man von unendlich vielen Auspr√§gungen aus; die Wahrscheinlichkeit einer bestimmten Auspr√§gung ist Null: $Pr(X=x_j)=0, \quad j=1,...,+\infty 


:::{#exm-groesse}
### Wahrscheinlichkeitsverteilung f√ºr die K√∂rpergr√∂√üe
So ist die Wahrscheinlichkeit, dass eine Person exakt 166,66666666... cm gro√ü ist, ist (praktisch) Null.
Man gibt stattdessen die *Dichte* der Wahrscheinlichkeit an: Das ist die Wahrscheinlichkeit(smasse) pro  Einheit von $X$.$\square$
:::


F√ºr praktische Fragen berechnet man zumeist die Wahrscheinlichkeit von Intervallen, s. @fig-wdichte-sinnbild.



#### Vertiefung: Verteilungsfunktion

::::: {.columns}

:::: {.column width="50%"}
:::{#def-vert-fun-stetig}
### Verteilungsfunktion
Die Verteilungsfunktion einer stetigen Zufallsvariablen gibt wie im diskreten Fall an,
wie gro√ü die Wahrscheinlichkeit f√ºr eine Realisation kleiner oder gleich einem vorgegebenen Realisationswert $x$ ist.$\square$

Die Verteilungsfunktion $F(x)$ ist analog zur kumulierten H√§ufigkeitsverteilung zu verstehen, vgl. @fig-F-Bus. $\square$
:::


::::

:::: {.column width="50%"}



```{r}
#| echo: false
#| fig-cap: 'Verteilungsfunktion F f√ºr X="Wartezeit auf den Bus"'
#| label: fig-F-Bus
#| fig-asp: 0.4
d <- 
  tibble(x=1:10,
         y= 1:10/10) 

ggplot(d, aes(x,y)) +
  geom_point(alpha = .5) +
  geom_line() +
  scale_x_continuous(breaks = 1:10) +
  theme_minimal()
```




::::

:::::







::: {#exr-wartenbus}

### Peer Instruction: Wieder auf den Bus warten

Sie warten wieder auf den Bus, s. @fig-bus.
Welche Aussage dazu ist *falsch*?

A) Die Wahrscheinlichkeit, genau 5 Minuten zu warten, ist am h√∂chsten.
B) Die Wahrscheinlichkeit, genau 10 Minuten zu warten, ist Null.
C) Mit 100% Wahrscheinlichkeit betr√§gt die Wartezeit zwischen 0 und 10 Minuten.
D) Die Wahrscheinlichkeit 1 Minute zu warten, betr√§gt 10%.
E) Keine der genannten. $\square$
:::


## Aufgaben


Die Webseite [datenwerk.netlify.app](https://datenwerk.netlify.app) stellt eine Reihe von einschl√§gigen √úbungsaufgaben bereit. Sie k√∂nnen die Suchfunktion der Webseite nutzen, 
um die Aufgaben mit den folgenden Namen zu suchen.




### Paper-Pencil-Aufgaben

- [prob-voll-esystem](https://sebastiansauer.github.io/Datenwerk/posts/prob-voll-esystem/)
- [prob-disjunkt](https://sebastiansauer.github.io/Datenwerk/posts/prob-disjunkt/)
- [prob-disjunkt2](https://sebastiansauer.github.io/Datenwerk/posts/prob-disjunkt2/index.html)
- [prob-elementarereignis](https://datenwerk.netlify.app/posts/prob-elementarereignis/index.html)
- [prob-vereinigung](https://datenwerk.netlify.app/posts/prob-vereinigung/index.html)
- [prob-ereignisraum](https://datenwerk.netlify.app/posts/prob-ereignisraum/index.html)
- [prob-sicher-unm√∂glich](https://datenwerk.netlify.app/posts/prob-sicher-unmoÃàglich/)


### Aufgaben, f√ºr die man einen Computer braucht

- [penguins-relationen](https://sebastiansauer.github.io/Datenwerk/posts/penguins-relationen/)
- [penguins-relationen2](https://sebastiansauer.github.io/Datenwerk/posts/penguins-relationen2/)
- [verteilungsfunktion-penguins](https://sebastiansauer.github.io/Datenwerk/posts/verteilungsfunktion-penguins/)


## Literatur

