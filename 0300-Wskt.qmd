

# Wahrscheinlichkeit



<!-- TODO Dieses Kapitel ist zu lang, zu voll. -->

```{r}
#| include: false
library(tidyverse)
library(easystats)
library(gmp)  # function "isprime"
library(titanic)
library(knitr)
library(DT)
library(kableExtra)
```




```{r r-setup}
#| echo: false
#| message: false
theme_set(theme_minimal())
scale_colour_discrete <- function(...) 
  scale_color_okabeito()
```


```{r define-plots-16}
#| echo: false
#| fig-width: 7

plot16a <-
  ggplot(data.frame(A = c(0, 1),
                    B = c(0, 1))) +
  aes(x = A, y = B) +
  annotate("rect", xmin = 0, xmax = 1, ymin = 0, ymax = .5,
           color = "#009E73FF", alpha = .7, fill = NA) +
  annotate("rect", xmin = 0, xmax = 1, ymin = 0.5, ymax = 1,
           color = NA, alpha = .5, fill = "#009E73FF") +
  annotate("rect", xmin = 0, xmax =0.5, ymin = 0, ymax = 1,
           color = "#56B4E9FF", alpha = .7, fill = NA) +
  annotate("rect", xmin = 0.5, xmax = 1, ymin = 0, ymax = 1,
           color = "#56B4E9FF", alpha = .5, fill = "#56B4E9FF") +
  scale_x_continuous(breaks = c(0.25, 0.75), labels = c("¬¨A", "A")) +
  scale_y_continuous(breaks = c(0.25, 0.75), labels = c("¬¨B", "B")) +
  theme_minimal()  +
  theme(axis.text = element_text(size = 18)) +
  labs(x = "", y = "")


plot_pr_b <-
  ggplot(data.frame(A = c(0, 1),
                    B = c(0, 1))) +
  aes(x = A, y = B) +
  annotate("rect", xmin = 0, xmax = 1, ymin = 0, ymax = .5,
           color = "grey", alpha = .7, fill = NA) +
  annotate("rect", xmin = 0, xmax = 1, ymin = 0.5, ymax = 1,
           color = NA, alpha = .5, fill = "#009E73FF") +
  scale_x_continuous(breaks = c(0.25, 0.75), labels = c("¬¨A", "A")) +
  scale_y_continuous(breaks = c(0.25, 0.75), labels = c("¬¨B", "B")) +
  annotate("label", x = .5, y = .75, label = "Pr(B) = 50%") +
  theme_minimal() +
  theme(axis.text = element_text(size = 18)) +
  labs(x = "", y = "")

plot_pr_a <-
  ggplot(data.frame(A = c(0, 1),
                    B = c(0, 1))) +
  aes(x = A, y = B) +
  annotate("rect", xmin = 0, xmax =0.5, ymin = 0, ymax = 1,
           color = "grey", alpha = .7, fill = NA) +
  annotate("rect", xmin = 0.5, xmax = 1, ymin = 0, ymax = 1,
           color = "#56B4E9FF", alpha = .5, fill = "#56B4E9FF") +
  scale_x_continuous(breaks = c(0.25, 0.75), labels = c("¬¨A", "A")) +
  scale_y_continuous(breaks = c(0.25, 0.75), labels = c("¬¨B", "B")) +
  annotate("label", x = .75, y = .5, label = "Pr(A) = 50%") +
  theme_minimal()  +
  theme(axis.text = element_text(size = 18)) +
  labs(x = "", y = "")


plot_pr_ab <-
ggplot(data.frame(A = c(0, 1),
                  B = c(0, 1))) +
  aes(x = A, y = B) +
  annotate("rect", xmin = 0, xmax = 1, ymin = 0, ymax = .5,
           color = "#009E73FF", alpha = .7, fill = NA) +
  annotate("rect", xmin = 0, xmax = 1, ymin = 0.5, ymax = 1,
           color = NA, alpha = .5, fill = "#009E73FF") +
  annotate("rect", xmin = 0, xmax =0.5, ymin = 0, ymax = 1,
           color = "#56B4E9FF", alpha = .7, fill = NA) +
  annotate("rect", xmin = 0.5, xmax = 1, ymin = 0, ymax = 1,
           color = "#56B4E9FF", alpha = .5, fill = "#56B4E9FF") +
  annotate("rect", xmin = 0.5, xmax = 1, ymin = 0.5, ymax = 1,
           color = "#E69F00FF", alpha = .7, fill = NA, linewidth = 2) +
  scale_x_continuous(breaks = c(0.25, 0.75), labels = c("¬¨A", "A")) +
  scale_y_continuous(breaks = c(0.25, 0.75), labels = c("¬¨B", "B")) +
  annotate(geom = "label", x = .75, y = 0.75, label = "A,B", color = "#E69F00FF") +
  theme_minimal()  +
  theme(axis.text = element_text(size = 18)) +
  labs(x = "", y = "")






plot_pr_b_geg_a <-
  ggplot(data.frame(A = c(0, 1),
                    B = c(0, 1))) +
  aes(x = A, y = B) +
  annotate("rect", xmin = 0, xmax = 1, ymin = 0.5, ymax = 1,
           color = "#009E73FF", alpha = .3, fill = NA, linewidth = 2) +
  annotate("rect", xmin = 0, xmax = 1, ymin = 0.5, ymax = 1,
           color = NA, alpha = .5, fill = "NA") +
  # annotate("rect", xmin = 0, xmax =0.5, ymin = 0, ymax = 1,
  #          color = "#56B4E9FF", alpha = .7, fill = NA) +
  annotate("rect", xmin = 0.5, xmax = 1, ymin = 0, ymax = 1,
           color = "#56B4E9FF", alpha = .7, fill = "#56B4E9FF", linewidth = 2) +
  annotate("rect", xmin = 0.5, xmax = 1, ymin = 0.5, ymax = 1,
           color = "#E69F00FF", alpha = .3, fill = "#E69F00FF", linewidth = 2) +
  scale_x_continuous(breaks = c(0.25, 0.75), labels = c("¬¨A", "A")) +
  scale_y_continuous(breaks = c(0.25, 0.75), labels = c("¬¨B", "B")) +
  annotate(geom = "label", x = .75, y = 0.75, label = "B|A", color = "#E69F00FF")  +
  theme(axis.text = element_text(size = 18)) +
  labs(x = "", y = "")


```

## Lernsteuerung


### Position im Modulverlauf

@fig-modulverlauf gibt einen √úberblick zum aktuellen Standort im Modulverlauf.




### √úberblick

Dieses Kapitel hat die Wahrscheinlichkeitstheorie (synonym: Wahrscheinlichkeitsrechnung) bzw. das Konzept der Wahrscheinlichkeit zum Thema.^[Die Wahrscheinlichkeitstheorie bildet zusammen mit der Statistik das Fachgebiet der Stochastik.]
Es geht sozusagen um die Mathematik des Zufalls.



### Wozu brauche ich dieses Kapitel?

Im wirklichen Leben sind Aussagen (Behauptungen) so gut wie nie sicher.

- "Weil sie so schlau ist, ist sie erfolgreich."
- "In Elektroautos liegt die Zukunft."
- "Das klappt sicher, meine Meinung."
- "Der n√§chste Pr√§sident wird XYZ."

Aussagen sind nur *mehr oder weniger* (graduell) sicher.
Wir k√∂nnen die Regeln der Wahrscheinlichkeitslogik verwenden, um den Grad der Sicherheit (von ganz unsicher bis ganz sicher) zu pr√§zisieren.
Daher sagt man auch, Wahrscheinlichkeit sei die Logik der Wissenschaft [@jaynes2003].



### Lernziele

Nach Absolvieren des jeweiligen Kapitels sollen folgende Lernziele erreicht sein.

Sie k√∂nnen ...


- die Grundbegriffe der Wahrscheinlichkeitsrechnung erl√§uternd definieren
- die drei Arten der direkten Ermittlung von Wahrscheinlichkeit erl√§utern
- typische Relationen (Operationen) von Ereignissen anhand von Beispielen veranschaulichen
- mit Wahrscheinlichkeiten rechnen



### Begleitliteratur

Lesen Sie zur Begleitung dieses Kapitels @bourier2011, Kap. 2-4. 


### Eigenstudium


:::{.callout-important}
Dieses Kapitel ist selbst√§ndig im Eigenstudium vorzubereiten vor dem Unterricht.
Lesen Sie dazu die angegebene Literatur.$\square$
:::


### Pr√ºfungsrelevanter Stoff

Der Stoff dieses Kapitels deckt sich (weitgehend) mit @bourier2011, Kap. 2-4. 
Weitere √úbungsaufgaben finden Sie im dazugeh√∂rigen √úbungsbuch, @bourier2022.

:::callout-note
In Ihrer [Hochschul-Bibliothek kann das Buch als Ebook verf√ºgbar](https://fantp20.bib-bvb.de/TouchPoint/singleHit.do?methodToCall=showHit&curPos=3&identifier=2_SOLR_SERVER_1157422278) sein. 
Pr√ºfen Sie, ob Ihr Dozent Ihnen weitere Hilfen im [gesch√ºtzten Bereich (Moodle)](https://moodle.hs-ansbach.de/mod/resource/view.php?id=136047) eingestellt hat.$\square$
:::





### Zentrale Begriffe


#### Grundbegriffe

- Zufallsvorgang (Zufallsexperiment)
- Elementarereignis
- Ereignisraum
- Zufallsereignis (zuf√§lliges Ereignis)
- Sicheres Ereignis
- Unm√∂gliches Ereignis


#### Wahrscheinlichkeitsbegriffe

- Klassische Wahrscheinlichkeit (LaPlace'sche Wahrscheinlichkeit)
- Statistische (empirische) Wahrscheinlichkeitsermittlung
- Subjektive (Bayes) Wahrscheinlichkeitsermittlung


#### Wahrscheinlichkeitsrelationen

- Vereinigung von Ereignissen
- Schnitt(menge) von Ereignissen
- Komplement√§rereignis
- Vollst√§ndiges Ereignissystem
- Anforderungen an eine Definition von Wahrscheinlichkeit


#### Wahrscheinlichkeitsrechnung

- Allgemeiner Additionsssatz
- Disjunkte Ereignisse
- Additionssatz f√ºr disjunkte Ereignisse
- Bedingte Wahrscheinlichkeit
- (Stochastische) Unabh√§ngigkeit
- Baumdiagramm f√ºr gemeinsame Wahrscheinlichkeit
- Allgemeiner Multiplikationssatz
- Multiplikationssatz f√ºr unabh√§ngige Ereignisse
- Totale Wahrscheinlichkeit
- Satz von Bayes




### Begleitvideos


- [Video zum Thema Wahrscheinlichkeit](https://youtu.be/rR6NspapEyo)




<!-- ## Unterst√ºtzung: Wahrscheinlichkeit in Bildern -->

<!-- Wahrscheinlichkeit in Bildern: zur einfachen Erschlie√üung des Materials, -->
<!-- ein Unterst√ºtzungsangebot. -->


<!-- Im Folgenden sind einige Schl√ºsselbegriffe und -regeln in (ver-)einfach(t)er Form schematisch bzw. visuell dargestellt mit dem Ziel, den Stoff einfacher zu erschlie√üen. -->







## Grundbegriffe





:::{#exm-muenz}
Klassisches Beispiel f√ºr einen Zufallsvorgang ist das (einmalige oder mehrmalige) Werfen einer M√ºnze.$\square$

Werfen Sie eine M√ºnze!
Diese hier zum Beispiel:

![](img/1024px-Coin-155597.svg.png){width=10% fig-align="center"}

[Quelle: By OpenClipartVectors, CC0]( https://pixabay.com/pt/moeda-euro-europa-fran%C3%A7a-dinheiro-155597)



Wiederholen Sie den Versuch 10 Mal.

Das reicht Ihnen nicht? Okay, wiederholen Sie den Versuch 100, nein 1000, nein: $10^6$ Mal.^[$10^6 = 1000000$]

Notieren Sie als Ergebnis, wie oft die Seite mit der Zahl oben liegen kommt ("Treffer").$\square$
:::


Oder probieren Sie die [App der Brown University](https://seeing-theory.brown.edu/basic-probability/index.html#section1), 
wenn Sie keine Sehnenscheidenentz√ºndung bekommen wollen.


:::{#def-zufall}
### Zufallsvorgang
Ein *Zufallsvorgang* oder *Zufallsexperiment* ist eine einigerma√üen klar beschriebene T√§tigkeit, deren Ergebnis nicht sicher ist. 
Allerdings ist die Menge m√∂glicher Ergebnisse bekannt und die Wahrscheinlichkeit f√ºr alle Ergebnisse kann quantifiziert werden.$\square$
:::


:::{#exr-zufall2}
Nennen Sie Beispiele f√ºr Zufallsvorg√§nge!\square^[Beispiele f√ºr Zufallsexperimente sind das Werfen einer M√ºnze, das Ziehen einer Karte aus einem Kartenspiel, 
das Messen eines Umweltph√§nomens wie der Temperatur oder die Anzahl der Kunden, 
die einen Laden betreten. 
In jedem dieser F√§lle sind die m√∂glichen Ergebnisse nicht im Voraus bekannt und h√§ngen von nicht komplett bekannten Faktoren ab.]
:::


:::callout-caution
Zufall hei√üt nicht, dass ein Vorgang keine Ursachen h√§tte. 
So gehorcht der Fall einer M√ºnze komplett den Gesetzen der Gravitation. 
W√ºrden wir diese Gesetze und die Ausgangsbedingungen (Luftdruck, Fallh√∂he, Oberfl√§chenbeschaffenheit, Gewichtsverteilungen, ...) exakt kennen, k√∂nnten wir theoretisch sehr genaue Vorhersagen machen. 
Der "Zufall" w√ºrde aus dem M√ºnzwurf verschwinden. Man sollte "Zufall" also besser verstehen als "unbekannt".$\square$
::::


:::{#exr-w√ºrfel-geo}
[Mit dieser App](https://www.geogebra.org/m/cbqee8h7) k√∂nnen Sie W√ºrfelw√ºrfe simulieren und die Ausg√§nge dieses Zufallsexperiments beobachten.$\square$
:::



:::{#def-ereignisraum}
### Ereignisraum
Die m√∂glichen Ergebnisse eines Zufallvorgangs fasst man als Menge mit dem Namen *Ereignisraum*^[leider gibt es eine F√ºlle synonymer Namen: *Ereignisraum*, *Elementarereignisraum*, *Ergebnisraum* oder *Grundraum*] zusammen. 
Man verwendet den griechischen Buchstaben $\Omega$ f√ºr diese Menge.
Die Elemente $\omega$ (kleines Omega) von $\Omega$ nennt man *Ergebnisse*.$\square$
:::

:::{#exm-grundraum}
Beobachtet man beim W√ºrfelwurf (s. @fig-wuerfel) die oben liegende Augenzahl, so ist 



$$\Omega = \{ 1,2,3,4,5,6 \} = \{‚öÄ, ‚öÅ, ‚öÇ, ‚öÉ, ‚öÑ, ‚öÖ\}$$

ein nat√ºrlicher Grundraum [@henze2019].$\square$
:::


Die Wahrscheinlichkeitsrechnung baut auf der Mengenlehre auf, daher wird die Notation  der Mengenlehre hier verwendet.



::: {.content-visible when-format="html"}
![Ein (sechsseitiger) W√ºrfel, Bildquelle: Peter Steinberg, Wikipedia, CC-BY-](img/120px-Hexahedron-slowturn.gif){#fig-wuerfel width=10%}

:::

::: {.content-visible unless-format="html"}


![Ein (sechsseitiger) W√ºrfel](img/Dice_2005.jpg){width=33%}

[Bildquelle: CC BY-SA 3.0](https://commons.wikimedia.org/w/index.php?curid=474827)
:::







:::{#def-ereignis}
### Ereignis
Jede Teilmenge^[$A$ ist eine Teilmenge von $B$, 
wenn alle Elemente von $A$ auch Teil von $B$ sind.] von $\Omega$ hei√üt *Ereignis*; $A \subseteq \Omega$ .$\square$
:::



:::{#exm-ereignis}
Beim Mensch-√§rger-dich-nicht Spielen habe ich eine 6 geworfen.^[Schon wieder.]
Das Nennen wir das Ereignis $A$: "Augenzahl 6 liegt oben" und schreiben in Kurzform:

$A= \{6\}\square$
:::


:::{#exm-muenzwurf}
Sie werfen eine M√ºnze (Sie haben keinen Grund, an ihrer Fairness zu zweifeln). "Soll ich jetzt lernen f√ºr die Klausur (Kopf) oder lieber zur Party gehen (Zahl)?"

@fig-baummuenz1 zeigt die m√∂glichen Ausg√§nge (T wie Treffer (Party) und N  (Niete, Lernen)) dieses Zufallexperiments.

```{mermaid}
%%| label: fig-baummuenz1
%%| fig-cap: Sie werfen eine M√ºnze. Party oder Lernen???
flowchart LR
 M[Sie werfen die M√ºnze] --> T["T (Treffer) ü•≥"]
  M --> N["N (Niete) üìö"]
```

Das Ereignis *Zahl* ist eingetreten! Treffer! Gl√ºck gehabt!^[?]$\square$
:::






:::{#def-unm-sich}
### Unm√∂gliches und sicheres Ereignis
Die leere Menge $\varnothing$ hei√üt das *um√∂gliche*, der Grundraum $\Omega$ hei√üt das *sichere Ereignis*. $\square$
:::

:::{#exm-unm}
### Unm√∂gliches Ereignis
Alois behauptet, er habe mit seinem W√ºrfel eine 7 geworfen.
Schorsch erg√§nzt, sein W√ºrfel liege auf einer Ecke, so dass keine Augenzahl oben liegt.
Draco hat seinen W√ºrfel runtergeschluckt. 
Dracos und Alois' Ereignisse sind *unm√∂gliche Ereignisse*, zumindest nach unserer Vorstellung des Zufallsexperiments.$\square$
:::


:::{#exm-sicher}
### Sicheres Ereignis
Nach dem der W√ºrfel geworfen wurde, liegt eine Augenzahl zwischen 1 und 6 oben.$\square$
:::


:::{#def-elementarereignis}
### Elementarereignis

Jede einelementige Teilmenge $\{\omega\}$ von $\Omega$ hei√üt *Elementarereignis* (h√§ufig mit $A$ bezeichnet).
^[Ein *Ergebnis* ist ein Element von $\Omega$. Elementarereignisse sind die einelementigen Teilmengen von $\Omega$. Konzeptionell sind die beiden Begriffe sehr √§hnlich, vgl. <https://de.wikipedia.org/wiki/Ergebnis_(Stochastik)>. Wir werden uns hier auf den Begriff *Elementarereignis* konzentrieren und den Begriff *Ergebnis* nicht weiter verwenden.] $\square$
:::


:::{#exm-elementarereignis}
### Elementarereignis

- Sie spielen Mensch-√§rger-dich-nicht. Und brauchen dringend eine `6`. Sie w√ºrfeln. Das Ereignis $A = \{1\}$ tritt ein.^[Na toll.]

- Sie schreiben eine Statistik-Klausur. Irgendwie haben Sie das Gef√ºhl, das Ergebnis sei ein Zufallsexperiment... Jedenfalls k√∂nnen nach Adam Riese zwei Dinge passieren: $\Omega= \{\text{bestehen, nicht bestehen}\}$.
Das erste der beiden Elementarereignisse tritt ein. Yeah!

- Sie f√ºhren eine Studie durch zur Wirksamkeit einer Lern-App. Es ist nicht klar, ob die App wirklich was bringt f√ºr den Lernerfolg. Vereinfacht gesprochen ist der Grundraum dieses Experiments: $\Omega = \{\text{schadet, bringt nichts, n√ºtzt}\}$.
Die Daten sprechen f√ºr das Ereignis $A = \{\text{bringt nichts}\}$.
:::




:::{#def-vollereignis}
### Vollst√§ndiges Ereignissystem
Wird der Grundraum $\Omega$ vollst√§ndig in paarweis disjunkte Ereignisse zerlegt, so bilden diese Ereignisse ein vollst√§ndiges Ereignissystem, s. @fig-vollereignis.$\square$
:::


![Zerlegung des Grundraums in ein vollst√§ndiges Ereignissystem](img/vollereignis.png){#fig-vollereignis width=50%}



:::::{#exm-vollereig1}
Sei $\Omega$ der typische Ereignisraum des W√ºrfelwurfs. Wir zerlegen den Grundraum in zwei Ereignisse, $A$ "gerade Zahlen", und $B$ "ungerade Zahlen". 
Damit haben wir ein vollst√§ndiges Ereignissystem erstellt, s. @fig-complete-event-system1.


::::{#fig-complete-event-system1}

::: {.figure-content}


\begin{align}
A = \{2,4,6\} \qquad \hfill \boxed{\color{gray}{1}\; \boxed{\color{black}{2}}\; \color{gray}{3}\; \boxed{\color{black}{4}}\; \color{gray}{5}\; \boxed{\color{black}{6}}\;} \\
B = \{1,3,5\} \qquad  \hfill \boxed{\boxed{\color{black}{1}}\; \color{gray}{2}\; \boxed{\color{black}{3}}\; \color{gray}{4}\; \boxed{\color{black}{5}}\; \color{gray}{6}\; } \\
\hline \\
\Omega = \{1,2,3,4,5,6\}  \qquad  \hfill \boxed{1\; 2\; 3\; 4\; 5\; 6 } 

\end{align}
:::

::::

Ein Beispiel f√ºr ein vollst√§ndiges Ereignissystem

:::::


:::::{#exm-vollereig2}
Sei $\Omega$ der typische Ereignisraum des W√ºrfelwurfs. Wir zerlegen den Grundraum in zwei Ereignisse, $A$ "1,2,3", und $B$ "4,5,6". 
Damit haben wir ein vollst√§ndiges Ereignissystem erstellt, s. @fig-complete-event-system1.

::::{#fig-fig-complete-event-system2}

::: {.figure-content}


\begin{align}
A = \{1,2,3\} \qquad \qquad \hfill  \boxed{\boxed{ \color{black}{1\; 2\; 3}}\; \color{gray}{4\; 5\; 6}} \\
B = \{4,5, 6\} \qquad \qquad  \hfill \boxed{\color{gray}{1 \; 2 \; 3}\; \boxed{\color{black}{4\; 5 \; 6}}} \\

\newline
\hline \\
\Omega = \{1,2,3,4,5,6\} \qquad \qquad \hfill  \boxed{1\; 2\; 3\; 4\; 5\;6}
\end{align}
:::

Noch ein Beispiel f√ºr ein vollst√§ndiges Ereignissystem

::::
:::::


:::{#def-macht}
### M√§chtigkeit
Die Anzahl der Elementarereignisse eines Ereignismraums nennt man die M√§chtigkeit (des Ereignisraums).^[Die Menge aller Teilmengen einer Menge $A$ nennt man die *Potenzmenge* $\mathcal{P}(A)$, vgl. [hier](https://de.wikipedia.org/wiki/Datei:Hasse_diagram_of_powerset_of_3.svg).]$\square$
:::


Die M√§chtigkeit von $\Omega$ bezeichnet man mit dem Symbol $|\Omega|$.

:::{#exm-macht}
Beim Wurf eines W√ºrfels mit $\Omega=\{1,2,3,4,5,6\}$ gibt es 6 Elementarereignisse. 
Die M√§chtigkeit ist also 6: $|\Omega|=6$.$\square$
:::

## Direkte Ermittlung von Wahrscheinlichkeiten

Wir haben schon mit @def-wskt eine erste Definition von Wahrscheinlichkeit versucht. 
Jetzt gehen wir die Sache noch etwas n√§her an.

### Formallogische Wahrscheinlichkeit

Die formallogische Konzeption von Wahrscheinlichkeit sieht Wahrscheinlichkeit als Erweiterung der formalen Logik [@jaynes2003].
^[Manchmal wird diese Art der Wahrscheinlichkeit auch *epistemologische* Wahrscheinlichkeit genannt.] 
In der formalen Logik ist ein Ereignis entweder *falsch* oder *wahr*. 
In der formallogischen Konzeption wird der Platz zwischen "falsch" (0) und "richtig" 
(1)
als die Wahrscheinlichkeit $0<p<1$, gesehen [@briggs2016], s. @fig-prob-logic.


![Wahrscheinlichkeit als Erweiterung der Logik](img/prob-logic.png){#fig-prob-logic}

Nach dieser "Wahrscheinlichkeitslogik" kann man ein Ereignis, 
von dessen Eintreten man "wenig √ºberzeugt" ist, z.B. mit 0.2 quantifizieren. 
Hingegen einem Ereignis, von man "recht sicher" ist, mit 0.8 quantifizieren, s. @fig-prob-logic2.


![Ein Ereignis von dessen Eintreten man gering bzw. stark √ºberzeugt ist](img/prob-logic2.png){#fig-prob-logic2}


:::{#def-indifferenzprinzip}
### Indifferenzprinzip

Das Indifferenzprinzip (synonym: Prinzip des unzureichenden Grundes) besagt, 
dass in Abwesenheit jeglicher Informationen, die bestimmte Ereignisse bevorzugen oder benachteiligen w√ºrden, 
alle m√∂glichen Ereignisse als gleich wahrscheinlich angesehen werden sollten. $\square$
:::


Vor uns liegt ein W√ºrfel. Schlicht, ruhig, unbesonders.
Wir haben keinen Grund anzunehmen, dass eine seiner $n=6$ Seiten bevorzugt nach oben zu liegen kommt. 
Jedes der sechs Elementarereignisse ist uns gleich plausibel;
der W√ºrfel erscheint uns fair.
In Ermangelung weiteres Wissens zu unserem W√ºrfel gehen wir schlicht davon aus, dass jedes der $n$ Elementarereignis gleich wahrscheinlich ist.
Es gibt keinerlei Notwendigkeit, den W√ºrfel in die Hand zu nehmen,
um zu einer Wahrscheinlichkeitsaussage auf diesem Weg zu kommen.
Nat√ºrlich *k√∂nnten* wir unsere Auffassung eines fairen W√ºrfels testen,
aber auch ohne das Testen k√∂nnen wir eine stringente Aussage (basierend auf dem Indifferenzprinzip (s. @def-indifferenzprinzip) der $n$ Elementarereignisse) zur Wahrscheinlichkeit eines bestimmten (Elementar-)Ereignisses $A$ kommen [@briggs2016], s. @thm-briggs.


:::{#thm-briggs}

### Indifferenzprinzip

$$Pr(A) = \frac{1}{n}= \frac{1}{|\Omega|} \quad \square$$
:::

:::{#exm-briggs}
Sei $A$ = "Der W√ºrfel wird beim n√§chsten Wurf eine 6 zeigen."
Die Wahrscheinlichkeit f√ºr $A$ ist $1/6. \square$
:::


:::{#def-laplace}
### Laplace-Experimt
Ein Zufallsexperiment, bei dem alle Elementarereignisse dieselbe Wahrscheinlichkeit haben, nennt man man ein *Laplace-Experiment*, s. @thm-laplace. $\square$
:::

In Erweiterung von @thm-briggs k√∂nnen wir f√ºr ein Laplace-Experiment schreiben, s. @thm-laplace.

:::{#thm-laplace}

### Laplace-Experiment

$$Pr(A)=\frac{\text{Anzahl Treffer}}{\text{Anzahl m√∂glicher Ergebnisse}} \quad \square$$
:::


### Frequentistische Wahrscheinlichkeit

In Ermangelung einer Theorie zum Verhalten eines (uns) unbekannten Zufallsvorgangs und unter der Vermutung, dass die Elementarereignisse nicht gleichwahrscheinlich sind, bleibt uns ein einfacher (aber aufw√§ndiger) Ausweg: Ausprobieren.

Angenommen, ein Statistik-Dozent, bekannt f√ºr seine Vorliebe zum Gl√ºcksspiel und 
mit scheinbar endlosen Gl√ºcksstr√§hnen (er wirft andauernd eine 6), 
hat seinen Lieblingsw√ºrfel versehentlich liegen gelassen. 
Das ist *die* Gelegenheit!
Sie greifen sich den W√ºrfel, und ... Ja, was jetzt?
Nach kurzer √úberlegung kommen Sie zum Entschluss, den W√ºrfel einem "Praxistest" zu unterziehen: 
Sie werfen ihn 1000 Mal (Puh!) und z√§hlen den Anteil der `6`.
Falls der W√ºrfel fair ist, m√ºsste gelten $Pr(A=6)=1/6\approx .17$. Schauen wir mal!



```{r}
#| echo: false
n <- 1e3

set.seed(42)
wuerfel_oft <- 
  sample(x = 1:6, size = n, replace = TRUE) 


wuerfel_tab <-
  tibble(
    id = 1:n,
    x = wuerfel_oft,
    ist_6 = ifelse(x == 6, 1, 0),
    ist_6_cumsum = cumsum(ist_6) / id
  )

```


Und hier der Anteil von  `6` im Verlauf unserer W√ºrfe, s. @fig-wuerfel.


```{r}
#| label: fig-wuerfel
#| fig-cap: "Das Gesetz der gro√üen Zahl am Beispiel der Stabilisierung des Trefferanteils beim wiederholten W√ºrfelwurf"
#| fig-asp: 0.5
#| echo: false

wuerfel_tab %>% 
  slice_head(n = 1e3) %>% 
  ggplot() +
  aes(x = id, y = ist_6_cumsum) +
  geom_hline(yintercept = 1/6, color = "grey80", size = 3) +
  geom_line() +
  labs(x = "Nummer des W√ºrfelwurfs",
       y = "Kummulierte H√§ufigkeit einer Sechs") +
  annotate("label", x = 1000, y = 1/6, label = "0.17")
```

Hm, auf den ersten Blick ist kein (starkes) Anzeichen f√ºr Schummeln bzw. einen gezinkten W√ºrfel zu finden.


### Subjektive Wahrscheinlichkeit

Um subjektiv zu einer Wahrscheinlichkeit zu kommen, sagt man einfach seine Meinung.
Das h√∂rt sich nat√ºrlich total plump an. 
Und tats√§chlich besteht die Gefahr, dass die so ermittelten Wahrscheinlichkeiten aus der Luft gegriffen, also haltlos, sind.

Allerdings kann diese Art von Wahrscheinlichkeitsermittlung auch sehr wertvoll sein.
In komplizierten Situation im echten Leben^[die sog. "Praxis"] kommt man oft in die Situation, 
dass weder die formallogischen noch die frequentistische Variante verwendet werden kann.
Dann muss man auf Sch√§tzungen, Vorwissen, Erfahrung, theoretischen √úberlegungen etc. zur√ºckgreifen.





### Kolmogorovs Wahrscheinlichkeitsdefinition {#sec-kolmogorov}

Wir richten eine Reihe von Forderungen an eine Definition von bzw. an das Rechnen mit Wahrscheinlichkeiten, die direkt plausibel erscheinen:^[Ein Herr Kolmogorov hat das mal aufgeschrieben.]

1. *Nichtnegativit√§t*: Die Wahrscheinlichkeit eines Ereignisses kann nicht negativ sein.
2. *Normierung*: Das sichere Ereignis hat die Wahrscheinlichkeit 1 bzw. 100%: $Pr(\Omega)=1$; das unm√∂gliche Ereignis hat die Wahrscheinlichkeit 0: $Pr(\emptyset)=0$.
3. *Additivit√§t*. Sind $A$ und $B$ disjunkt, dann ist die Wahrscheinlichkeit, 
dass mindestens eines der beiden Ereignisse eintritt ($A\cup B$) gleich der Summe der beiden Einzelwahrscheinlichkeiten von $A$ und $B$.

## Relationen von Ereignissen

F√ºr das Rechnen mit Wahrscheinlichkeiten ist es hilfreich, ein paar Werkzeuge zu kennen, die wir uns im Folgenden anschauen.

:::{#def-relation}
### Relation
Eine Relation (zweier Ereignisse) bezeichnet die Beziehung, 
in der die beiden Ereignisse zueinander stehen. $\square$
:::

Typische Relationen sind Gleichheit, Ungleichheit, Vereinigung, Schnitt.


### √úberblick


Wir gehen von Grundraum $\Omega$ aus, mit dem Ereignis $A$ als Teilmenge von $\Omega$: $A \subset \Omega$.


Da wir Ereignisse als Mengen auffassen, verwenden wir im Folgenden die beiden Begriffe synonym.


Dabei nutzen wir u.a. Venn-Diagramme.
Venn-Diagramme eigenen sich, um typische Operationen (Relationen) auf Mengen zu visualisieren. Die folgenden Venn-Diagramme stammen von [Wikipedia (En)](https://en.wikipedia.org/wiki/Venn_diagram).

:::callout-note
### Wozu sind die Venn-Diagramme gut? Warum soll ich die lernen?
Venn-Diagramme zeigen Kreise und ihre √ºberlappenden Teile;
daraus lassen sich R√ºckschl√ºsse auf Rechenregeln f√ºr Wahrscheinlichkeiten ableiten.
Viele Menschen tun sich leichter, 
Rechenregeln visuell aufzufassen als mit Formeln und Zahlen alleine. Aber entscheiden Sie selbst!$\square$
:::


[Diese App](https://www.geogebra.org/m/QZvCMSDs) versinnbildlicht das Rechnen mit Relationen von Ereignissen anhand von Venn-Diagrammen.^[<https://www.geogebra.org/m/QZvCMSDs>]



### Vereinigung von Ereignissen

:::{#def-mengen-verein}
### Vereinigung von Ereignissen
Vereinigt man zwei Ereignisse $A$ und $B$, dann besteht das neue Ereignis $C$ genau aus den Elementarereignissen der vereinigten Ereignisse.
Man schreibt $C = A \cup B$, lies: "C ist A vereinigt mit B".$\square$
:::

@fig-cup zeigt ein Venn-Diagramm zur Verdeutlichung der Vereinigung von Ereignissen.

![$A \cup B$: Vereinigung](img/Venn0111.svg.png){#fig-cup width=25%}

::::::{#exm-mengen-verein}

Um einen (hohen!) Geldpreis zu gewinnen, muss bei ihrem n√§chsten Wurf mindestens eines der beiden Ereignisse $A= {1,2}$ oder $B={2,3}$ eintreten, s. @fig-venn-mengen-verein.

:::::{#fig-venn-mengen-verein}

:::: {.figure-content}


\begin{aligned}
A = \{1,2\} \qquad \boxed{\boxed{1\; 2}\; \color{gray}{ 3\; 4\; 5\; 6}} \\
B = \{2,3\} \qquad  \boxed{1\; \boxed{2\; 3}\; \color{gray}{ 4\; 5\; 6}} \\
\newline
\hline \\
A \cup B = \{1,2,3\} \qquad \boxed{\boxed{1\; 2\; 3}\; \color{gray}{4\; 5\; \boxed{6}}}
\end{aligned}
::::


Beispiel zur Vereinigung zweier Mengen

:::::


::::::


Zur besseren Verbildlichung betrachten Sie mal diese
[Animation zur Vereinigung von Mengen](https://www.geogebra.org/m/GEZV4xXc#material/cmXR8fHN); [Quelle](Geogebra, J. Merschhemke).


In R hei√üt die Vereinigung von Mengen `union()`. Praktisch zum Ausprobieren:

```{r}
A <- c(1, 2)
B <- c(2, 3)

union(A, B)
```



### (Durch-)Schnitt von Ereignissen


:::{#def-mengen-schnitt}
### Schnittmenge von Ereignissen
Die Schnittmenge zweier Ereignisse $A$ und $B$ umfasst genau die Elementarereignisse, 
die Teil beider Ereignisse sind. Man schreibt: $A \cap B.$^[Synonym und k√ºrzer: $AB$ anstelle von $A \cap B$.] Lies: "A geschnitten B". $\square$
:::

@fig-cap zeigt ein Sinnbild zur Schnittmenge zweier Ereignisse.



![$A \cap B$: Schnitt zweier Mengen](img/Venn0001.svg.png){#fig-cap width=25%}


::::::{#exm-mengen-schnitt}

Um einen (hohen!) Geldpreis zu gewinnen, muss bei ihrem n√§chsten Wurf sowohl das Ereignis $A$ = "gerade Augenzahl" als auch $B$ = "Augenzahl gr√∂√üer 4", s. @fig-mengen-schnitt.

:::::{#fig-mengen-schnitt}

:::: {.figure-content}

\begin{align}
& A = \{2,4,6\} \qquad \hfill \boxed{\color{gray}{1}\; \boxed{\color{black}{2}}\; \color{gray}{3}\; \boxed{\color{black}{4}}\; \color{gray}{5}\; \boxed{\color{black}{6}}\;} \\
& B = \{5,6\} \qquad \qquad \hfill  \boxed{ \color{gray}{1\; 2\; 3\; 4\;} \boxed{\color{black}{5\; 6}}} \\
\newline
\hline \\
& A \cap B = \{6\} \qquad \qquad \hfill  \boxed{\color{gray}{1\; 2\; 3\; 4\; 5\;} \color{black}{6}}
\end{align}
::::

Beispiel zum Schnitt zweier Mengen

:::::
::::::


```{r}
A <- c(2, 4, 6)
B <- c(5, 6)
intersect(A, B)
```



:::callout-note
#### Eselsbr√ºcke zur Vereinigungs- und Schnittmenge

Das Zeichen f√ºr eine Vereinigung zweier Mengen kann man leicht mit dem Zeichen f√ºr einen Schnitt zweier Mengen leicht verwechseln; 
daher kommt eine Eselbr√ºcke gelesen, s. @fig-esel.

![Eselsbr√ºcke f√ºr Vereinigungs- und Schnittmenge](img/ven_cup_cap.jpeg){#fig-esel width=55%}
:::


### Komplement√§rereignis


:::{#def-menge-komplement}
### Komplement√§rereignis
Ein Ereignis $A$ ist genau dann ein Komplement√§rereignis zu $B$, 
wenn es genau die Elementarereignisse von $\Omega$ umfasst, 
die nicht Elementarereignis des anderen Ereignisses sind, s. @fig-neg.$\square$
:::


Man schreibt f√ºr das Komplement√§rereignis^[synonym: Komplement] von $A$ oft $\bar{A}$ oder $\neg A$^[manchmal auch $A^C$; *C* wie *c*omplementary event]; lies "Nicht-A" oder "A-quer".


:::::{#exm-mengen-komplement}

Beim normalen W√ºrfelwurf sei $A$ das Ereignis "gerade Augenzahl"; 
das Komplement√§rereignis^[das "Komplement", nicht zu verwechseln mit "Kompliment"] 
ist dann $\neg A$ "ungerade Augenzahl", s. @fig-mengen-komplement.


::::{#fig-mengen-komplement}

::: {.figure-content}


\begin{align}
A = \{2,4,6\} \qquad \hfill \boxed{\color{gray}{1}\; \boxed{\color{black}{2}}\; \color{gray}{3}\; \boxed{\color{black}{4}}\; \color{gray}{5}\; \boxed{\color{black}{6}}\;} \\
\hline \\
\neg A = \{1,3,5\} \qquad  \hfill \boxed{\boxed{\color{black}{1}}\; \color{gray}{2}\; \boxed{\color{black}{3}}\; \color{gray}{4}\; \boxed{\color{black}{5}}\; \color{gray}{6}\; } \\
\end{align}

:::
Ein Beispiel f√ºr ein Komplement

:::::

::::::


![$\bar{A}$: Komplement](img/2560px-Venn1010.svg.png){#fig-neg width=25%}


### Logische Differenz

:::{#def-mengen-diff}
### Logische Differenz
Die logische Differenz der Ereignisse $A$ und $B$ ist das Ereignis, 
das genau aus den Elementarereignissen besteht von $A$ besteht, 
die nicht zugleich Elementarereignis von $B$ sind, s. @fig-setminus.$\square$
:::

Die logische Differenz von $A$ zu $B$ schreibt man h√§ufig so: $A \setminus B$; lies "A minus B".


![$A \setminus B$](img/Venn0100.svg.png){#fig-setminus width=25%}

:::::{#exm-mengen-setminus}

Sei $A$ die Menge "gro√üe Zahlen" mit $A = \{4,5,6 \}$.
Sei $B$ die Menge "gerade Zahlen" mit $B = \{2,4,6\}$.
Wir suchen die logische Differenz, $A \setminus B$, s. @fig-mengen-setminus.

::::{#fig-mengen-setminus}

::: {.figure-content}


\begin{align}
A = \{4,5, 6\} \qquad \hfill \boxed{\color{red}{4}\; \color{green}{5}\; \color{red}{6}} \\
B = \{2,4,6\} \qquad  \hfill \boxed{\color{grey}{2}\; \color{red}{4}\; \color{red}{6}} \\
\hline \\
A \setminus B \qquad \hfill \boxed{\color{green}{5}}
\end{align}
:::

Beispiel f√ºr die logische Differenz

::::
:::::


In R gibt es die Funktion `setdiff()`, die eine Mengendifferenz ausgibt.

```{r}
A <- c(4, 5, 6)
B <- c(2, 4, 6)

setdiff(A, B)
```

ü§Ø Von der Menge $A$ die Menge $B$ abzuziehen, ist etwas anderes, als von $B$ die Menge $A$ abzuziehen:

```{r}
setdiff(B, A)
```

:::callout-caution
$A \setminus B \ne B \setminus A$.
:::


### Disjunkte Ereignisse




Seien $A= \{1,2,3\}; B= \{4,5,6\}$.

$A$ und $B$ sind disjunkt^[engl. disjoint]: ihre Schnittmenge ist leer: $A \cap B = \emptyset$,
s. @fig-disjunkt.




![Zwei disjunkte Ereignisse, dargestellt noch √ºberlappungsfreie Kreise](img/2880px-Disjunkte_Mengen.svg.png){#fig-disjunkt width="25%" fig-align="center"}





[Quelle: rither.de](http://www.rither.de/a/mathematik/stochastik/mengentheorie-und-venn-diagramme/)


::::: {#exm-disjunkt1}
Das Ereignis $A$ "Gerade Augenzahl beim W√ºrfelwurf", $A={2,4,6}$ und das Ereignis $B$ "Ungerade Augenzahl beim W√ºrfelwurf", $B={1,3,5}$ sind disjunkt, s. @fig-disjunkt1.


::::{#fig-disjunkt1}

::: {.figure-content}


\begin{align}
A = \{2,4, 6\} \qquad \hfill \boxed{2\; 4\; 6} \\
B = \{1,3,5\} \qquad  \hfill \boxed{1\; 3\; 5} \\
\hline \\
A \cap B = \qquad  \hfill  \emptyset
\end{align}
:::

Beispiel f√ºr disjunkte Ereignisse

::::


:::::


:::{exm-disjunkt2}
Die Ereignisse "normaler Wochentag" und "Sonntag" sind disjunkt. $\square$
:::

### Vertiefung


[Animation zu Mengenoperationen](https://seeing-theory.brown.edu/compound-probability/index.html)



## Indirekte Ermittlung von Wahrscheinlichkeiten

Die indirekte Ermittlung von Wahrscheinlichkeiten meint das Ableiten von Wahrscheinlichkeitsaussagen, 
wenn man schon etwas √ºber die Wahrscheinlichkeiten des Grundraums wei√ü. 
Dazu greift man auf Rechenregeln der Stochastik zur√ºck: Man rechnet mit Wahrscheinlichkeiten.
Das h√∂rt sich vielleicht wild an, ist aber oft ganz einfach.


:::{#exm-add-ereignisse}
### Wahrscheinlichkeit f√ºr eine gerade Zahl beim W√ºrfelwurf
Ein (normaler) W√ºrfel wird geworfen. Was ist die Wahrscheinlichkeit f√ºr eine gerade Zahl, also f√ºr das Ereignis $A=\{2, 4, 6\}$? Diese Wahrscheinlichkeit betr√§gt $Pr(\text{gerade Zahl}) = 1/6 + 1/6 + 1/6 = 3/6 = 1/2$. $\square$
::: 

:::{#exm-ind}
### Gezinkter W√ºrfel
Ein gezinkter W√ºrfel hat eine erh√∂hte Wahrscheinlichkeit f√ºr das Ereignis $A=$"6 liegt oben", 
und zwar gelte $Pr(A)=1/3$.
Was ist die Wahrscheinlichkeit, *keine*  `6` zu w√ºrfeln?$\square$^[Die Wahrscheinlichkeit, 
keine `6` zu w√ºrfeln, liegt bei $2/3$.]
:::



## Additionssatz 

Der Additionssatz wird verwendet, wenn wir an der Wahrscheinlichkeit interessiert sind, 
dass *mindestens eines der Ereignisse* A und B eintritt.
"Mindestens eines der Ereignisse A und " schreibt man $A \cup B$ und sagt "A vereinigt B".

### Disjunkte Ereignisse

Gegeben sei $\Omega = {1,2,3,4,5,6}$ beim normalen W√ºrfelwurf. 
Als Sinnbild: $\boxed{1\; 2\; 3\; 4\; 5\; 6}$.
Gesucht sei die Wahrscheinlichkeit des Ereignis $A=\{1,2\}$, das also eine 1 oder  
eine 2 geworfen wird.
Man beachte, dass die beiden Ergebnisse disjunkt sind, s. @fig-disjunkt.


$\boxed{\boxed{1\; 2}\; \color{gray}{ 3\; 4\; 5\; 6}}$

Die Wahrscheinlichkeit f√ºr $A$ ist die Summe der Wahrscheinlichkeiten der einzelnen Ereignisse (*1* und *2*):

$P(1 \cup 2) = \frac{1}{6} + \frac{1}{6} = \frac{2}{6}$


:::{#def-add-disjunkt}
### Additionssatz f√ºr disjunkte Ereignisse
Die Wahrscheinlichkeit, dass mindestens eines der beiden Ereignisse eintritt, ist die Summe der Einzelwahrscheinlichkeiten, s. @thm-add-disjunkt.
:::

::: {#thm-add-disjunkt}


### Additionssatz f√ºr disjunkte Ereignisse


$$P(A \cup B) = P(A) + P(B) \square$$

:::




:::{#exm-sonntag}
Was ist die Wahrscheinlichkeit, an einem Samstag oder Sonntag geboren zu sein?
Unter der (vereinfachten) Annahme, dass alle Jahre zu gleichen Teilen aus allen
Wochentagen bestehen und dass an allen Tagen gleich viele Babies geworden
werden^[vermutlich gibt es noch mehr Annahmen, die wir uns explizit machen sollten.], ist die Antwort $Pr(A)=1/7 + 1/7 = 2/7$.$\square$
:::



### Allgemein (disjunkt oder nicht disjunkt)


Bei der Addition der Wahrscheinlichkeiten f√ºr $A$ und $B$ wird der Schnitt $A\cap B$ (der √úberlappungsbereich) *doppelt* erfasst.^[sofern sie nicht disjunkt sind, 
aber wenn sie disjunkt sind, so ist der Schnitt gleich Null und wir machen auch 
dann nichts falsch.] Der √úberlappungsbereich muss daher noch abgezogen werden, s. @fig-add.


:::{#def-additionssatz}
### Allgemeiner Additionssatz
Die Wahrscheinlichkeit, dass mindestens eines der beiden Ereignisse $A$ und $B$ eintritt, 
ist gleich der Summe ihrer Wahrscheinlichkeiten minus ihrer gemeinsamen Wahrscheinlichkeit, s. @thm-add und @fig-add. $\square$
:::

:::{#thm-add}

### Allgemeiner Additionssatz
$$P(A \cup B) = P(A) + P(B) - P(A\cap B) \square$$
:::


![Der allgemeine Additionssatz gibt die Wahrscheinlichkeit, dass mindestens eines der beiden Ereignisse eintritt.](img/addition-rule.png){#fig-add}

<!-- ::: {#fig-sets2 layout-ncol=2} -->

<!-- ![$A \cup B$](img/Venn0111.svg.png){width=20%}{#fig-sets2-a} -->




<!-- ![$A \cap B$](img/Venn0001.svg.png){width=20%}{#fig-sets2-b} -->



<!-- Die Schnittmenge muss beim Vereinigen abgezogen werden, -->
<!-- damit sie nicht doppelt gez√§hlt wird. -->


<!-- ::: -->



:::{#exm-klausur-bed-wskt}
### Lernen und Klausur bestehen

In einem angewandten Psychologie-Studiengang sind die Studis verdonnert, zwei Statistik-Module ($S1, S2$) zu belegen.
Die meisten bestehen ($B$), einige leider nicht ($N$), s. @tbl-klausur2.

Ereignis $S_1B$ sei "Klausur Statistik 1 bestanden"
Ereignis $S_2B$ ist analog f√ºr "Klausur Statistik 2".

Wir suchen die Wahrscheinlichkeit des Ereignisses A, mindestens eine der beiden Klausuren zu bestehen: 
$Pr(A) = Pr(S_1B \cup S_2B)$.




```{r}
#| echo: false
#| label: tbl-klausur2
#| tbl-cap: "Daten von 100 Studis; L: Lerner, B: Bestanden, N: Negation/Nicht"
d <- tribble(
  ~ ., ~ S1_B, ~ S1_NB, ~ Summe,
  "S2_B", 85, 9, 94,
  "S2_NB", 5, 1, 6,
  "Summe", 90, 10, 100
)

kable(d)
```


\begin{aligned}
Pr(A) &= Pr(S_1B \cup S_2B) \\
&= Pr(S1_B) + Pr(S_2B) - Pr(S_1B \cap S_2B)  \\
&= (90 + 94 - 85) / 100 = 99 / 100\\
\end{aligned}

Die Wahrscheinlichkeit, mindestens eine der beiden Klausuren zu bestehen liegt bei 99%.
Umgekehrt liegt die Wahrscheinlichkeit, keine der beiden Klausuren zu bestehen, 
liegt bei $Pr(\neg A) = 1- Pr(A) = 0.01 = 1\%$; ein Sachverhalt, 
der sich auch mit einem kurzen Blick in die Datentabelle erkennen l√§sst.$\square$
:::





## Bedingte Wahrscheinlichkeit


### Illustration zur bedingten Wahrscheinlichkeit

:::{#def-pr-cond}
### Bedingte Wahrscheinlichkeit
Die bedingte Wahrscheinlichkeit ist die Wahrscheinlichkeit, dass $A$ eintritt, 
*gegeben dass* $B$ schon eingetreten ist. $\square$
:::

Man schreibt: $Pr(A|B).$ Lies: "A gegeben B" oder "A wenn B".









:::{#exr-victorpowell}
Schauen Sie sich mal diese [Wahnsinnsanimation von Victor Powell an](https://setosa.io/conditional/) zu bedingten Wahrscheinlichkeiten. 
Hammer!
:::



::::: {.content-visible when-format="html"}

@fig-schema-p illustriert *unbedingte Wahrscheinlichkeit*, $Pr(A), Pr(B)$, *gemeinsame Wahrscheinlichkeit* $P(A \cap B)$, und *bedingte Wahrscheinlichkeit*, $P(A|B)$.


::::{#fig-schema-p}

:::{.panel-tabset}



#### $Pr(A)$



```{r}
#| echo: false
#| label: fig-pr-a
#| fig-cap: "Unbedingte Wahrscheinlichkeit f√ºr Ereignis A: 50%=1/2"
#| out-width: 50%
plot_pr_a
```



#### $Pr(B)$


```{r}
#| echo: false
#| out-width: 50%
#| label: fig-pr-b
#| fig-cap: "Unbedingte Wahrscheinlichkeit f√ºr Ereignis B: 50%=1/2"
plot_pr_b
```



#### $Pr(B|A)$



```{r}
#| echo: false
#| label: fig-b-geg-a
#| out-width: 50%
#| fig-cap: "Wahrscheinlichkeit f√ºr Ereignis B gegeben A, $Pr(B|A)=1/2$"
plot_pr_b_geg_a
```


#### $Pr(A \cap B)$


$Pr(A \cap B)$ wird auch h√§ufig (synonym) geschrieben als $Pr(AB)$.

```{r}
#| echo: false
#| out-width: 50%
#| label: fig-ab
#| fig-cap: "Wahrscheinlichkeit f√ºr das gemeinsame Eintreten von A und B: $Pr(AB)=1/4$"

plot_pr_ab
```


:::


Illustration von unbedingter, gemeinsamer und bedingter Wahrscheinlichkeit
::::

:::::




::: {.content-visible unless-format="html"}


@fig-schema-p-pdf illustriert *gemeinsame Wahrscheinlichkeit*, $P(A \cap B)$, und *bedingte Wahrscheinlichkeit*, $P(A|B)$.


```{r bed-w-schema-pdf}
#| echo: false
#| fig-cap: Illustration von gemeinsamer und bedingter Wahrscheinlichkeit
#| label: fig-schema-p
#| layout-ncol: 3
#| fig-subcap: 
#|   - "unbedingte Wahrscheinlichkeit f√ºr Ereignis B: 50%"
#|   - "unbedingteWahrscheinlichkeit f√ºr Ereignis A: 50%"
#|   - "Wahrscheinlichkeit f√ºr Ereignis B gegeben A, $Pr(B|A)$: 50%"
plot_pr_b
plot_pr_a
plot_pr_b_geg_a
```


:::

:::{#exm-bed-p}
### Bedingte Wahrscheinlichkeit
Sei $A$ "Sch√∂nes Wetter" und $B$ "Klausur steht an".
Dann meint $Pr(A|B)$ die Wahrscheinlichkeit, dass das Wetter sch√∂n ist, wenn gerade eine Klausur ansteht.$\square$
:::


:::{#exm-papst}
### Von P√§psten und M√§nnern

Man(n) beachte, dass die Wahrscheinlichkeit, Papst $P$ zu sein, wenn man Mann $M$ ist *etwas anderes* ist, als die Wahrscheinlichkeit, Mann zu sein, wenn man Papst ist:
$Pr(P|M) \ne Pr(M|P)$. Das h√∂rt sich erst verwirrend an, aber wenn man dar√ºber nachdenkt, wird es plausibel.$\square$
:::



:::{#exm-hirn-gr√ºtze}
Gustav Gro√ü-G√ºtz verkauft eine Tinktur^[genauer besehen sieht sie eher aus wie eine Gr√ºtze oder ein Brei], die schlau machen soll, "G√ºtzis Gehirn Gr√ºtze".^[Sie schmeckt scheu√ülich.]
Gustav trinkt die Gr√ºtze und sagt schlaue Dinge.
Was schlie√üen wir daraus?
Sei $H$ (wie *H*ypothese) "G√ºtzis Gr√ºtze macht schlau"; sei $D$ (wie *D*aten) die Beobachtung, 
dass Gustav schlaue Dinge gesagt hat.
Ohne exakte Zahlen zu suchen, wie hoch ist wohl $Pr(D|H)$? In Worten: "Wie wahrscheinlich ist es, 
schlaue Dinge gesagt zu haben, wenn die Gr√ºtze wirklich schlau macht?".
Vermutlich ist diese Wahrscheinlichkeit sehr hoch.
Aber wie hoch ist wohl $Pr(H|D)$? In Worten: "Wie wahrscheinlich ist es,
dass die Gr√ºtze wirklich schlau macht, gegeben, dass wir gesehen hat, 
dass jemand etwas schlaues gesagt hat, 
nachdem er besagte Gr√ºtze getrunken hat?"
Skeptische Geister werden der Meinung sein, $Pr(H|D)$ ist gering.
Das Beispiel zeigt u.a. $Pr(H|D) \ne Pr(D|H).\square$
:::


### Bedingte Wahrscheinlichkeit als Filtern einer Tabelle



```{r}
#| echo: false
d <- 
  tibble::tribble(
      ~id, ~kalt, ~Regen,
      "1", 0L, 0L,
      "2", 0L, 1L,
      "3", 1L, 0L,
      "4", 1L, 1L,
  "SUMME", 2L, 2L
  )

d_kalt_regen <-
   tibble::tribble(
      ~id, ~kalt, ~Regen,
      "1", "nein", "nein",
      "2", "nein", "ja",
      "3", "ja", "nein",
      "4", "ja", "ja")
```


:::{exm-kalt-regen-filter}

Betrachten wir @tbl-kalt-regen-filter. Dort sind sind vier Tage aufgelistet, mit jeweils Regen (oder kein Regen) bzw. an denen es kalt ist (oder nicht). Filtern wir z.B. die Tabelle so, dass nur kalte Tage √ºbrig bleiben, dann gibt der Anteil der Zeilen, die "Regen" anzeigen, die bedingte Wahrscheinlichkeit  $Pr(\text{Regen}|\text{kalt})$ an.

```{r}
#| echo: false
#| label: tbl-kalt-regen-filter
#| tbl-cap: "Die Tabelle zeigt vier Tage, an denen es jeweils kalt ist (oder nicht) bzw. regnet (oder nicht). Die bedingte Wahrscheinlichkeit $Pr(\\text{Regen}|\\text{kalt})$ entspricht dem Anteil der Zeilen mit Regen, wenn f√ºr 'kalt' ein Filter in der Tabelle gesetzt ist."
d_kalt_regen |>  
  datatable(filter = "top", options = list(pageLength = 4))
```

:::

Also: Das Berechnen einer bedingten Wahrscheinlichkeit, $Pr(A|B)$, 
ist vergleichbar zum Filtern einer Tabelle, s. @tbl-bed-wskt.


```{r}
#| label: tbl-bed-wskt
#| echo: false
#| tbl-cap: Eine bedingte Wahrscheinlichkeit kann man als gefilterte Tabelle verstehen
knitr::kable(d)
```

Es ergeben sich folgende Wahrscheinlichkeiten:

$Pr(A) = 2/4; Pr(B) = 2/4; Pr(A \cap B) = 1/4; Pr(A|B) = 1/2$




Die Wahrscheinlichkeit f√ºr $A$, wenn $B$ schon eingetreten ist, 
berechnet sich so, s. @thm-pr-cond und @fig-b-geg-a.

::: {#thm-pr-cond}

### Bedingte Wahrscheinlichkeit

$$Pr(A|B) = \frac{Pr(A \cap B)}{Pr(B)}$$

Au√üerdem gilt analog

$$Pr(B|A) = \frac{Pr(B \cap A)}{Pr(A)} \quad \square$$
:::



:::{#exm-klausur-bed-wskt}
### Lernen und Klausur bestehen


Sie bereiten sich gerade auf die Klausur bei Prof. S√º√ü vor.
Das hei√üt: Sie √ºberlegen, ob Sie sich auf die Klausur vorbereiten sollten.
Vielleicht lohnt es sich ja gar nicht? Vielleicht ist die Wahrscheinlichkeit zu bestehen, 
wenn man nicht gelernt hat, sehr gro√ü?
Aber da Sie nun mal auf Fakten stehen, haben Sie sich nach einiger Recherche folgende Zahlen besorgen k√∂nnen, s. @tbl-klausur-lernen.
In der Tabelle sind die Daten von 100 Studis ausgewiesen.
Ein Teil hat sich vorbereitet, ordentlich gelernt, nenen wir sie die "*L*erner".
Ein anderer Teil hat nicht gelernt, $NL$ bzw. $\neg L$. Ein Teil hat bestanden, $B$, 
ein Teil nicht $NB$ oder $\neg B$.

Wir suchen die Wahrscheinlichkeit, zu bestehen, wenn man nicht gelernt hat: $Pr(B |\neg L)$.



```{r}
#| echo: false
#| label: tbl-klausur-lernen
#| tbl-cap: "Daten von 100 Studis; L: Lerner, B: Bestanden, N: Negation/Nicht"
d <- tribble(
  ~ ., ~ L, ~ NL, ~ Summe,
  "B", 80, 1, 81,
  "NB", 5, 14, 19,
  "Summe", 85, 15, 100
)

kable(d) %>%
  kable_styling() %>%
  column_spec(1, bold = TRUE)  # Makes the first column bold

```


\begin{aligned}
Pr(B |\neg L) &= \frac{Pr(B \cap \neg L)}{Pr(\neg L)} \\
&=\frac{1/100}{15/100} = 1/15 \\
\end{aligned}

Die Wahrscheinlichkeit, zu bestehen, wenn man nicht gelernt hat, liegt bei 1 von 15,
also ca. 7%.^[$Pr(L).85; Pr(\neg L) = .15; Pr(B) =.81; Pr(\neg B) = .19$] $\square$
:::


:::{#exm-kalt-regen}
### Kalt und Regen
Die Wahrscheinlichkeit, dass es kalt ist, wenn es regnet, 
ist gleich der Wahrscheinlichkeit, dass es gleichzeitig kalt ist und regnet,
geteilt durch die Wahrscheinlichkeit, dass es regnet.$\square$
:::



## Stochastische (Un-)Abh√§ngigkeit


### Unabh√§ngigkeit

Stochastische Unabh√§ngigkeit ist ein Spezialfall von Abh√§ngigkeit: 
Es gibt sehr viele Auspr√§gungen f√ºr Abh√§ngigkeit, aber nur eine f√ºr Unabh√§ngigkeit.
K√∂nnen wir Unabh√§ngigkeit nachweisen, haben wir also eine *starke* Aussage get√§tigt.


:::{#def-indep}
### Stochastische Unabh√§ngigkeit
Zwei Ereignisse sind (stochastisch) unabh√§ngig voneinander, 
wenn die Wahrscheinlichkeit von $A$ nicht davon abh√§ngt, ob $B$ der Fall ist, s. @thm-indep.
Anders gesagt:^[Exakte Gleichheit ist in dieser Welt empirisch schwer zu finden. 
Daher kann man vereinbaren, dass Unabh√§ngigkeit erf√ºllt ist, wenn die Gleichheit "einigerma√üen" oder "ziemlich" gilt, die Gleichheit gewisserma√üen "praktisch bedeutsam" ist.]

 \newcommand{\indep}{\perp \!\!\! \perp}

::: {#thm-indep}

### Stochastische Unabh√§ngigkeit

$$Pr(A|B) = Pr(A) = Pr(A|\neg B). \square$$

Die Unabh√§ngigkeit von $A$ und $B$ wird manchmal so in Kurzschreibweise ausgedr√ºckt: $\indep(A, B) \square$.
:::


::: {#thm-indep2}

### Stochastische Unabh√§ngigkeit 2

Setzt man @thm-pr-cond in @thm-indep (linke Seite) ein, so folgt^[Vgl. @thm-multtheorem]

$$Pr(A \cap B) = Pr(A) \cdot Pr(B).\quad \square$$
:::


:::{#exm-indep1}
### Augenfarbe und Statistikliebe
Ich vermute, dass die Ereignisse $A$, "Augenfarbe ist blau", und $B$, 
"Ich liebe Statistik", voneinander unabh√§ngig sind.$\square$^[Wer Daten dazu hat oder eine Theorie, 
der melde sich bitte bei mir.]
:::





```{r}
#| echo: false
data("titanic_train")

titanic2 <- 
titanic_train %>%
  select(Pclass, Survived, Embarked, Age) %>%
  mutate(Survived = factor(Survived)) |> 
  filter(Embarked %in% c("C", "Q", "S")) %>%
  mutate(Survived = factor(Survived)) |> 
  mutate(Age_prime_num = isprime(Age),
         Age_prime = case_when(
           Age_prime_num == 0 ~ "prime",
           TRUE ~ "non-prime"
         ),
         Age_prime = factor(Age_prime)) %>%
  mutate(Survived = factor(Survived))

plottitanic1 <-
  titanic2 |> 
  ggplot(aes(x = Pclass)) +
  geom_bar(aes(fill = Survived), position = "fill") +
  theme(legend.position = "bottom")  +
  scale_fill_okabeito()


plottitanic2 <-
  titanic2 |> 
  ggplot(aes(x = Embarked)) +
  geom_bar(aes(fill = Survived), position = "fill") +
  theme(legend.position = "bottom") +
  scale_fill_okabeito()

plottitanic3 <-
  titanic2 |> 
  ggplot(aes(x = Age_prime)) +
  geom_bar(aes(fill = Survived), position = "fill") +
  theme(legend.position = "bottom") +
  scale_x_discrete(breaks = c(0, 2),
                   labels = c("Nicht Prim", "Prim")) +
  scale_fill_okabeito()

# plottitanic3

```



:::{#exm-titanic}
### √úberleben auf der Titanic
S. @fig-abh, links: √úberleben (√ú) auf der Titanic ist offenbar *abh√§ngig* von der Passagierklasse ($K_1, K_2, K_3$). 
In @fig-abh, links gilt also $Pr(√ú|K_1) \ne Pr(√ú|K_2) \ne Pr(√ú|K_3) \ne Pr(√ú)$.

Auf der anderen Seite: Das Ereignis *√úberleben* (√ú) auf der Titanic ist *un*abh√§ngig 
vom Ereignis *Alter ist eine Primzahl* (P), s. @fig-abh, rechts.
Also: $Pr(√ú|P) = Pr(√ú|\neg P) = Pr(√ú)$, vgl. @tbl-titanic-prime. 

```{r}
#| tbl-cap: "Kontingenztablle (H√§ufigkeiten) f√ºr '√úberleben auf der Titanic' und 'Alter ist Primzahl'. Wie man sieht, gibt es keine stochastische Abh√§ngigkeit."
#| label: tbl-titanic-prime
#| echo: false
titanic2 |> 
  count(Survived, Age_prime) |> 
  group_by(Survived) |> 
  mutate(prop = n/sum(n)) |> 
  gt::gt() |> 
  gt::fmt_number(columns = prop, decimals = 2)
```


:::





```{r QM2-Thema1-WasistInferenz-31, out.width="100%"}
#| fig-cap: "Abh√§ngigkeit und Unabh√§ngigkeit zweier Ereignisse"
#| label: fig-abh
#| echo: false
#| layout-ncol: 2
#| fig-subcap: 
#|   - "√úberleben und Passagierklasse sind abh√§ngig"
#|   - "√úberleben und 'Geburstag ist eine Primzahl' sind nicht abh√§ngig"
plottitanic1
plottitanic3
```




### Stochastische Abh√§ngigkeit

Liegt keine Unabh√§ngigkeit vor, so spricht man von (stochatistischer) *Abh√§ngigkeit*, s. @thm-abh. 
In diesem Fall ver√§ndert sich unser Wissen √ºber die Wahrscheinlihkeit von $A$, 
wenn wir wissen, 
dass $B$ eingetroffen ist, s. @thm-abh.

::: {#thm-abh}

### Stochastische Abh√§ngigkeit

$$Pr(A|B) \ne Pr(A) \ne Pr(A|\neg B) \quad \square$$

@thm-abh gilt nat√ºrlich in dieser Form f√ºr alle anderen Variablen ebenso, s. z.B. @thm-abh2. $\square$
:::

::: {#thm-abh2}

### Stochastische Abh√§ngigkeit 2

$$Pr(B|A) \ne Pr(B) \ne Pr(B|\neg A) \quad \square$$
:::


:::{#exm-abh-lernen}
Die Ereignisse "*L*ernen" und "*K*lausur bestehen" seien voneinander abh√§ngig. 
Unsere Ansicht zur Wahrscheinlichkeit von *K* √§ndert sich, wenn wir wissen, dass *L* vorliegt. 
Genauso wird sich unsere Einsch√§tzung zur Wahrschienlichkeit von *K* √§ndern, 
wenn wir wissen, dass *L* nicht vorliegt. $\square$
:::



:::{#exm-covid}

### Zusammenhang von Covidsterblichkeit und Impfquote


Sind die Ereignisse *Tod durch Covid*  bzw. *Impfquote* ($A$) 
und *Land*^[hier mit den zwei Auspr√§gungen *DEU* und *USA*] ($B$) voneinander abh√§ngig (s. @fig-covid1)?

```{r covid1}
#| message: false
#| echo: false
#| cache: true
#| fig-cap: Impfquote und Sterblichkeit sind voneinander abh√§ngig (bezogen auf Covid, auf Basis vorliegender Daten)
#| label: fig-covid1
#| layout-ncol: 2
#| fig-subcap: 
#|   - Impfquote und Land sind voneinander abh√§ngig
#|   - Anteil Corona-Tote und Land sind voneinander abh√§ngig


# source: https://ourworldindata.org/covid-vaccinations
# access date: 2021-09-24
# licence: https://ourworldindata.org/covid-vaccinations#licence



dfile <- "data/owid-covid-data.csv"



d <- read_csv(dfile)

d2<-
  d %>%
  filter(iso_code %in% c("DEU", "USA")) %>%
  mutate(date = as_date(date)) %>%
  rename(Land = iso_code) %>%
  select(date,
         Land,
         #total_deaths,
         #new_deaths,
         people_fully_vaccinated_per_hundred,
         total_deaths_per_million,
         #new_vaccinations,
         total_vaccinations) %>%
  filter(date == "2021-09-23") %>%
  group_by(Land)



plot_covid1 <-
  d2 %>%
  ggplot(aes(x = Land,
             y = people_fully_vaccinated_per_hundred)) +
  geom_col() +
  labs(title = "Anteil komplett geimpfter Personen",
       subtitle = "2021-09-23")




plot_covid2 <-
  d2 %>%
  ggplot(aes(x = Land,
             y = total_deaths_per_million)) +
  geom_col()+
  labs(title = "Corona-Tote pro Million",
       subtitle = "2021-09-23")


plot_covid1
plot_covid2
```

Ja, die beiden Ereignisse sind abh√§ngig, da in beiden Diagrammen gilt: $P(A|B) \ne Pr(A) \ne Pr(A|\neg B)$.$\square$^[
Daten von Our World in Data, <https://ourworldindata.org/covid-deaths>.]


:::





### Unabh√§ngigkeit ist symmetrisch

Stochastische Unabh√§ngigkeit ist symmetrisch: 
Wenn $A$ unabh√§ngig zu $B$ ist auch $B$ unabh√§ngig zu $A$, s. @thm-indep-symm.

::: {#thm-indep-symm}

### Symmetrie der Unabh√§ngigkeit

$$Pr(A|B) = Pr(A) \leftrightarrow Pr(B|A) = Pr(B)$$

Man beachte, dass stochastische Unabh√§ngigkeit und kausale Unabh√§ngigkeit unterschiedliche Dinge sind [@henze2019]: 
Stochastische Unabh√§ngigkeit impliziert nicht kausale Unabh√§ngigkeit. $\square$
:::













## Multiplikationssatz

Gegeben seien die Ereignisse $A$ und $B$. 
Der Multiplikationssatz wird verwendet, 
wenn wir an der Wahrscheinlichkeit interessiert sind, 
dass *beide Ereignisse* $A$ und $B$ eintreten; 
s. @fig-ab verdeutlicht dies f√ºr zwei unabh√§ngige Ereignisse.
Man schreibt "A und B" als $A \cap B$ (lies "A geschnitten B" oder "A und B") oder kurz $AB$, um anzuzeigen, dass sowohl $A$ als auch $B$ eingetreten sind.

```{r, ref.label = "fig-ab"}
#| out-width: 50%
#| echo: false
#| fig-cap: "Beide Ereignisse A und B sind eingetreten"
```


:::{#exm-kalt-regen-mult}
### Wieder kalt und Regen
Es ist eine Sache, zu fragen, wie wahrscheinlich ist ist, dass es *k*alt ist (bei *K*√§lte), 
wenn es *r*egnet (bei *R*egen): $Pr(K|R)$. 
Anders gesagt: "Wie gro√ü ist die Wahrscheinlichkeit f√ºr K√§lte, gegeben dass es regnet?"
Eine andere Sache ist es,
nach der Wahrscheinlichkeit zu fragen, 
dass es gleichzeitig kalt ist und regnet, 
dass also beide Ereignisse (kalt und Regen) eintreten: $Pr(K \cap R), PR(KR)$. $\square$
:::


### Gemeinsame Wahrscheinlichkeit unabh√§ngiger Ereignisse


:::{#exm-indep2}
Wir f√ºhren das Zufallsexperiment "Wurf einer fairen M√ºnze" zwei Mal aus (@fig-zweimuenzen).
Wie gro√ü ist die Wahrscheinlichkeit, 2 Mal *K*opf zu werfen?
Dabei vereinbaren wir, dass "Kopf" als "Treffer" z√§hlt (und "Zahl" als "Niete").$\square$
:::


![Wir werfen zwei faire M√ºnzen: Zweifach wiederholtes Zufallexperiment](img/muenz1.png){#fig-zweimuenzen width="50%"}

 @fig-zweimuenzen zeigt ein *Baumdiagramm*. 
Jeder *Kasten* (Knoten) zeigt ein *Ergebnis* des Zufallexperiments. 
Die Pfeile (Kanten) symbolisieren die Abfolge des Experiments: Vom "Start" (schwarzer Kreis) 
f√ºhren zwei m√∂gliche Ergebniss ab, jeweils mit Wahrscheinlichkeit 1/2.
Die untersten Knoten nennt man auch *Bl√§tter* (Endknoten), 
sie zeigen das Endresultat des (in diesem Fall) zweifachen M√ºnzwurfs.
Der Weg vom Start zu einem bestimmten Blatt nennt man *Pfad*. 
Die Anzahl der Pfade entspricht der Anzahl der Bl√§tter.
In diesen Diagramm gibt es vier Pfade (und Bl√§tter).

Den Wurf der ersten M√ºnze nennen wir in gewohnter Manier $A$; 
den Wurf der zweiten M√ºnze $B$.


Die Wahrscheinlichkeiten der resultierenden Ereignisse finden sich in @tbl-muenz2.


```{r}
#| echo: false
#| label: tbl-muenz2
#| tbl-cap: Wahrscheinlichkeiten der Ereignisse im zweimaligen M√ºnzwurf
tibble::tribble(
  ~Ereignis,               ~Pr,
       "0K", "1/2 * 1/2 = 1/4",
       "1K", "1/4 + 1/4 = 1/2",
       "2K", "1/2 * 1/2 = 1/4"
  ) %>% kable()
```


Sei $K_1$ das Ereignis, mit der 1. M√ºnze Kopf zu werfen; sei $K_2$ das Ereignis, 
mit der 2. M√ºnze Kopf zu werfen-

Wir suchen $Pr(K_1 \cap K_2)$. Aufgrund der stochastischen Unabh√§ngigkeit der beiden Ereignisse gilt: $Pr(K_1 \cap K_2) = Pr(K_1) \cdot Pr(K_2)$.

```{r}
Pr_K1K2 <- 1/2 * 1/2
Pr_K1K2
```


:::{#def-multsatz}
### Multiplikationssatz f√ºr unabh√§ngige Ereignisse
Die Wahrscheinlichkeit, dass zwei (oder mehr) unabh√§ngige Ereignisse $A$ und $B$ *gemeinsam* eintreten, 
ist gleich dem Produkt ihrer jeweiligen Wahrscheinlichkeiten, s. @thm-multtheorem. $\square$
:::


::: {#thm-multtheorem}
### Multiplikationssatz f√ºr unabh√§ngige Ereignisse

$$Pr(A \cap B) = Pr(A) \cdot Pr(B)$$ 

Man beachte, dass es egal ist, ob $A$ gemeinsam mit $B$ oder $B$ gemeinsam mit $A$ eintreten: $Pr(A \cap B) = Pr(B \cap A)$.^[Man spricht auch von *Symmetrie* der Multiplikation.] $\square$
:::



Mit [dieser App](https://www.geogebra.org/m/gzxz57ak) k√∂nnen Sie das Baumdiagramm f√ºr den zweifachen M√ºnzwurf n√§her erkunden.




Wir f√ºhren das Zufallsexperiment "Wurf einer fairen M√ºnze" drei Mal aus (@fig-dreimuenzen).
Dabei vereinbaren wir wieder, dass "Kopf" (K) als "Treffer" gilt und "Zahl" (Z) als "Niete".

![Wir werfen drei faire M√ºnzen: Dreifach wiederholtes Zufallexperiment](img/muenz2.png){#fig-dreimuenzen}


Beim Wurf von "fairen" M√ºnzen gehen wir davon aus, dass Kenntnis des Ergebnis des 1. Wurfes unsere Einsch√§tzung des Ergebnis des 2. Wurfes nicht ver√§ndert etc.
Anders gesagt: Wir gehen von (stochastischer) Unabh√§ngigkeit aus.


F√ºr z.B. das Ereignis $A=\{ZZZ\}$ gilt: $Pr(A) = 1/2 \cdot 1/2 \cdot 1/2 = (1/2)^3$.
Da jeder Endknoten (jedes Blatt) gleichwahrscheinlich ist, ist die Wahrscheinlichkeit jeweils gleich.

Allgemeiner gilt: F√ºr ein Zufallsexpriment, das aus $k$ Wiederholungen besteht 
und in jeder Wiederholung die Wahrscheinlichkeit $Pr(X)=p$ ist,
so ist die Wahrscheinlichkeit f√ºr einen Endkonten $Pr(X^k)=p^k$.





```{r QM2-Thema1-WasistInferenz-27}
#| echo: false
#| tbl-cap: "Ausgew√§hlte Wahrscheinlichkeiten von Ereignissen im dreifachen M√ºnzwurf"
#| label: tbl-baum3
tibble::tribble(
  ~Ereignis,                     ~Pr,
       "0K", "1/2 * 1/2 * 1/2 = 1/8",
       "1K", "1/8 + 1/8 + 1/8 = 3/8",
       "2K",          "3 * 1/8 = 3/8",
       "3K", "1/2 * 1/2 * 1/2 = 1/8"
  ) 
```


Da die Endknoten disjunkte Elementarereignisse sind, kann man ihre Wahrscheinlichkeit addieren, 
um zu anderen (zusammengesetzten) Ereignissen zu kommen, vgl. @tbl-baum3.


@fig-schema-p versinnbildlicht nicht nur die Bedingtheit zweier Ereignisse, 
sondern auch die (Un-)Abh√§ngigkeit zweier Ereignisse, $A$ und $B$.
In diesem Fall ist die Wahrscheinlichkeit von $A$ gleich $B$: 
$Pr(A)=Pr(B)=.5$.
Man sieht, dass die Wahrscheinlichkeit von $A$ bzw. von $B$ jeweils die H√§lfte 
der Fl√§che (der Gesamtfl√§che, d.h von $Pr(\Omega)=1$) ausmacht. 
Die Schnittmenge der Fl√§che von $A$ und $B$ entspricht einem Viertel der Fl√§che: 
$Pr(AB) = Pr(A) \cdot Pr(B) = 50\% \cdot 50\% = 25\%.$
In diesem Fall sind $A$ und $B$ unabh√§ngig.
@fig-schema-p zeigt weiterhin, dass gilt: $P(A\cap B) = P(A) \cdot P(B) = P(B) \cdot P(A)$.
Man beachte dass diese Formel nur bei *Unabh√§ngigkeit* (von A und B) gilt.



### Gemeinsame Wahrscheinlichkeit abh√§ngiger Ereignisse



Ein Baumdiagramm bietet sich zur Visualisierung abh√§ngiger Ereignisse an, s.  @fig-baum-abh. 
F√ºr unabh√§ngige Ereignisse √ºbrigens auch.


:::{#exm-urne1}
In einer Urne befinden sich f√ºnf Kugeln, von denen vier rot sind und eine blau ist.




Hier ist unsere Urne:

$$\boxed{\color{red}{R, R, R, R}, \color{blue}B}$$

Wie gro√ü ist die Wahrscheinlichkeit, dass bei zwei Ziehungen ohne Zur√ºcklegen (*ZOZ*) *zwei rote Kugeln* gezogen werden [@bourier2011], S. 47. 
Ereignis A: "Kugel im 1. Zug hat die Farbe Rot". 
Ereignis B: "Kugel im 2. Zug hat die Farbe Rot".

Und jetzt ziehen wir. Hier ist das Baumdiagramm, s.  @fig-baum-abh.


```{mermaid}
%%| fig-cap: "Baumdiagramm f√ºr ein ein zweistufiges Zufallsereignis, wobei der 2. Zug (Stufe) abh√§ngig ist vom 1. Zug."
%%| label: fig-baum-abh
flowchart LR
  A[Start] -->|4/5|B[Zug 1 - R]
  A -->|1/5|C[Zug 1 - B]
  B -->|3/4|D[Zug 2 - R]
  B -->|1/4|E[Zug 2 -  B]
  D --- H[Fazit: RR - 4/5*3/4 = 12/20]
  E --- I[Fazit: RB - 4/5*1/4 = 4/20]
  C -->|4/4|F[Zug 2 - R]
  C -->|0/4|G[Zug 2 - B]
  F --- J[Fazit: BR - 1/5*4/4 = 4/20]
  G --- K[Fazit: BB - 1/5*0/4 = 0/20]
  
```



Wie man in @fig-baum-abh nachrechnen kann gilt also: $P(A\cap B) = P(A) \cdot P(B|A) \square$.
:::


:::{#def-gem-wskt-abh}
### Gemeinsame Wahrscheinlichkeit

Die Wahrscheinlichkeit, dass zwei abh√§ngige Ereignisse $A$ und $B$ *gemeinsam* eintreten, 
ist gleich dem Produkt der Wahrscheinlichkeit von $A$ und der bedingten Wahrscheinlichkeit von $B$ gegeben $A$, 
s. @thm-gem-wskt-abh. $\square$
:::

:::{#thm-gem-wskt-abh}

### Gemeinsame Wahrscheinlichkeit

$$Pr(A \cap B) = Pr(A) \cdot Pr(B|A) \quad \square$$
:::



:::{#exm-kaltregen}
### Kalt und Regen

Von @mcelreath2020 stammt diese Verdeutlichung der gemeinsamen Wahrscheinlichkeit.
Was ist die Wahrscheinlichkeit f√ºr das gemeinsame Auftreten von *kalt ‚ùÑ und Regen ‚õàÔ∏è*? 
Die Wahrscheinlichkeit f√ºr kalt und Regen ist die Wahrscheinlichkeit von *Regen* ‚õà, 
wenn's *kalt* ‚ùÑ ist mal die Wahrscheinlichkeit von *K√§lte* ‚ùÑ.

Ebenfalls gilt:
Die Wahrscheinlichkeit f√ºr kalt und Regen ist die Wahrscheinlichkeit von *K√§lte* ‚ùÑ, 
wenn's *regnet* ‚õàÔ∏è mal die Wahrscheinlichkeit von *Regen* ‚õàÔ∏è.

Das Gesagte als Emoji-Gleichung ist in @eq-kalt-regen-emoji dargestellt.

$$P(‚ùÑÔ∏è \text{ und } ‚õàÔ∏è) = P(‚õàÔ∏è |‚ùÑÔ∏è ) \cdot P(‚ùÑÔ∏è) =  P(‚ùÑÔ∏è |‚õàÔ∏è ) \cdot P(‚õàÔ∏è)$${#eq-kalt-regen-emoji}


Man kann  die "Gleichung drehen"^[Der Multiplikationssatz ist symmetrisch], s. @eq-kalt-regen-emoji2. 

$$P(‚ùÑÔ∏è \text{ und } ‚õàÔ∏è) = P(‚õàÔ∏è \text{ und } ‚ùÑÔ∏è)$${#eq-kalt-regen-emoji2} $\square$
:::



:::{#exm-bertie-botts}
### Bertie Botts Bohnen jeder Geschmacksrichtung

>   Sei blo√ü vorsichtig mit denen. Wenn sie sagen jede Geschmacksrichtung, dann meinen sie es auch - Du kriegst zwar alle gew√∂hnlichen wie Schokolade und Pfefferminz und Erdbeere, aber auch Spinat und Leber und Kutteln. George meint, er habe mal eine mit Popelgeschmack gehabt.‚Äú

‚Äî Ron Weasley zu Harry Potter^[Quelle: <https://harrypotter.fandom.com/de/wiki/Bertie_Botts_Bohnen_jeder_Geschmacksrichtung>]

In einem Beutel liegen $n=20$ *Bertie Botts Bohnen jeder Geschmacksrichtung*.
Uns wurde verraten, dass fast alle gut schmecken, also z.B. nach Schokolade, 
Pfefferminz oder Marmelade. Leider gibt es aber auch $x=2$ furchtbar 
scheu√üliche Bohnen (Ohrenschmalz-Geschmacksrichtung oder Schlimmeres).
Sie haben sich nun bereit erkl√§rt, $k=2$ Bohnen zu ziehen. 
Und zu essen, und zwar direkt und sofort!
Also, jetzt hei√üt es tapfer sein. Ziehen und runter damit!

Wie gro√ü ist die Wahrscheinlichkeit, *genau eine* scheu√üliche Bohne zu erwischen?

Es gibt 2 Pfade f√ºr 1 Treffer bei 2 Wiederholungen (Z√ºge aus dem Beutel): 

```{r}
Pfad1 <- 3/20 * 17/19  # scheu√üliche Bohne im 1. Zug
Pfad2 <- 17/20 * 3/19  # scheu√üliche Bohne im 2. Zug


Gesamt_Pr <- Pfad1 + Pfad2 
Gesamt_Pr
```



Nutzen Sie [diese App](https://www.geogebra.org/m/j545R3Fu), um das auszuprobieren.
Sie m√ºssen in der App noch die Zahl der Bohnen ($n$) 
und die Zahl der scheu√ülichen Bohnen ($x$) einstellen.$\square$
:::








:::{#def-kettenregel}
### Kettenregel
Allgemein gesagt, spricht man von der *Kettenregel* der Wahrscheinlichkeitsrechnung, 
wenn man die gemeinsame Wahrscheinlichkeit auf die bedingte zur√ºckf√ºhrt, s. @thm-kette. $\square$
:::


::: {#thm-kette}

### Kettenregel

$$P(A\cap B) = P(A) \cdot P(B|A) = P(B) \cdot P(A|B) \square$$

In Worten: Die Wahrscheinlichkeit von $A$ gegeben $B$ ist gleich der Wahrscheinlichkeit von $A$ mal der Wahrscheinlichkeit von $B$ gegeben $A$.
:::












:::{#exr-baum3}
### Baumdiagramm sucht Problem

√úberlegen Sie sich eine Problemstellung (Aufgabenstellung), die mit [dieser Baumdiagramm-App](
https://www.geogebra.org/m/WVKur4yM) gel√∂st werden kann.$\square$
:::





## Totale Wahrscheinlichkeit


:::{#exm-bourier-s56}
### Gesamter Ausschussanteil
Die folgende Aufgabe bezieht sich auf @bourier2011, S. 56. 
Drei Maschinen ($M_1, M_2, M_3$) 
produzieren einen Artikel. 
Die Maschinen haben einen Anteil an der Produktion von 60, 10 bzw. 30% und eine Ausschussquote von 5,2 bzw. 4%. 
Sei $M$ das Ereignis, dass ein Artikel von Maschine $M$ stammt, und $S$ (Schrott) das Ereignis, dass ein Artikel Ausschuss (Schrott) ist.
Wie gro√ü ist der Ausschussanteil insgesamt (√ºber alle drei Maschinen?) 
Anders gesagt: Wie hoch ist die Wahrscheinlichkeit, dass ein zuf√§llig gezogener Artikel Ausschuss ist. @fig-tot-wskt zeigt das Baumdiagramm f√ºr die Aufgabe. $\square$
:::

```{mermaid}
%%| fig-cap: Totale Wahrscheinlichkeit
%%| label: fig-tot-wskt
flowchart LR
  A[Start] -->|0.6|B[M1]
  A -->|0.1|C[M2]
  A -->|0.3|D[M3]
  B -->|0.05|E[S]
  B -.->|0.95|F[Nicht-S]
  C -->|0.02|G[S]
  C -.->|0.98|H[Nicht-S]
  D -->|0.04|I[S]
  D -.->|0.96|J[Nicht-S]
```



Gesucht ist also die Wahrscheinlichkeit $P(B)$; $A$ ist ein vollst√§ndiges Ereignissystem.


Dazu addieren wir die Wahrscheinlichkeiten der relevanten √Ñste.
Jeder Ast stellt wiederum das gemeinsame Auftreten der Ereignisse $A_i$ und $B$ dar.

```{r}
W_Ast1 <- 0.6 * 0.05  # Wahrscheinlichkeit f√ºr Ast 1
W_Ast2 <- 0.1 * 0.02  # ... Ast 2
W_Ast3 <- 0.3 * 0.04  # ... Ast 3
W_total <- W_Ast1 + W_Ast2 + W_Ast3  # totale W.
W_total
```

Die *totale Wahrscheinlichkeit* (f√ºr Ausschuss) betr√§gt in diesem Beispiel also $P(B) = 4.4\%$.^[Einfacher als das Rechnen mit Wahrscheinlichkeiten ist es in solchen F√§llen, wenn man anstelle von Wahrscheinlichkeiten absolute H√§ufigkeiten zum Rechnen verwendet.]

:::{#def-totwskt}
### Totale Wahrscheinlichkeit
Bilden die Ereignisse $A_1, A_2, ..., A_n$ ein vollst√§ndiges Ereignissystem und ist $B$ ein beliebiges Ereignis dann gilt @thm-totale-wskt. $\square$
:::

::: {#thm-totale-wskt}
### Totale Wahrscheinlichkeit
$$Pr(B) = \sum_{i=1}^n Pr(A_i) \cdot Pr(B|A_i).\square$$
:::


Man kann die totale Wahrscheinlichkeit auffassen als die *Summe der gewichteten Teil-Wahrscheinlichkeiten*.

::: {exm-totwskt1}
In @fig-tot-wskt (@exm-bourier-s56) gilt $Pr(B) = 0.6 \cdot 0.05 + 0.1 \cdot 0.02 + 0.3 \cdot  0.04 = 0.03 + 0.002 + 0.012 = 0.04.\square$
:::





:::{#exr-bertie-botts2}
### Bertie Botts Bohnen jeder Geschmacksrichtung, Teil 2

```{r}
#| echo: false
Pr_keine_Bohne <- 17/20 * 16/19 * 15/18
Pr_nicht_keine_Bohne <- round(1 - Pr_keine_Bohne, 2)
```


Es ist die gleich Aufgabe wie @exm-bertie-botts, aber jetzt lautet die Frage: 
Wie gro√ü ist die Wahrscheinlichkeit, *mindestens eine* scheu√üliche Bohne bei 3 Z√ºgen zu erwischen?^[Die Wahrscheinlichkeit *keine* scheu√üliche Bohne zu ziehen ist $17/20 \cdot 16/19 \cdot 15/18) \approx `r Pr_keine_Bohne`$. 
Daher ist die gesuchte Wahrscheinlichkeit (mindestens eine scheu√üliche Bohne) 
das Komplement davon:  $`r Pr_nicht_keine_Bohne`$.] $\square$
:::




### Baumsammlung

Baumdiagramme sind ein hilfreiches Werkzeug f√ºr wiederholte Zufallsexperimente.
Daher ist hier eine "Baumsammlung"^[Wald?] zusammengestellt.

- Sie werfen 1 M√ºnze, @fig-baummuenz1.
- Sie werfen 2 M√ºnzen, @fig-zweimuenzen.
- Sie werfen 3 M√ºnzen, @fig-dreimuenzen.
- Sie werfen 4 M√ºnzen, @fig-4muenzen.
- Sie werfen 9 M√ºnzen, ü§Ø @fig-binom2.


## Vertiefung



Bei @henze2019 findet sich eine anspruchsvollere Einf√ºhrung in das Rechnen mit Wahrscheinlichkeit; dieses Kapitel behandelt ein Teil des Stoffes  der Kapitel 2 und 3 von @henze2019.

Mit [dieser App, die ein zweistufiges Baumdiagramm zeigt](https://www.geogebra.org/m/n4xrk4x5), k√∂nnen Sie das Verhalten von verschiedenen Arten von Wahrscheinlichkeiten weiter untersuchen.

[Diese App l√§sst dich herausfinden, ob man wirklich krank ist, wenn der Arzt es bheauptet.](https://www.geogebra.org/m/tkdfxa8f)

Das [Video zu Bayes von 3b1b](https://youtu.be/HZGCoVF3YvM) verdeutlicht das Vorgehen der Bayes-Methode auf einfache und anschauliche Weise.

@mittag2020 stellen in Kap. 11 die Grundlagen der Wahrscheinlichkeitstheorie vor.
√Ñhnliche Darstellungen finden sich in einer gro√üen Zahl an Lehrb√ºchern.


## Aufgaben

Bearbeiten Sie die Aufgabe in der angegeben Literatur.

Die Webseite [datenwerk.netlify.app](https://datenwerk.netlify.app) stellt eine Reihe von einschl√§gigen √úbungsaufgaben bereit. Sie k√∂nnen die Suchfunktion der Webseite nutzen, 
um die Aufgaben mit den folgenden Namen zu suchen:


### Paper-Pencil-Aufgaben



1. [Additionssatz1](https://datenwerk.netlify.app/posts/additionssatz1/additionssatz1.html)
2. [Nerd-gelockert](https://datenwerk.netlify.app/posts/nerd-gelockert/nerd-gelockert.html) 
3. [Urne1](https://datenwerk.netlify.app/posts/urne1/urne1.html) 
3. [Urne2](https://datenwerk.netlify.app/posts/urne2/urne2.html)
3. [k-coins-k-hits](https://datenwerk.netlify.app/posts/k-coins-k-hits/k-coins-k-hits.html)
5. [sicherheit](https://datenwerk.netlify.app/posts/sicherheit/sicherheit.html)
4. [sicherheit2](https://datenwerk.netlify.app/posts/sicherheit2/sicherheit2.html)
6. [Klausuren-bestehen](https://datenwerk.netlify.app/posts/klausuren-bestehen/klausuren-bestehen.html)
3. [Gem-Wskt1](https://datenwerk.netlify.app/posts/gem-wskt1/gem-wskt1.html)
3. [Gem-Wskt2](https://datenwerk.netlify.app/posts/gem-wskt2/gem-wskt2.html)
3. [Gem-Wskt3](https://datenwerk.netlify.app/posts/gem-wskt3/gem-wskt3.html)
3. [Gem-Wskt4](https://datenwerk.netlify.app/posts/gem-wskt4)
3. [wuerfel05](https://datenwerk.netlify.app/posts/wuerfel05/wuerfel05)
3. [wuerfel06](https://datenwerk.netlify.app/posts/wuerfel06/wuerfel06)
3. [Bed-Wskt1](https://datenwerk.netlify.app/posts/bed-wskt1/bed-wskt1.html)
3. [Bed-Wskt2](https://datenwerk.netlify.app/posts/bed-wskt2/bed-wskt2.html)
3. [Bed-Wskt3](https://datenwerk.netlify.app/posts/bed-wskt3/bed-wskt3.htmlv)
3. [voll-normal](https://datenwerk.netlify.app/posts/voll-normal)
3. [corona-blutgruppe](https://datenwerk.netlify.app/posts/corona-blutgruppe/corona-blutgruppe.html)
3. [totale-Wskt1](https://datenwerk.netlify.app/posts/totale-wskt1/totale-wskt1.html)
5. [wskt-quiz14](https://datenwerk.netlify.app/posts/wskt-quiz14/wskt-quiz14.html)
5. [wskt-quiz09](https://datenwerk.netlify.app/posts/wskt-quiz09/wskt-quiz09.html)



### Aufgaben, f√ºr die man einen Computer braucht

1. [mtcars-abhaengig](https://datenwerk.netlify.app/posts/mtcars-abhaengig/mtcars-abhaengig.html)
3. [mtcars-abhaengig-var2](https://datenwerk.netlify.app/posts/mtcars-abhaengig_var2/mtcars-abhaengig_var2)
3. [mtcars-abhaengig_var3a](https://datenwerk.netlify.app/posts/mtcars-abhaengig_var3a/mtcars-abhaengig_var3a)
4. [wskt-df-r](https://datenwerk.netlify.app/posts/wskt-df-r/wskt-df-r) 






## ---



![](img/outro-03.jpg){width=100%}






