

# Hallo, Wahrscheinlichkeit



<!-- TODO Dieses Kapitel ist zu lang, zu voll. -->

```{r}
#| include: false
library(tidyverse)
library(easystats)
library(gmp)  # function "isprime"
library(titanic)
library(knitr)
library(DT)
library(kableExtra)
library(ggraph)


source("funs/uniformplot.R")
source("funs/binomial_plot.R")
```




```{r r-setup}
#| echo: false
#| message: false
theme_set(theme_minimal())
scale_colour_discrete <- function(...) 
  scale_color_okabeito()
```


```{r define-plots-16}
#| echo: false
#| fig-width: 7

plot16a <-
  ggplot(data.frame(A = c(0, 1),
                    B = c(0, 1))) +
  aes(x = A, y = B) +
  annotate("rect", xmin = 0, xmax = 1, ymin = 0, ymax = .5,
           color = "#009E73FF", alpha = .7, fill = NA) +
  annotate("rect", xmin = 0, xmax = 1, ymin = 0.5, ymax = 1,
           color = NA, alpha = .5, fill = "#009E73FF") +
  annotate("rect", xmin = 0, xmax =0.5, ymin = 0, ymax = 1,
           color = "#56B4E9FF", alpha = .7, fill = NA) +
  annotate("rect", xmin = 0.5, xmax = 1, ymin = 0, ymax = 1,
           color = "#56B4E9FF", alpha = .5, fill = "#56B4E9FF") +
  scale_x_continuous(breaks = c(0.25, 0.75), labels = c("¬¨A", "A")) +
  scale_y_continuous(breaks = c(0.25, 0.75), labels = c("¬¨B", "B")) +
  theme_minimal()  +
  theme(axis.text = element_text(size = 18)) +
  labs(x = "", y = "")


plot_pr_b <-
  ggplot(data.frame(A = c(0, 1),
                    B = c(0, 1))) +
  aes(x = A, y = B) +
  annotate("rect", xmin = 0, xmax = 1, ymin = 0, ymax = .5,
           color = "grey", alpha = .7, fill = NA) +
  annotate("rect", xmin = 0, xmax = 1, ymin = 0.5, ymax = 1,
           color = NA, alpha = .5, fill = "#009E73FF") +
  scale_x_continuous(breaks = c(0.25, 0.75), labels = c("¬¨A", "A")) +
  scale_y_continuous(breaks = c(0.25, 0.75), labels = c("¬¨B", "B")) +
  annotate("label", x = .5, y = .75, label = "Pr(B) = 50%") +
  theme_minimal() +
  theme(axis.text = element_text(size = 18)) +
  labs(x = "", y = "")

plot_pr_a <-
  ggplot(data.frame(A = c(0, 1),
                    B = c(0, 1))) +
  aes(x = A, y = B) +
  annotate("rect", xmin = 0, xmax =0.5, ymin = 0, ymax = 1,
           color = "grey", alpha = .7, fill = NA) +
  annotate("rect", xmin = 0.5, xmax = 1, ymin = 0, ymax = 1,
           color = "#56B4E9FF", alpha = .5, fill = "#56B4E9FF") +
  scale_x_continuous(breaks = c(0.25, 0.75), labels = c("¬¨A", "A")) +
  scale_y_continuous(breaks = c(0.25, 0.75), labels = c("¬¨B", "B")) +
  annotate("label", x = .75, y = .5, label = "Pr(A) = 50%") +
  theme_minimal()  +
  theme(axis.text = element_text(size = 18)) +
  labs(x = "", y = "")


plot_pr_ab <-
ggplot(data.frame(A = c(0, 1),
                  B = c(0, 1))) +
  aes(x = A, y = B) +
  annotate("rect", xmin = 0, xmax = 1, ymin = 0, ymax = .5,
           color = "#009E73FF", alpha = .7, fill = NA) +
  annotate("rect", xmin = 0, xmax = 1, ymin = 0.5, ymax = 1,
           color = NA, alpha = .5, fill = "#009E73FF") +
  annotate("rect", xmin = 0, xmax =0.5, ymin = 0, ymax = 1,
           color = "#56B4E9FF", alpha = .7, fill = NA) +
  annotate("rect", xmin = 0.5, xmax = 1, ymin = 0, ymax = 1,
           color = "#56B4E9FF", alpha = .5, fill = "#56B4E9FF") +
  annotate("rect", xmin = 0.5, xmax = 1, ymin = 0.5, ymax = 1,
           color = "#E69F00FF", alpha = .7, fill = NA, linewidth = 2) +
  scale_x_continuous(breaks = c(0.25, 0.75), labels = c("¬¨A", "A")) +
  scale_y_continuous(breaks = c(0.25, 0.75), labels = c("¬¨B", "B")) +
  annotate(geom = "label", x = .75, y = 0.75, label = "A,B", color = "#E69F00FF") +
  theme_minimal()  +
  theme(axis.text = element_text(size = 18)) +
  labs(x = "", y = "")






plot_pr_b_geg_a <-
  ggplot(data.frame(A = c(0, 1),
                    B = c(0, 1))) +
  aes(x = A, y = B) +
  annotate("rect", xmin = 0, xmax = 1, ymin = 0.5, ymax = 1,
           color = "#009E73FF", alpha = .3, fill = NA, linewidth = 2) +
  annotate("rect", xmin = 0, xmax = 1, ymin = 0.5, ymax = 1,
           color = NA, alpha = .5, fill = "NA") +
  # annotate("rect", xmin = 0, xmax =0.5, ymin = 0, ymax = 1,
  #          color = "#56B4E9FF", alpha = .7, fill = NA) +
  annotate("rect", xmin = 0.5, xmax = 1, ymin = 0, ymax = 1,
           color = "#56B4E9FF", alpha = .7, fill = "#56B4E9FF", linewidth = 2) +
  annotate("rect", xmin = 0.5, xmax = 1, ymin = 0.5, ymax = 1,
           color = "#E69F00FF", alpha = .3, fill = "#E69F00FF", linewidth = 2) +
  scale_x_continuous(breaks = c(0.25, 0.75), labels = c("¬¨A", "A")) +
  scale_y_continuous(breaks = c(0.25, 0.75), labels = c("¬¨B", "B")) +
  annotate(geom = "label", x = .75, y = 0.75, label = "B|A", color = "#E69F00FF")  +
  theme(axis.text = element_text(size = 18)) +
  labs(x = "", y = "")


```

## Lernsteuerung


### Position im Modulverlauf

@fig-modulverlauf gibt einen √úberblick zum aktuellen Standort im Modulverlauf.




### √úberblick

Dieses Kapitel hat die Wahrscheinlichkeitstheorie (synonym: Wahrscheinlichkeitsrechnung) bzw. das Konzept der Wahrscheinlichkeit zum Thema.^[Die Wahrscheinlichkeitstheorie bildet zusammen mit der Statistik das Fachgebiet der Stochastik.]
Es geht sozusagen um die Mathematik des Zufalls.



### Wozu brauche ich dieses Kapitel?

Im wirklichen Leben sind Aussagen (Behauptungen) so gut wie nie sicher.

- "Weil sie so schlau ist, ist sie erfolgreich."
- "In Elektroautos liegt die Zukunft."
- "Das klappt sicher, meine Meinung."
- "Der n√§chste Pr√§sident wird XYZ."

Aussagen sind nur *mehr oder weniger* (graduell) sicher.
Wir k√∂nnen die Regeln der Wahrscheinlichkeitslogik verwenden, um den Grad der Sicherheit (von ganz unsicher bis ganz sicher) zu pr√§zisieren.
Daher sagt man auch, Wahrscheinlichkeit sei die Logik der Wissenschaft [@jaynes2003].



### Lernziele

Nach Absolvieren des jeweiligen Kapitels sollen folgende Lernziele erreicht sein.

Sie k√∂nnen ...


- die Grundbegriffe der Wahrscheinlichkeitstheorie erl√§uternd definieren
- die drei Arten der direkten Ermittlung von Wahrscheinlichkeit erl√§utern
- typische Relationen (Operationen) von Ereignissen anhand von Beispielen veranschaulichen
- erl√§utern, was eine Zufallsvariable ist



### Begleitliteratur

Lesen Sie zur Begleitung dieses Kapitels @bourier2011, Kap. 2-4. 


### Eigenstudium


:::{.callout-important}
Dieses Kapitel ist selbst√§ndig im Eigenstudium vorzubereiten vor dem Unterricht.
Lesen Sie dazu die angegebene Literatur.$\square$
:::


### Pr√ºfungsrelevanter Stoff

Der Stoff dieses Kapitels deckt sich (weitgehend) mit @bourier2011, Kap. 2-4. 
Weitere √úbungsaufgaben finden Sie im dazugeh√∂rigen √úbungsbuch, @bourier2022.

:::callout-note
In Ihrer [Hochschul-Bibliothek kann das Buch als Ebook verf√ºgbar](https://fantp20.bib-bvb.de/TouchPoint/singleHit.do?methodToCall=showHit&curPos=3&identifier=2_SOLR_SERVER_1157422278) sein. 
Pr√ºfen Sie, ob Ihr Dozent Ihnen weitere Hilfen im [gesch√ºtzten Bereich (Moodle)](https://moodle.hs-ansbach.de/mod/resource/view.php?id=136047) eingestellt hat.$\square$
:::





### Zentrale Begriffe


#### Grundbegriffe

- Zufallsvorgang (Zufallsexperiment)
- Elementarereignis
- Ereignisraum
- Zufallsereignis (zuf√§lliges Ereignis)
- Sicheres Ereignis
- Unm√∂gliches Ereignis


#### Wahrscheinlichkeitsbegriffe

- Klassische Wahrscheinlichkeit (LaPlace'sche Wahrscheinlichkeit)
- Statistische (empirische) Wahrscheinlichkeitsermittlung
- Subjektive (Bayes) Wahrscheinlichkeitsermittlung


#### Wahrscheinlichkeitsrelationen

- Vereinigung von Ereignissen
- Schnitt(menge) von Ereignissen
- Komplement√§rereignis
- Vollst√§ndiges Ereignissystem
- Anforderungen an eine Definition von Wahrscheinlichkeit


#### Wahrscheinlichkeitsrechnung

- Allgemeiner Additionsssatz
- Disjunkte Ereignisse
- Additionssatz f√ºr disjunkte Ereignisse
- Bedingte Wahrscheinlichkeit
- (Stochastische) Unabh√§ngigkeit
- Baumdiagramm f√ºr gemeinsame Wahrscheinlichkeit
- Allgemeiner Multiplikationssatz
- Multiplikationssatz f√ºr unabh√§ngige Ereignisse
- Totale Wahrscheinlichkeit
- Satz von Bayes




### Begleitvideos


- [Video zum Thema Wahrscheinlichkeit](https://youtu.be/rR6NspapEyo)




<!-- ## Unterst√ºtzung: Wahrscheinlichkeit in Bildern -->

<!-- Wahrscheinlichkeit in Bildern: zur einfachen Erschlie√üung des Materials, -->
<!-- ein Unterst√ºtzungsangebot. -->


<!-- Im Folgenden sind einige Schl√ºsselbegriffe und -regeln in (ver-)einfach(t)er Form schematisch bzw. visuell dargestellt mit dem Ziel, den Stoff einfacher zu erschlie√üen. -->





## Grundbegriffe





:::{#exm-muenz}
Klassisches Beispiel f√ºr einen Zufallsvorgang ist das (einmalige oder mehrmalige) Werfen einer M√ºnze.$\square$

Werfen Sie eine M√ºnze!
Diese hier zum Beispiel:

![](img/1024px-Coin-155597.svg.png){width=10% fig-align="center"}

[Quelle: By OpenClipartVectors, CC0]( https://pixabay.com/pt/moeda-euro-europa-fran%C3%A7a-dinheiro-155597)



Wiederholen Sie den Versuch 10 Mal.

Das reicht Ihnen nicht? Okay, wiederholen Sie den Versuch 100, nein 1000, nein: $10^6$ Mal.^[$10^6 = 1000000$]

Notieren Sie als Ergebnis, wie oft die Seite mit der Zahl oben liegen kommt ("Treffer").$\square$
:::


Oder probieren Sie die [App der Brown University](https://seeing-theory.brown.edu/basic-probability/index.html#section1), 
wenn Sie keine Sehnenscheidenentz√ºndung bekommen wollen.


:::{#def-zufall}
### Zufallsvorgang
Ein *Zufallsvorgang* oder *Zufallsexperiment* ist eine einigerma√üen klar beschriebene T√§tigkeit, deren Ergebnis nicht sicher ist. 
Allerdings ist die Menge m√∂glicher Ergebnisse bekannt und die Wahrscheinlichkeit f√ºr alle Ergebnisse kann quantifiziert werden.$\square$
:::


:::{#exr-zufall2}
Nennen Sie Beispiele f√ºr Zufallsvorg√§nge!\square^[Beispiele f√ºr Zufallsexperimente sind das Werfen einer M√ºnze, das Ziehen einer Karte aus einem Kartenspiel, 
das Messen eines Umweltph√§nomens wie der Temperatur oder die Anzahl der Kunden, 
die einen Laden betreten. 
In jedem dieser F√§lle sind die m√∂glichen Ergebnisse nicht im Voraus bekannt und h√§ngen von nicht komplett bekannten Faktoren ab.]
:::


:::callout-caution
Zufall hei√üt nicht, dass ein Vorgang keine Ursachen h√§tte. 
So gehorcht der Fall einer M√ºnze komplett den Gesetzen der Gravitation. 
W√ºrden wir diese Gesetze und die Ausgangsbedingungen (Luftdruck, Fallh√∂he, Oberfl√§chenbeschaffenheit, Gewichtsverteilungen, ...) exakt kennen, k√∂nnten wir theoretisch sehr genaue Vorhersagen machen. 
Der "Zufall" w√ºrde aus dem M√ºnzwurf verschwinden. Man sollte "Zufall" also besser verstehen als "unbekannt".$\square$
::::


:::{#exr-w√ºrfel-geo}
[Mit dieser App](https://www.geogebra.org/m/cbqee8h7) k√∂nnen Sie W√ºrfelw√ºrfe simulieren und die Ausg√§nge dieses Zufallsexperiments beobachten.$\square$
:::



:::{#def-ereignisraum}
### Ereignisraum
Die m√∂glichen Ergebnisse eines Zufallvorgangs fasst man als Menge mit dem Namen *Ereignisraum*^[leider gibt es eine F√ºlle synonymer Namen: *Ereignisraum*, *Elementarereignisraum*, *Ergebnisraum* oder *Grundraum*] zusammen. 
Man verwendet den griechischen Buchstaben $\Omega$ f√ºr diese Menge.
Die Elemente $\omega$ (kleines Omega) von $\Omega$ nennt man *Ergebnisse*.$\square$
:::

:::{#exm-grundraum}
Beobachtet man beim W√ºrfelwurf (s. @fig-wuerfel) die oben liegende Augenzahl, so ist 



$$\Omega = \{ 1,2,3,4,5,6 \} = \{‚öÄ, ‚öÅ, ‚öÇ, ‚öÉ, ‚öÑ, ‚öÖ\}$$

ein nat√ºrlicher Grundraum [@henze2019].$\square$
:::


Die Wahrscheinlichkeitsrechnung baut auf der Mengenlehre auf, daher wird die Notation  der Mengenlehre hier verwendet.



::: {.content-visible when-format="html"}
![Ein (sechsseitiger) W√ºrfel, Bildquelle: Peter Steinberg, Wikipedia, CC-BY-](img/120px-Hexahedron-slowturn.gif){#fig-wuerfel width=10%}

:::

::: {.content-visible unless-format="html"}


![Ein (sechsseitiger) W√ºrfel](img/Dice_2005.jpg){width=33%}

[Bildquelle: CC BY-SA 3.0](https://commons.wikimedia.org/w/index.php?curid=474827)
:::







:::{#def-ereignis}
### Ereignis
Jede Teilmenge^[$A$ ist eine Teilmenge von $B$, 
wenn alle Elemente von $A$ auch Teil von $B$ sind.] von $\Omega$ hei√üt *Ereignis*; $A \subseteq \Omega$ .$\square$
:::



:::{#exm-ereignis}
Beim Mensch-√§rger-dich-nicht Spielen habe ich eine 6 geworfen.^[Schon wieder.]
Das Nennen wir das Ereignis $A$: "Augenzahl 6 liegt oben" und schreiben in Kurzform:

$A= \{6\}\square$
:::


:::{#exm-muenzwurf}
Sie werfen eine M√ºnze (Sie haben keinen Grund, an ihrer Fairness zu zweifeln). "Soll ich jetzt lernen f√ºr die Klausur (Kopf) oder lieber zur Party gehen (Zahl)?"

@fig-baummuenz1 zeigt die m√∂glichen Ausg√§nge (T wie Treffer (Party) und N  (Niete, Lernen)) dieses Zufallexperiments.

```{mermaid}
%%| label: fig-baummuenz1
%%| fig-cap: Sie werfen eine M√ºnze. Party oder Lernen???
flowchart LR
 M[Sie werfen die M√ºnze] --> T["T (Treffer) ü•≥"]
  M --> N["N (Niete) üìö"]
```

Das Ereignis *Zahl* ist eingetreten! Treffer! Gl√ºck gehabt!^[?]$\square$
:::






:::{#def-unm-sich}
### Unm√∂gliches und sicheres Ereignis
Die leere Menge $\varnothing$ hei√üt das *um√∂gliche*, der Grundraum $\Omega$ hei√üt das *sichere Ereignis*. $\square$
:::

:::{#exm-unm}
### Unm√∂gliches Ereignis
Alois behauptet, er habe mit seinem W√ºrfel eine 7 geworfen.
Schorsch erg√§nzt, sein W√ºrfel liege auf einer Ecke, so dass keine Augenzahl oben liegt.
Draco hat seinen W√ºrfel runtergeschluckt. 
Dracos und Alois' Ereignisse sind *unm√∂gliche Ereignisse*, zumindest nach unserer Vorstellung des Zufallsexperiments.$\square$
:::


:::{#exm-sicher}
### Sicheres Ereignis
Nach dem der W√ºrfel geworfen wurde, liegt eine Augenzahl zwischen 1 und 6 oben.$\square$
:::


:::{#def-elementarereignis}
### Elementarereignis

Jede einelementige Teilmenge $\{\omega\}$ von $\Omega$ hei√üt *Elementarereignis* (h√§ufig mit $A$ bezeichnet).
^[Ein *Ergebnis* ist ein Element von $\Omega$. Elementarereignisse sind die einelementigen Teilmengen von $\Omega$. Konzeptionell sind die beiden Begriffe sehr √§hnlich, vgl. <https://de.wikipedia.org/wiki/Ergebnis_(Stochastik)>. Wir werden uns hier auf den Begriff *Elementarereignis* konzentrieren und den Begriff *Ergebnis* nicht weiter verwenden.] $\square$
:::


:::{#exm-elementarereignis}
### Elementarereignis

- Sie spielen Mensch-√§rger-dich-nicht. Und brauchen dringend eine `6`. Sie w√ºrfeln. Das Ereignis $A = \{1\}$ tritt ein.^[Na toll.]

- Sie schreiben eine Statistik-Klausur. Irgendwie haben Sie das Gef√ºhl, das Ergebnis sei ein Zufallsexperiment... Jedenfalls k√∂nnen nach Adam Riese zwei Dinge passieren: $\Omega= \{\text{bestehen, nicht bestehen}\}$.
Das erste der beiden Elementarereignisse tritt ein. Yeah!

- Sie f√ºhren eine Studie durch zur Wirksamkeit einer Lern-App. Es ist nicht klar, ob die App wirklich was bringt f√ºr den Lernerfolg. Vereinfacht gesprochen ist der Grundraum dieses Experiments: $\Omega = \{\text{schadet, bringt nichts, n√ºtzt}\}$.
Die Daten sprechen f√ºr das Ereignis $A = \{\text{bringt nichts}\}$.
:::




:::{#def-vollereignis}
### Vollst√§ndiges Ereignissystem
Wird der Grundraum $\Omega$ vollst√§ndig in paarweis disjunkte Ereignisse zerlegt, so bilden diese Ereignisse ein vollst√§ndiges Ereignissystem, s. @fig-vollereignis.$\square$
:::


![Zerlegung des Grundraums in ein vollst√§ndiges Ereignissystem](img/vollereignis.png){#fig-vollereignis width=50%}



:::::{#exm-vollereig1}
Sei $\Omega$ der typische Ereignisraum des W√ºrfelwurfs. Wir zerlegen den Grundraum in zwei Ereignisse, $A$ "gerade Zahlen", und $B$ "ungerade Zahlen". 
Damit haben wir ein vollst√§ndiges Ereignissystem erstellt, s. @fig-complete-event-system1.


::::{#fig-complete-event-system1}

::: {.figure-content}


\begin{align}
A = \{2,4,6\} \qquad \hfill \boxed{\color{gray}{1}\; \boxed{\color{black}{2}}\; \color{gray}{3}\; \boxed{\color{black}{4}}\; \color{gray}{5}\; \boxed{\color{black}{6}}\;} \\
B = \{1,3,5\} \qquad  \hfill \boxed{\boxed{\color{black}{1}}\; \color{gray}{2}\; \boxed{\color{black}{3}}\; \color{gray}{4}\; \boxed{\color{black}{5}}\; \color{gray}{6}\; } \\
\hline \\
\Omega = \{1,2,3,4,5,6\}  \qquad  \hfill \boxed{1\; 2\; 3\; 4\; 5\; 6 } 

\end{align}
:::

::::

Ein Beispiel f√ºr ein vollst√§ndiges Ereignissystem

:::::


:::::{#exm-vollereig2}
Sei $\Omega$ der typische Ereignisraum des W√ºrfelwurfs. Wir zerlegen den Grundraum in zwei Ereignisse, $A$ "1,2,3", und $B$ "4,5,6". 
Damit haben wir ein vollst√§ndiges Ereignissystem erstellt, s. @fig-complete-event-system1.

::::{#fig-fig-complete-event-system2}

::: {.figure-content}


\begin{align}
A = \{1,2,3\} \qquad \qquad \hfill  \boxed{\boxed{ \color{black}{1\; 2\; 3}}\; \color{gray}{4\; 5\; 6}} \\
B = \{4,5, 6\} \qquad \qquad  \hfill \boxed{\color{gray}{1 \; 2 \; 3}\; \boxed{\color{black}{4\; 5 \; 6}}} \\

\newline
\hline \\
\Omega = \{1,2,3,4,5,6\} \qquad \qquad \hfill  \boxed{1\; 2\; 3\; 4\; 5\;6}
\end{align}
:::

Noch ein Beispiel f√ºr ein vollst√§ndiges Ereignissystem

::::
:::::


:::{#def-macht}
### M√§chtigkeit
Die Anzahl der Elementarereignisse eines Ereignismraums nennt man die M√§chtigkeit (des Ereignisraums).^[Die Menge aller Teilmengen einer Menge $A$ nennt man die *Potenzmenge* $\mathcal{P}(A)$, vgl. [hier](https://de.wikipedia.org/wiki/Datei:Hasse_diagram_of_powerset_of_3.svg).]$\square$
:::


Die M√§chtigkeit von $\Omega$ bezeichnet man mit dem Symbol $|\Omega|$.

:::{#exm-macht}
Beim Wurf eines W√ºrfels mit $\Omega=\{1,2,3,4,5,6\}$ gibt es 6 Elementarereignisse. 
Die M√§chtigkeit ist also 6: $|\Omega|=6$.$\square$
:::

## Was ist Wahrscheinlichkeit?

Wir haben schon mit @def-wskt eine erste Definition von Wahrscheinlichkeit versucht. 
Jetzt gehen wir die Sache noch etwas n√§her an und vergleichen verschiedene Ideen (Definitionen) von Wahrscheinlichkeit.


### Formallogische Definitition

Die formallogische Konzeption von Wahrscheinlichkeit sieht Wahrscheinlichkeit als Erweiterung der formalen Logik [@jaynes2003].
^[Manchmal wird diese Art der Wahrscheinlichkeit auch *epistemologische* Wahrscheinlichkeit genannt.] 
In der formalen Logik ist ein Ereignis entweder *falsch* oder *wahr*. 
In der formallogischen Konzeption wird der Platz zwischen "falsch" (0) und "richtig" 
(1)
als die Wahrscheinlichkeit $0<p<1$, gesehen [@briggs2016], s. @fig-prob-logic.


![Wahrscheinlichkeit als Erweiterung der Logik](img/prob-logic.png){#fig-prob-logic}

Nach dieser "Wahrscheinlichkeitslogik" kann man ein Ereignis, 
von dessen Eintreten man "wenig √ºberzeugt" ist, z.B. mit 0.2 quantifizieren. 
Hingegen einem Ereignis, von man "recht sicher" ist, mit 0.8 quantifizieren, s. @fig-prob-logic2.


![Ein Ereignis von dessen Eintreten man gering bzw. stark √ºberzeugt ist](img/prob-logic2.png){#fig-prob-logic2}


:::{#def-indifferenzprinzip}
### Indifferenzprinzip

Das Indifferenzprinzip (synonym: Prinzip des unzureichenden Grundes) besagt, 
dass in Abwesenheit jeglicher Informationen, die bestimmte Ereignisse bevorzugen oder benachteiligen w√ºrden, 
alle m√∂glichen Ereignisse als gleich wahrscheinlich angesehen werden sollten. $\square$
:::


Vor uns liegt ein W√ºrfel. Schlicht, ruhig, unbesonders.
Wir haben keinen Grund anzunehmen, dass eine seiner $n=6$ Seiten bevorzugt nach oben zu liegen kommt. 
Jedes der sechs Elementarereignisse ist uns gleich plausibel;
der W√ºrfel erscheint uns fair.
In Ermangelung weiteres Wissens zu unserem W√ºrfel gehen wir schlicht davon aus, dass jedes der $n$ Elementarereignis gleich wahrscheinlich ist.
Es gibt keinerlei Notwendigkeit, den W√ºrfel in die Hand zu nehmen,
um zu einer Wahrscheinlichkeitsaussage auf diesem Weg zu kommen.
Nat√ºrlich *k√∂nnten* wir unsere Auffassung eines fairen W√ºrfels testen,
aber auch ohne das Testen k√∂nnen wir eine stringente Aussage (basierend auf dem Indifferenzprinzip (s. @def-indifferenzprinzip) der $n$ Elementarereignisse) zur Wahrscheinlichkeit eines bestimmten (Elementar-)Ereignisses $A$ kommen [@briggs2016], s. @thm-briggs.


:::{#thm-briggs}

### Indifferenzprinzip

$$Pr(A) = \frac{1}{n}= \frac{1}{|\Omega|} \quad \square$$
:::

:::{#exm-briggs}
Sei $A$ = "Der W√ºrfel wird beim n√§chsten Wurf eine 6 zeigen."
Die Wahrscheinlichkeit f√ºr $A$ ist $1/6. \square$
:::


:::{#def-laplace}
### Laplace-Experimt
Ein Zufallsexperiment, bei dem alle Elementarereignisse dieselbe Wahrscheinlichkeit haben, nennt man man ein *Laplace-Experiment*, s. @thm-laplace. $\square$
:::

In Erweiterung von @thm-briggs k√∂nnen wir f√ºr ein Laplace-Experiment schreiben, s. @thm-laplace.

:::{#thm-laplace}

### Laplace-Experiment

$$Pr(A)=\frac{\text{Anzahl Treffer}}{\text{Anzahl m√∂glicher Ergebnisse}} \quad \square$$
:::


### Frequentistische Definition

In Ermangelung einer Theorie zum Verhalten eines (uns) unbekannten Zufallsvorgangs und unter der Vermutung, dass die Elementarereignisse nicht gleichwahrscheinlich sind, bleibt uns ein einfacher (aber aufw√§ndiger) Ausweg: Ausprobieren.

Angenommen, ein Statistik-Dozent, bekannt f√ºr seine Vorliebe zum Gl√ºcksspiel und 
mit scheinbar endlosen Gl√ºcksstr√§hnen (er wirft andauernd eine 6), 
hat seinen Lieblingsw√ºrfel versehentlich liegen gelassen. 
Das ist *die* Gelegenheit!
Sie greifen sich den W√ºrfel, und ... Ja, was jetzt?
Nach kurzer √úberlegung kommen Sie zum Entschluss, den W√ºrfel einem "Praxistest" zu unterziehen: 
Sie werfen ihn 1000 Mal (Puh!) und z√§hlen den Anteil der `6`.
Falls der W√ºrfel fair ist, m√ºsste gelten $Pr(A=6)=1/6\approx .17$. Schauen wir mal!



```{r}
#| echo: false
n <- 1e3

set.seed(42)
wuerfel_oft <- 
  sample(x = 1:6, size = n, replace = TRUE) 


wuerfel_tab <-
  tibble(
    id = 1:n,
    x = wuerfel_oft,
    ist_6 = ifelse(x == 6, 1, 0),
    ist_6_cumsum = cumsum(ist_6) / id
  )

```


Und hier der Anteil von  `6` im Verlauf unserer W√ºrfe, s. @fig-wuerfel.


```{r}
#| label: fig-wuerfel
#| fig-cap: "Das Gesetz der gro√üen Zahl am Beispiel der Stabilisierung des Trefferanteils beim wiederholten W√ºrfelwurf"
#| fig-asp: 0.5
#| echo: false

wuerfel_tab %>% 
  slice_head(n = 1e3) %>% 
  ggplot() +
  aes(x = id, y = ist_6_cumsum) +
  geom_hline(yintercept = 1/6, color = "grey80", size = 3) +
  geom_line() +
  labs(x = "Nummer des W√ºrfelwurfs",
       y = "Kummulierte H√§ufigkeit einer Sechs") +
  annotate("label", x = 1000, y = 1/6, label = "0.17")
```

Hm, auf den ersten Blick ist kein (starkes) Anzeichen f√ºr Schummeln bzw. einen gezinkten W√ºrfel zu finden.


### Subjektive Definition

Um subjektiv zu einer Wahrscheinlichkeit zu kommen, sagt man einfach seine Meinung.
Das h√∂rt sich nat√ºrlich total plump an. 
Und tats√§chlich besteht die Gefahr, dass die so ermittelten Wahrscheinlichkeiten aus der Luft gegriffen, also haltlos, sind.

Allerdings kann diese Art von Wahrscheinlichkeitsermittlung auch sehr wertvoll sein.
In komplizierten Situation im echten Leben^[die sog. "Praxis"] kommt man oft in die Situation, 
dass weder die formallogischen noch die frequentistische Variante verwendet werden kann.
Dann muss man auf Sch√§tzungen, Vorwissen, Erfahrung, theoretischen √úberlegungen etc. zur√ºckgreifen.





### Kolmogorovs Definition {#sec-kolmogorov}

Wir richten eine Reihe von Forderungen an eine Definition von bzw. an das Rechnen mit Wahrscheinlichkeiten, die direkt plausibel erscheinen:^[Ein Herr Kolmogorov hat das mal aufgeschrieben.]

1. *Nichtnegativit√§t*: Die Wahrscheinlichkeit eines Ereignisses kann nicht negativ sein.
2. *Normierung*: Das sichere Ereignis hat die Wahrscheinlichkeit 1 bzw. 100%: $Pr(\Omega)=1$; das unm√∂gliche Ereignis hat die Wahrscheinlichkeit 0: $Pr(\emptyset)=0$.
3. *Additivit√§t*. Sind $A$ und $B$ disjunkt, dann ist die Wahrscheinlichkeit, 
dass mindestens eines der beiden Ereignisse eintritt ($A\cup B$) gleich der Summe der beiden Einzelwahrscheinlichkeiten von $A$ und $B$.



## Relationen von Ereignissen

F√ºr das Rechnen mit Wahrscheinlichkeiten ist es hilfreich, ein paar Werkzeuge zu kennen, die wir uns im Folgenden anschauen.

:::{#def-relation}
### Relation
Eine Relation (zweier Ereignisse) bezeichnet die Beziehung, 
in der die beiden Ereignisse zueinander stehen. $\square$
:::

Typische Relationen sind Gleichheit, Ungleichheit, Vereinigung, Schnitt.


### √úberblick


Wir gehen von Grundraum $\Omega$ aus, mit dem Ereignis $A$ als Teilmenge von $\Omega$: $A \subset \Omega$.


Da wir Ereignisse als Mengen auffassen, verwenden wir im Folgenden die beiden Begriffe synonym.


Dabei nutzen wir u.a. Venn-Diagramme.
Venn-Diagramme eigenen sich, um typische Operationen (Relationen) auf Mengen zu visualisieren. Die folgenden Venn-Diagramme stammen von [Wikipedia (En)](https://en.wikipedia.org/wiki/Venn_diagram).

:::callout-note
### Wozu sind die Venn-Diagramme gut? Warum soll ich die lernen?
Venn-Diagramme zeigen Kreise und ihre √ºberlappenden Teile;
daraus lassen sich R√ºckschl√ºsse auf Rechenregeln f√ºr Wahrscheinlichkeiten ableiten.
Viele Menschen tun sich leichter, 
Rechenregeln visuell aufzufassen als mit Formeln und Zahlen alleine. Aber entscheiden Sie selbst!$\square$
:::


[Diese App](https://www.geogebra.org/m/QZvCMSDs) versinnbildlicht das Rechnen mit Relationen von Ereignissen anhand von Venn-Diagrammen.^[<https://www.geogebra.org/m/QZvCMSDs>]



### Vereinigung von Ereignissen

:::{#def-mengen-verein}
### Vereinigung von Ereignissen
Vereinigt man zwei Ereignisse $A$ und $B$, dann besteht das neue Ereignis $C$ genau aus den Elementarereignissen der vereinigten Ereignisse.
Man schreibt $C = A \cup B$, lies: "C ist A vereinigt mit B".$\square$
:::

@fig-cup zeigt ein Venn-Diagramm zur Verdeutlichung der Vereinigung von Ereignissen.

![$A \cup B$: Vereinigung](img/Venn0111.svg.png){#fig-cup width=25%}

::::::{#exm-mengen-verein}

Um einen (hohen!) Geldpreis zu gewinnen, muss bei ihrem n√§chsten Wurf mindestens eines der beiden Ereignisse $A= {1,2}$ oder $B={2,3}$ eintreten, s. @fig-venn-mengen-verein.

:::::{#fig-venn-mengen-verein}

:::: {.figure-content}


\begin{aligned}
A = \{1,2\} \qquad \boxed{\boxed{1\; 2}\; \color{gray}{ 3\; 4\; 5\; 6}} \\
B = \{2,3\} \qquad  \boxed{1\; \boxed{2\; 3}\; \color{gray}{ 4\; 5\; 6}} \\
\newline
\hline \\
A \cup B = \{1,2,3\} \qquad \boxed{\boxed{1\; 2\; 3}\; \color{gray}{4\; 5\; \boxed{6}}}
\end{aligned}
::::


Beispiel zur Vereinigung zweier Mengen

:::::


::::::


Zur besseren Verbildlichung betrachten Sie mal diese
[Animation zur Vereinigung von Mengen](https://www.geogebra.org/m/GEZV4xXc#material/cmXR8fHN); [Quelle](Geogebra, J. Merschhemke).


In R hei√üt die Vereinigung von Mengen `union()`. Praktisch zum Ausprobieren:

```{r}
A <- c(1, 2)
B <- c(2, 3)

union(A, B)
```



### (Durch-)Schnitt von Ereignissen


:::{#def-mengen-schnitt}
### Schnittmenge von Ereignissen
Die Schnittmenge zweier Ereignisse $A$ und $B$ umfasst genau die Elementarereignisse, 
die Teil beider Ereignisse sind. Man schreibt: $A \cap B.$^[Synonym und k√ºrzer: $AB$ anstelle von $A \cap B$.] Lies: "A geschnitten B". $\square$
:::

@fig-cap zeigt ein Sinnbild zur Schnittmenge zweier Ereignisse.



![$A \cap B$: Schnitt zweier Mengen](img/Venn0001.svg.png){#fig-cap width=25%}


::::::{#exm-mengen-schnitt}

Um einen (hohen!) Geldpreis zu gewinnen, muss bei ihrem n√§chsten Wurf sowohl das Ereignis $A$ = "gerade Augenzahl" als auch $B$ = "Augenzahl gr√∂√üer 4", s. @fig-mengen-schnitt.

:::::{#fig-mengen-schnitt}

:::: {.figure-content}

\begin{align}
& A = \{2,4,6\} \qquad \hfill \boxed{\color{gray}{1}\; \boxed{\color{black}{2}}\; \color{gray}{3}\; \boxed{\color{black}{4}}\; \color{gray}{5}\; \boxed{\color{black}{6}}\;} \\
& B = \{5,6\} \qquad \qquad \hfill  \boxed{ \color{gray}{1\; 2\; 3\; 4\;} \boxed{\color{black}{5\; 6}}} \\
\newline
\hline \\
& A \cap B = \{6\} \qquad \qquad \hfill  \boxed{\color{gray}{1\; 2\; 3\; 4\; 5\;} \color{black}{6}}
\end{align}
::::

Beispiel zum Schnitt zweier Mengen

:::::
::::::


```{r}
A <- c(2, 4, 6)
B <- c(5, 6)
intersect(A, B)
```



:::callout-note
#### Eselsbr√ºcke zur Vereinigungs- und Schnittmenge

Das Zeichen f√ºr eine Vereinigung zweier Mengen kann man leicht mit dem Zeichen f√ºr einen Schnitt zweier Mengen leicht verwechseln; 
daher kommt eine Eselbr√ºcke gelesen, s. @fig-esel.

![Eselsbr√ºcke f√ºr Vereinigungs- und Schnittmenge](img/ven_cup_cap.jpeg){#fig-esel width=55%}
:::


### Komplement√§rereignis


:::{#def-menge-komplement}
### Komplement√§rereignis
Ein Ereignis $A$ ist genau dann ein Komplement√§rereignis zu $B$, 
wenn es genau die Elementarereignisse von $\Omega$ umfasst, 
die nicht Elementarereignis des anderen Ereignisses sind, s. @fig-neg.$\square$
:::


Man schreibt f√ºr das Komplement√§rereignis^[synonym: Komplement] von $A$ oft $\bar{A}$ oder $\neg A$^[manchmal auch $A^C$; *C* wie *c*omplementary event]; lies "Nicht-A" oder "A-quer".


:::::{#exm-mengen-komplement}

Beim normalen W√ºrfelwurf sei $A$ das Ereignis "gerade Augenzahl"; 
das Komplement√§rereignis^[das "Komplement", nicht zu verwechseln mit "Kompliment"] 
ist dann $\neg A$ "ungerade Augenzahl", s. @fig-mengen-komplement.


::::{#fig-mengen-komplement}

::: {.figure-content}


\begin{align}
A = \{2,4,6\} \qquad \hfill \boxed{\color{gray}{1}\; \boxed{\color{black}{2}}\; \color{gray}{3}\; \boxed{\color{black}{4}}\; \color{gray}{5}\; \boxed{\color{black}{6}}\;} \\
\hline \\
\neg A = \{1,3,5\} \qquad  \hfill \boxed{\boxed{\color{black}{1}}\; \color{gray}{2}\; \boxed{\color{black}{3}}\; \color{gray}{4}\; \boxed{\color{black}{5}}\; \color{gray}{6}\; } \\
\end{align}

:::
Ein Beispiel f√ºr ein Komplement

:::::

::::::


![$\bar{A}$: Komplement](img/2560px-Venn1010.svg.png){#fig-neg width=25%}


### Logische Differenz

:::{#def-mengen-diff}
### Logische Differenz
Die logische Differenz der Ereignisse $A$ und $B$ ist das Ereignis, 
das genau aus den Elementarereignissen besteht von $A$ besteht, 
die nicht zugleich Elementarereignis von $B$ sind, s. @fig-setminus.$\square$
:::

Die logische Differenz von $A$ zu $B$ schreibt man h√§ufig so: $A \setminus B$; lies "A minus B".


![$A \setminus B$](img/Venn0100.svg.png){#fig-setminus width=25%}

:::::{#exm-mengen-setminus}

Sei $A$ die Menge "gro√üe Zahlen" mit $A = \{4,5,6 \}$.
Sei $B$ die Menge "gerade Zahlen" mit $B = \{2,4,6\}$.
Wir suchen die logische Differenz, $A \setminus B$, s. @fig-mengen-setminus.

::::{#fig-mengen-setminus}

::: {.figure-content}


\begin{align}
A = \{4,5, 6\} \qquad \hfill \boxed{\color{red}{4}\; \color{green}{5}\; \color{red}{6}} \\
B = \{2,4,6\} \qquad  \hfill \boxed{\color{grey}{2}\; \color{red}{4}\; \color{red}{6}} \\
\hline \\
A \setminus B \qquad \hfill \boxed{\color{green}{5}}
\end{align}
:::

Beispiel f√ºr die logische Differenz

::::
:::::


In R gibt es die Funktion `setdiff()`, die eine Mengendifferenz ausgibt.

```{r}
A <- c(4, 5, 6)
B <- c(2, 4, 6)

setdiff(A, B)
```

ü§Ø Von der Menge $A$ die Menge $B$ abzuziehen, ist etwas anderes, als von $B$ die Menge $A$ abzuziehen.


:::callout-caution
$A \setminus B \ne B \setminus A$.
:::

```{r}
setdiff(B, A)
```

### Disjunkte Ereignisse




Seien $A= \{1,2,3\}; B= \{4,5,6\}$.

$A$ und $B$ sind disjunkt^[engl. disjoint]: ihre Schnittmenge ist leer: $A \cap B = \emptyset$,
s. @fig-disjunkt.




![Zwei disjunkte Ereignisse, dargestellt noch √ºberlappungsfreie Kreise](img/2880px-Disjunkte_Mengen.svg.png){#fig-disjunkt width="25%" fig-align="center"}





[Quelle: rither.de](http://www.rither.de/a/mathematik/stochastik/mengentheorie-und-venn-diagramme/)


::::: {#exm-disjunkt1}
Das Ereignis $A$ "Gerade Augenzahl beim W√ºrfelwurf", $A={2,4,6}$ und das Ereignis $B$ "Ungerade Augenzahl beim W√ºrfelwurf", $B={1,3,5}$ sind disjunkt, s. @fig-disjunkt1.


::::{#fig-disjunkt1}

::: {.figure-content}


\begin{align}
A = \{2,4, 6\} \qquad \hfill \boxed{2\; 4\; 6} \\
B = \{1,3,5\} \qquad  \hfill \boxed{1\; 3\; 5} \\
\hline \\
A \cap B = \qquad  \hfill  \emptyset
\end{align}
:::

Beispiel f√ºr disjunkte Ereignisse

::::


:::::


:::{exm-disjunkt2}
Die Ereignisse "normaler Wochentag" und "Sonntag" sind disjunkt. $\square$
:::

### Vertiefung


[Animation zu Mengenoperationen](https://seeing-theory.brown.edu/compound-probability/index.html)





## Zufallsvariable


:::{#exm-thesis}
Schorsch sucht eine Betreuerin f√ºr seine Abschlussarbeit.
An die ideale Betreuerin setzt er 4 Kriterien an: 
a) klare, schriftliche fixierte Rahmenbedingungen, 
b) viel Erfahrung, 
c) guten Ruf und 
d) interessante Forschungsinteressen.
Je mehr dieser 4 Kriterien erf√ºllt sind, desto besser. 
Schorsch geht davon aus, dass die 4 Kriterien voneinander unabh√§ngig sind (ob eines erf√ºllt ist oder nicht, √§ndert nichts an der Wahrscheinlichkeit eines anderen Kriteriums).
Schorsch interessiert sich also f√ºr die *Anzahl* der erf√ºllten Kriterien, also eine Zahl von 0 bis 4.
Er sch√§tzt die Wahrscheinlichkeit f√ºr einen "Treffer" in jedem seiner 4 Kriterien auf 50%.
Viel Gl√ºck, Schorsch!
Sein Zufallsexperiment hat 16 Ausg√§nge (Knoten 16 bis 31), s. @fig-4muenzen und @tbl-schorsch-zufall. Ganz sch√∂n komplex.
Eigentlich w√ºrden ihm ja eine Darstellung mit 5 Ergebnissen, also der "Gutachter-Score" von 0 bis 4 ja reichen. 
Wie k√∂nnen wir es √ºbersichtlicher f√ºr Schorsch?$\square$
:::


 

```{r}
#| echo: false
#| label: fig-4muenzen
#| fig-cap: "Ein Baumdiagramm mit 16 Ausg√§ngen, analog zur 4 M√ºnzw√ºrfen. Jede M√ºnze ist in einer anderen Farbe dargestellt. Der Knoten '1' ist der Start, da ist noch keine M√ºnze geworfen."
my_tree <- tidygraph::create_tree(31, 2, mode = "out")

my_tree %>%
  mutate(lab = 1:31) %>%
  mutate(muenze = case_when(
    lab == 1 ~ 0,
    lab <= 3 ~ 1,
    lab <= 7 ~ 2,
    lab <= 15 ~ 3,
    TRUE ~ 4
  )) %>% 
  mutate(muenze = factor(muenze)) %>% 
  ggraph(circular = FALSE) +
  geom_edge_link() +
  geom_node_label(mapping = aes(label = lab, fill = muenze)) +
  coord_flip() +
  scale_y_reverse() +
  theme_void() +
  theme(text = element_text(size = 12)) +
  labs(fill = "M√ºnze ") +
  guides(fill = guide_legend(override.aes = aes("")))  # Suppress the 'a'
```



```{r}
#| echo: false
#| tbl-cap: Schorschs Zufallsexperiment, Auszug der Elementarereignisse
#| label: tbl-schorsch-zufall
d <- tibble::tribble(
   ~i, ~Elementarereignis, ~`Pr(EE)`, ~Trefferzahl, ~`Pr(Trefferzahl)`,
  "1",             "NNNN",    "1/16",          "0",             "1/16",
  "2",             "NNNT",    "1/16",          "1",              "1/4",
  "3",             "NNTN",    "1/16",          "1",              "1/4",
  "4",             "NTNN",    "1/16",          "1",              "1/4",
  "5",             "TNNN",    "1/16",          "1",              "1/4",
  "6",             "NNTT",    "1/16",          "2",                "‚Ä¶",
  "‚Ä¶",                "‚Ä¶",       "‚Ä¶",          "‚Ä¶",                "‚Ä¶"
  )


kable(d, booktabs = TRUE)
```


Schorsch braucht also eine √ºbersichtlichere Darstellung;
die Zahl der Treffer und ihre Wahrscheinlichkeit w√ºrde ihm ganz reichen.
In vielen Situationen ist man an der *Anzahl der Treffer* interessiert.
Die Wahrscheinlichkeit f√ºr eine bestimmte Trefferanzahl bekommt man einfach durch Addieren der Wahrscheinlichkeiten der zugeh√∂rigen Elementarereignisse, s. @tbl-schorsch-zufall.
Hier kommt die *Zufallsvariable* ins Spiel.
Wir nutzen sie, um die Anzahl der Treffer in einem Zufallsexperiment zu z√§hlen.


:::{#def-zufallsvariable}
### Zufallsvariable
Die Zuordnung der Elementarereignisse eines Zufallsexperiments zu genau einer Zahl 
$\in \mathbb{R}$ nennt man Zufallsvariable.$\square$
:::

Die den Elementarereignissen zugewiesenen Zahlen nennt man *Realisationen* oder Auspr√§gungen der Zufallsvariablen.

:::{#exm-lotto}
### Lotto
Ein Lottospiel hat ca. 14 Millionen Elementarereignisse. Die Zufallsvariable "Anzahl der Treffer" hat nur 7 Realisationen: 0,1,...,6.$\square$
:::

Es hat sich eingeb√ºrgert, Zufallszahlen mit $X$ zu bezeichnen (oder anderen Buchstaben weit hinten aus dem Alphabet).

Man schreibt f√ºr eine Zufallsvariable kurz: $X: \Omega \rightarrow \mathbb{R}$.
"X ist eine Zufallsvariable, die jedem Elementarereignis $\omega$ eine reelle Zahl zuordnet."
Um die Vorschrift der Zuordnung genauer zu bestimmen, kann man folgende Kurzschreibweise nutzen:


${\displaystyle X(\omega )={\begin{cases}1,&{\text{wenn }}\omega ={\text{Kopf}},\\[6pt]0,&{\text{wenn }}\omega ={\text{Zahl}}.\end{cases}}}$


@fig-zv stellt diese Abbildung dar.



```{mermaid}
%%| fig-cap: Eine Zufallsvariable ist eine Abbildung eines Ereignisses im Ereignisraum zu den Realisationen der Zufallsvariable. Au√üerdem sieht man, wie diskrete Wahrscheinlichkeitsfunktionen genutzt werden, um den numerischen Ausg√§ngen eines Zufallsexperiments eine Wahrscheinlichkeit zuzuordnen, d.h. um Wahrscheinlichkeiten zu bestimmen.
%%| label: fig-zv
flowchart LR
  subgraph A[Ereignis]
    Kopf
    Zahl
  end
  subgraph B[Realisation]
    null[0]
    eins[1]
  end
  subgraph C[Wahrscheinlichkeit]
    half[50%]
  end
  
  Kopf --> null
  Zahl --> eins
  null --> half
  eins --> half
```



Zufallsverteilungen kann im zwei Artein einteilen:

1. diskrete Zufallsvariablen
2. stetige Zufallsvariablen



### Diskrete Zufallsvariable

#### Grundlagen

Eine diskrete Zufallsvariable ist dadurch gekennzeichnet, dass nur bestimmte Realisationen m√∂glich sind, zumeist nat√ºrliche Zahlen, wie 0, 1, 2,..., .
@fig-zuv-disk versinnbildlicht die Zufallsvariable des "Gutachter-Scores", s. @exm-thesis.


```{r}
#| echo: false
#| fig-asp: 0.2
#| label: fig-zuv-disk
#| fig-cap: Sinnbild einer diskreten Zufallsvariablen X f√ºr Schorschs Suche nach einer Betreuerin seiner Abschlussarbeit. X gibt den Score der Gutachterin wider.
ggplot(data.frame(x=c(0:4), y = 0), aes(x,y)) +
  geom_point(size = 5, alpha = .7) +
  scale_y_continuous(limits = c(-1,1), breaks = NULL) +
  scale_x_continuous(breaks = 0:4, labels = 0:4) +
  theme_minimal() +
  labs(y = "", x = "")

```



:::{#exm-zv-disk}
### Diskrete Zufallsvariablen

- Anzahl der Bewerbungen bis zum ersten Job-Interview
- Anzahl Anl√§ufe bis zum Bestehen der Statistik-Klausur
- Anzahl der Absolventen an der HS Ansbach pro Jahr
- Anzahl Treffer beim Kauf von Losen
- Anzahl Betriebsunf√§lle
- Anzahl der Produkte in der Produktpalette$\square$
:::



:::{#exm-zweiwuerfel}
Der zweifache W√ºrfelwurf ist ein typisches Lehrbuchbeispiel f√ºr eine diskrete Zufallsvariable.
^[da einfach und deutlich]
Hier ist $S$^[S wie Summe] die Augen*s*umme des zweifachen W√ºrfelwurfs 
und $S$ ist eine Zahl zwischen 2 und 12.
F√ºr jede Realisation $X=x$ kann man die Wahrscheinlichkeit berechnen, 
@fig-zweiwuerfel-vert versinnbildlicht die Wahrscheinlichkeit f√ºr jede Realisation von $X$.$\square$
:::

![Augensumme des zweifachen W√ºrfelwurfs; f√ºr jede Realisation von S ist die zugeh√∂rige Wahrscheinlichkeit dargestellt. Bildquelle: Tim Stellmach, Wikipedia, PD](img/Dice_Distribution_(bar).svg.png){#fig-zweiwuerfel-vert width=50%}



*Wahrscheinlichkeitsverteilungen* dienen dazu, den Realisationen einer Zufallsvariablen eine Wahrscheinlichkeit zuzuordnen.








:::{#def-wvert-disk}
### Diskrete Wahrscheinlichkeitsverteilung
Eine *diskrete* Wahrscheinlichkeitsverteilung der (diskreten) Zufallsvariablen $X$ ordnet jeder der $k$ Auspr√§gungen $X=x$ eine Wahrscheinlichkeit $p$ zu.$\square$
:::

:::{#exm-babies}
### Wahrscheinlichkeit des Geschlechts bei der Geburt
So hat die Variable *Geschlecht eines Babies* die beiden Auspr√§gungen *M√§dchen* und *Junge* mit den Wahrscheinlichkeiten $p_M = 51.2\%$ bzw. $p_J = 48.8\%$, laut einer Studie [@gelman2021].$\square$
:::


Zwischen der deskriptiven Statistik und der Wahrscheinlichkeitstheorie bestehen enge Parallelen, @tbl-wkeit-desk stellt einige zentrale Konzepte gegen√ºber.

```{r}
#| echo: false
#| label: tbl-wkeit-desk
#| tbl-cap: "Gegen√ºberstellung von Wahrscheinlichkeitstheorie und deskriptiver Statistik"
d <- tibble::tribble(
         ~Wahrscheinlichkeitstheorie,                      ~Desktiptive.Statistik,
                   "Zufallsvariable",                                   "Merkmal",
                "Wahrscheinlichkeit",               "relative H√§ufigkeit, Anteil",
       "Wahrscheinlichkeitsfunktion",   "einfache relative H√§ufigkeitsverteilung",
               "Verteilungsfunktion", "kumulierte relative H√§ufigkeitsverteilung",
                    "Erwartungswert",                                "Mittelwert",
                           "Varianz",                                   "Varianz"
       )
gt::gt(d)
```




```{r}
#| echo: false
#| message: false
#| warning: false
dice_outcomes <- expand.grid(Die1 = 1:6, Die2 = 1:6)

# Calculate the sum of the two dice for each outcome
dice_outcomes$Sum <- dice_outcomes$Die1 + dice_outcomes$Die2

# Calculate the probability of each sum using the table function
sum_counts <- table(dice_outcomes$Sum)
total_outcomes <- sum(sum_counts)
probabilities <- sum_counts / total_outcomes

twodice <- tibble(
  Augensumme = 2:12,
  p = probabilities) |> 
  mutate(p_cum = cumsum(p))

p_twodice <- 
  ggplot(twodice, aes(x = Augensumme, y = p)) + 
  geom_col() +
  geom_label(aes(y = p, label = round(p, 2), nudge_y = .1)) +
  scale_x_continuous(breaks = 1:12)
```




```{r}
#| echo: false
#| message: false
#| warning: false
#| 
num_trials <- 1000  # You can change this to the desired number of trials

# Simulate repeated throws of two dice
results <- replicate(num_trials, {
  die1 <- sample(1:6, 1, replace = TRUE)  # Simulate the first die
  die2 <- sample(1:6, 1, replace = TRUE)  # Simulate the second die
  c(Die1 = die1, Die2 = die2)  # Return the results as a vector
}) |> 
  t() |> 
  as_tibble() |> 
  mutate(Augensumme  = Die1 + Die2)

# Display

results_count <-
  results |> 
  count(Augensumme) |> 
  mutate(prop = n/num_trials) |> 
  mutate(n_cum = cumsum(n),
         prop_cum = cumsum(prop))

p_sim2dice <-
  ggplot(results_count) +
  aes(x = Augensumme, y = n) +
  geom_col() +
  geom_label(aes(y = n, label = round(prop, 2))) +
  scale_x_continuous(breaks = 1:12)
```





Eine *Verteilung* zeigt, welche Auspr√§gungen eine Variable aufweist und wie h√§ufig bzw. wahrscheinlich diese sind. 
Einfach gesprochen veranschaulicht eine Balken- oder Histogramm eine Verteilung. Man unterscheidet H√§ufigkeitsverteilungen (s. Abb. @fig-2dice-sim) von Wahrscheinlichkeitsverteilungen (Abb. @fig-2dice-prob).




:::: {.columns}

::: {.column width="50%"}
```{r}
#| echo: false
#| fig-cap: Wahrscheinlichkeitsverteilung der Zufallsvariable "Augenzahl im zweifachen W√ºrfelwurf"
#| label: fig-2dice-prob
p_twodice
```

:::

::: {.column width="50%"}
```{r}
#| echo: false
#| fig-cap: (relative und absolute) H√§ufigkeiten des zweifachen W√ºrfelwurfs, 1000 Mal wiederholt
#| label: fig-2dice-sim
p_sim2dice
```
:::

::::



:::{#exm-wert-wuerfel}
### Wahrscheinlichkeitsfunktion eines W√ºrfels
@fig-w-wuerfel zeigt die Wahrscheinlichkeitsfunktion eines einfachen W√ºrfelwurfs.$\square$
:::

![Wahrscheinlichkeitsfunktion eines einfachen W√ºrfelwurfs, Bildrechte: Olex Alexandrov, Wikipedia, PD](img/220px-Fair_dice_probability_distribution.svg.png){#fig-w-wuerfel width=33%}




:::{#exm-mtcars)
<!-- ### H√§ufigkeitsverteilung bei `mtcars` -->

Die H√§ufigkeitsverteilung eines *diskreten* Merkmals $X$ mit $k$ Auspr√§gungen zeigt (vgl. @tbl-hauef-tab),
wie h√§ufig die einzelnen Auspr√§gungen sind.
So hat die Variable *Zylinder* (in einem Datensatz) etwa die Auspr√§gungen 4,6 und 8.$\square$
:::


:::: {.columns}

::: {.column width="70%"}


```{r}
#| echo: false
#| fig-cap: "H√§ufigkeitsverteilung von `cyl` und `hp` (diskretisiert in 10 K√∂rbe oder Gruppen)"
#| label: fig-mtcars-freq
p1 <- 
  mtcars %>% 
  ggplot(aes(x = cyl)) +
  geom_bar()


p2 <- mtcars %>% 
  ggplot(aes(x = hp)) +
  geom_histogram(bins=10)

plots(p1, p2, n_rows = 1)
```
  
:::

::: {.column width="30%"}
```{r }
#| eval: true
#| echo: false
#| tbl-cap: Eine diskrete H√§ufigkeitsverteilung, dargestellt in einer H√§ufigkeitstabelle
#| label: tbl-hauef-tab
data(mtcars)
  mtcars %>% 
    count(cyl)
```
:::

::::



  
  
Abb. @fig-mtcars-freq, links, visualisiert die H√§ufigkeitsverteilung von `cyl`.
Ein *stetiges* Merkmal, wie `hp` (PS-Zahl), l√§sst sich durch Klassenbildung in ein diskretes umwandeln (diskretisieren), s. Abb. @fig-mtcars-freq, rechts.

#### Wahrscheinlichkeitsfunktion

:::{#def-w-fun}
### Wahrscheinlichkeitsfunktion

Die Funktion $f$, die den m√∂glichen Realisationen $x_i$ der diskreten Zufallsvariablen $X$ die Eintrittswahrscheinlichkeiten zuordnet, hei√üt Wahrscheinlichkeitsfunktion.$\square$
:::


:::{#exm-w-fun}
Die Wahrscheinlichkeitsfunktion f√ºr $X$ "Augensumme im zweifachen W√ºrfelwurf" ist in @fig-2dice-prob visualisiert.$\square$ 
:::


:::{#exm-muenz}
Die Wahrscheinlichkeitsfunktion f√ºr $X$ "Treffer im einfachen M√ºnzwurf, mit Zahl ist Treffer" ist $Pr(X=1)=1/2.$, vgl. @fig-zv.$\square$
:::

üí° Einfach gesprochen gibt die Wahrscheinlichkeitsfunktion die Wahrscheinlichkeit einer bestimmten Realisation einer Zufallsvariable an.

#### Verteilungsfunktion



:::{#def-vert-fun}
### Verteilungsfunktion
Die Verteilungsfunktion $F$ gibt die Wahrscheinlichkeit an, dass die diskrete Zufallsvariable $X$ eine Realisation annimmt, die kleiner oder gleich $x$ ist.$\square$
:::



Die Berechnung von $F(x)$ erfolgt, indem die Wahrscheinlichkeiten aller m√∂glichen Realisationen $x_i$, die kleiner oder gleich dem vorgegebenen Realisationswert $x$ sind, addiert werden:

$F(x) = \sum_{x_ \le x} Pr(X=x_i).$


```{r}
#| echo: false
p_F <- 
  ggplot(twodice, aes(x = Augensumme, y = p_cum)) + 
  geom_col() +
  geom_line() +
  geom_label(aes(label = round(p_cum, 2))) + 
  scale_x_continuous(breaks = 1:12) +
  labs(y = "Verteilungsfunktion F")


y_lab <- "empirische Verteilungsfunktion F emp."

p_F_emp <-
  ggplot(results_count) +
  aes(x = Augensumme, y = prop_cum) +
  geom_col() +
  geom_line() +
  geom_label(aes(y = prop_cum, label = round(prop_cum, 2))) +
  labs(y = y_lab) +
  scale_x_continuous(breaks = 2:12)
```


Die Verteilungsfunktion ist das Pendant zur *kumulierten H√§ufigkeitsverteilung*, vgl. @fig-kum-h-vert und @fig-kum-h-vert-emp:
Was die kumulierte H√§ufigkeitsverteilung f√ºr H√§ufigkeiten ist, ist die Verteilungsfunktion f√ºr Wahrscheinlichkeiten.


:::: {.columns}

::: {.column width="50%"}
```{r}
#| echo: false
#| fig-cap: Verteilungsfunktion $F(X \le x_i)$ f√ºr die Zufallsvariable "Augenzahl im zweifachen W√ºrfelwurf"
#| label: fig-kum-h-vert
p_F
```

:::

::: {.column width="50%"}
```{r}
#| echo: false
#| fig-cap: Empirische Verteilungsfunktion (kumulierte H√§ufigkeitsverteilung) $F(X \le x_i)$ von 1000 zweifachen M√ºnzw√ºrfen
#| label: fig-kum-h-vert-emp
p_F_emp 
```

:::

::::


### Stetige Zufallsvariablen


üì∫ [Verteilungen metrischer Zufallsvariablen](https://www.youtube.com/watch?v=7GqIE4sKDs4&list=PLRR4REmBgpIGgz2Oe2Z9FcoLYBDnaWatN&index=4)

@fig-zv-stetig-groesse versinnbildlicht die stetige Zufallsvariable "K√∂rpergr√∂√üe", die (theoretisch, in Ann√§herung) jeden beliebigen Wert zwischen 0 und (vielleicht) 2 Meter annehmen kann.

```{r echo = FALSE}
#| fig-cap: Sinnbild f√ºr eine stetige Zufallsvariable X "K√∂rpergr√∂√üe"
#| label: fig-zv-stetig-groesse
#| fig-asp: 0.2
 
ggplot(data.frame(x=0, y = 0), aes(x,y)) +
  #geom_point(size = 5, alpha = .7) +
  scale_y_continuous(limits = c(-1,1), breaks = NULL) +
  scale_x_continuous(breaks = c(0, 50, 100, 150, 200)) +
  annotate("segment", x = 0, xend = 200, y = 0, yend = 0, color = "red")  +
  theme_minimal() +
  annotate("label", x = 200, y = 0, label = "...") +
  labs(y = "", x = "")
```


:::{#def-zv-stetig}
### Stetige Zufallsvariable
Eine stetige Zufallsvariable gleicht einer diskreten, nur dass alle Werte im Intervall erlaubt sind.$\square$
:::


:::{#exm-zu-stetig}
- Spritverbrauch
- K√∂rpergewicht von Professoren
- Schnabell√§ngen von Pinguinen
- Geschwindigkeit beim Geblitztwerden$\square$
:::


:::{#exr-bus-42}
### Warten auf den Bus, 42 Sekunden
Sie stehen an der Bushaltestellen und warten auf den Bus.
Langweilig.
Da kommt Ihnen ein Gedanken in den Sinn: 
Wie hoch ist wohl die Wahrscheinlichkeit, dass Sie *exakt* 42 Sekunden auf den Bus warten m√ºssen, s. @fig-p42?
Weiterhin √ºberlegen Sie, dass davon auszugehen ist, dass jede Wartezeit zwischen 0 und 10 Minuten gleich wahrscheinlich ist.
Sp√§testens nach 10 Minuten kommt der Bus, so ist die Taktung (extrem zuverl√§ssig).
Exakt hei√üt *exakt*, also nicht 42.1s, nicht 42.01s, nicht 42.001s, etc. bis zur x-ten Dezimale.$\square$
:::


Nicht so einfach (?). Hingegen ist die Frage, wie hoch die Wahrscheinlichkeit ist, zwischen 0 und 5 Minuten auf den Bus zu warten ($0<x<5$), einfach: Sie betr√§gt 50%, wie man in @fig-p05 gut sehen kann.




:::: {.columns}

::: {.column width="50%"}

![Wie gro√ü ist die Wahrscheinlichkeit, zwischen 0 und 5 Minuten auf den Bus zu warten? 50 Prozent!](img/p_bus2.png){#fig-p05}


:::

::: {.column width="50%"}
```{r}
#| echo: false
#| fig-cap: "Wie gro√ü ist die Wahrscheinlichkeit, genau 42 Sekunden auf den Bus zu warten? Hm."
#| label: fig-p42
#| warning: false
p_bus1 <- 
  uniform_Plot(0, 10) + 
  geom_vline(xintercept = .42, color = "#56B4E9FF", size = 1) +
  annotate("label", x = .42, y = .05, hjust = 0, label = "Pr(X=0.42)=?", color="#56B4E9FF") +
  annotate("point", x = .42, y = 0, size = 5, color = "#56B4E9FF", alpha = .7) +
  annotate("label", x= 5, y = 0.1, label = "f(x) = 1/10", color = "#009E73FF", size = 10)

p_bus1
```
:::

::::

Vergleicht man @fig-p42 und @fig-p05 kommt man (vielleicht) zu dem Schluss, dass die Wahrscheinlichkeit exakt 42s auf den Bus zu warten, praktisch Null ist.
Der Grund ist, dass die Fl√§che des Intervalls gegen Null geht, wenn das Intervall immer schm√§ler wird.
Aus diesem Grund kann man bei stetigen Zufallszahlen nicht von einer Wahrscheinlichkeit eines bestimmten Punktes $X=x$ sprechen.
F√ºr einen bestimmten Punkt $X=x$ kann man aber die *Dichte* der Wahrscheinlichkeit angeben.

Was  gleich ist in beiden Situationen ($Pr(X=.42)$ und $Pr(0<x<0.5)$) ist die *Wahrscheinlichkeitsdichte*, $f$.
In @fig-p42 und @fig-p05 ist die Wahrscheinlichkeitsdichte gleich, $f=1/10=0.1$.

:::{#def-wdichte}
### Wahrscheinlichkeitsdichte
Die Wahrscheinlichkeitsdichte $f(x)$ gibt an, wie viel Wahrscheinlichkeitsmasse pro Einheit von $X$ an an der Stelle $x$ ist.$\square$
:::


Die Wahrscheinlichkeitsdichte zeigt an, an welchen Stellen $x$ die Wahrscheinlichkeit besonders "geballt" oder "dicht" sind, s. @fig-wdichte-sinnbild.

![Die Wahrscheinlichkeit, dass eine Zufallsvariable einen Wert zwischen und annimmt, entspricht dem Inhalt der Fl√§che unter dem Graph der Wahrscheinlichkeitsdichtefunktion. Bildrechte: 4C, Wikipedia, CC-BY-SA .](img/260px-Integral_as_region_under_curve.svg.png){#fig-wdichte-sinnbild width=33%}





::::: {.columns}

:::: {.column width="50%"}
:::{#def-vert-fun-stetig}
### Verteilungsfunktion
Die Verteilungsfunktion einer stetigen Zufallsvariablen gibt wie im diskreten Fall an,
wie gro√ü die Wahrscheinlichkeit f√ºr eine Realisation kleiner oder gleich einem vorgegebenen Realisationswert $x$ ist.$\square$

Die Verteilungsfunktion $F(x)$ ist analog zur kumulierten H√§ufigkeitsverteilung zu verstehen, vgl. @fig-F-Bus. $\square$
:::


::::

:::: {.column width="50%"}



```{r}
#| echo: false
#| fig-cap: 'Verteilungsfunktion F f√ºr X="Wartezeit auf den Bus"'
#| label: fig-F-Bus
#| fig-asp: 0.4
d <- 
  tibble(x=1:10,
         y= 1:10/10) 

ggplot(d, aes(x,y)) +
  geom_point(alpha = .5) +
  geom_line() +
  scale_x_continuous(breaks = 1:10) +
  theme_minimal()
```




::::

:::::






:::{#def-wvert-stetig}
### Stetige Wahrscheinlichkeitsverteilung
Bei *stetigen* Zufallsvariablen $X$ geht man von unendlich vielen Auspr√§gungen aus; die Wahrscheinlichkeit einer bestimmten Auspr√§gung ist Null: $Pr(X=x_j)=0, \quad j=1,...,+\infty \square$.
:::


:::{#exm-groesse}
### Wahrscheinlichkeitsverteilung f√ºr die K√∂rpergr√∂√üe
So ist die Wahrscheinlichkeit, dass eine Person exakt 166,66666666... cm gro√ü ist, (praktisch) Null.
Man gibt stattdessen die *Dichte* der Wahrscheinlichkeit an: Das ist die Wahrscheinlichkeit(smasse) pro  Einheit von $X$.$\square$
:::


F√ºr praktische Fragen berechnet man zumeist die Wahrscheinlichkeit von Intervallen, s. @fig-wdichte-sinnbild.




## Aufgaben

Bearbeiten Sie die Aufgabe in der angegeben Literatur.

Die Webseite [datenwerk.netlify.app](https://datenwerk.netlify.app) stellt eine Reihe von einschl√§gigen √úbungsaufgaben bereit. Sie k√∂nnen die Suchfunktion der Webseite nutzen, 
um die Aufgaben mit den folgenden Namen zu suchen:


### Paper-Pencil-Aufgaben

