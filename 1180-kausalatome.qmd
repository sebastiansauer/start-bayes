
# Die Atome des Kausalit√§t {#sec-kausal2}

## Lernsteuerung


\newcommand{\indep}{\perp \!\!\! \perp}



### Position im Modulverlauf

@fig-modulverlauf gibt einen √úberblick zum aktuellen Standort im Modulverlauf.



### R-Pakete


F√ºr dieses Kapitel ben√∂tigen Sie folgende R-Pakete:

```{r libs}
#| warning: false
library(tidyverse)
library(rstanarm)
library(easystats)
```



```{r libs-hidden}
#| echo: false
library(gt)
#library(DT)
library(ggdag)
library(dagitty)  # DAGs zeichnen

theme_set(theme_modern())
```

### Daten  {#sec-kausal2-daten}


Wir nutzen den Datensatz [Saratoga County](https://vincentarelbundock.github.io/Rdatasets/csv/mosaicData/SaratogaHouses.csv); s. @tbl-saratoga.
Hier gibt es eine 
[Beschreibung des Datensatzes](https://vincentarelbundock.github.io/Rdatasets/doc/mosaicData/SaratogaHouses.html).

{{< downloadthis data/SaratogaHouses.csv dname="SaratogaHouses" >}}

Sie k√∂nnen ihn entweder √ºber die Webseite herunterladen:

```{r data-saratoga, echo = TRUE}
#| eval: false
SaratogaHouses_path <- "https://vincentarelbundock.github.io/Rdatasets/csv/mosaicData/SaratogaHouses.csv"

d <- read.csv(SaratogaHouses_path)
```

Oder aber √ºber das Paket `mosaic` importieren:

```{r}
data("SaratogaHouses", package = "mosaicData")
d <- SaratogaHouses  # k√ºrzerer Name, das ist leichter zu tippen
```



### Lernziele


Nach Absolvieren des jeweiligen Kapitels sollen folgende Lernziele erreicht sein.

Sie k√∂nnen ... 

- erkl√§ren, wann eine Kausalaussage gegeben eines DAGs berechtigt ist
- erkl√§ren, warum ein statistisches Modell ohne Kausalmodell zumeist keine Kausalaussagen treffen kann
- die ‚ÄúAtome‚Äù der Kausalit√§t eines DAGs benennen
- ‚Äúkausale Hintert√ºren‚Äù schlie√üen

### Begleitliteratur

Dieses Kapitel vermittelt die Grundlagen der Kausalinferenz mittels graphischer Modelle. √Ñhnliche Darstellungen wie in diesem Kapitel findet sich bei @rohrer_thinking_2018.







## Kollision




üì∫ [Kollision](https://www.youtube.com/watch?v=RymOFwVU3PQ)





### Kein Zusammenhang von Intelligenz und Sch√∂nheit (?)

[Gott ist gerecht (?)](https://twitter.com/TheTweetOfGod/status/1462594155176026123)



Zumindest findet sich in folgenden Daten kein Zusammenhang von Intelligenz (`talent`) und Sch√∂nheit (`looks`),
wie @fig-iq-schoen illustriert.
F√ºr geringe Intelligenzwerte gibt es eine breites Spektrum von Sch√∂nheitswerten
und f√ºr hohe Intelligenzwerte sieht es genauso aus.

```{r p-coll1}
#| echo: false
#| out-width: "50%"
#| label: fig-iq-schoen
#| fig-cap: "Kein Zusammenhang von Intelligenz und Sch√∂nheit in den Daten"

myf <- function(x) -x+0.75

myf2 <- function(x) -x + 1.25

n <- 1e3

d2 <- tibble(
  x = runif(n),
  y = runif(n),
  status = case_when(
    y > myf(x) & y < myf2(x) ~ TRUE,
    TRUE ~ FALSE
  )
)


p_coll1 <-
  d2 %>% 
  ggplot() +
  aes(x  = x,
      y = y) +
  geom_point() +
 # scale_color_manual(values = c("grey80", "black")) +
  theme_bw() +
  labs(x = "Looks",
       y = "Talent") +
    theme(legend.position = "bottom",
          axis.text = element_blank())

p_coll1
```




### Aber Ihre Dates sind entweder schlau oder sch√∂n

Seltsamerweise beobachten Sie, dass die Menschen, die Sie daten (Ihre Dates),
entweder sch√∂n sind oder schlau - aber seltens beides gleichzeitig (schade),
s. @fig-dates-beauty.


```{r p-coll2}
#| echo: false
#| label: fig-dates-beauty
#| fig-cap: Ihre Datingpartner sind komischerweise entweder schlau oder sch√∂n (aber nicht beides), zumindest in der Tendenz.
#| out-width: "50%"
p_coll2 <- 
  d2 %>% 
  ggplot() +
  aes(x  = x,
      y = y,
      color = status) +
  geom_point() +
  scale_color_manual(values = c("grey80", "black")) +
  theme_bw() +
  labs(x = "Looks",
       y = "Talent") +
    theme(legend.position = "bottom",
          axis.text = element_blank())

p_coll2
```

Wie kann das sein?




### DAG zur Rettung 

ü¶π ü¶∏

Der DAG in @fig-coll1-dag bietet eine rettende Erkl√§rung.




```{r plot-coll-dag1}
#| echo: false
#| fig-cap: "Date als gemeinsame Wirkung von Sch√∂nheit und Intelligenz. Stratifiziert man die gemeinsame Wirkung (dates), so kommt es zu einer Scheinkorrelation zwischen Sch√∂nheit und Intelligenz."
#| out-width: "50%"
#| label: fig-coll1-dag

coll1_dag <-
  dagify(date ~ Looks + Talent)

p_coll_dag1 <- 
coll1_dag %>% 
  ggdag() +
  theme_dag()

p_coll_dag1
```


Eine √§hnliche Visualisierung des gleichen Sachverhalts zeigt @fig-coll2-dag.

```{r p-coll-dag22, fig.width=9}
#| echo: false
#| label: fig-coll2-dag
#| fig-cap: "Durch Kontrolle der gemeinsamen Wirkung entsteht eine Scheinkorrelation zwischen den Ursachen"
p_coll_dag2 <-
  collider_triangle(x = "Looks",
                  y = "Talent",
                  m = "date") %>% 
  ggdag_dseparated(controlling_for = "m",
                   text = TRUE,
                   use_labels = "label") +
  theme_dag()

p_coll_dag2
```




```{r eval = FALSE}
#| echo: false
#| eval: false
ggdag_adjust(coll1_dag, var = "date") +
  theme_dag()
```



### Was ist eine Kollision?



:::{#def-collision}
### Kollision
Als *Kollision* (Kollisionsverzerrung, Auswahlverzerrung, engl. collider) bezeichnet man einen DAG, bei dem eine Wirkung zwei Ursachen hat (eine gemeinsame Wirkung zweier Ursachen) [@pearl_causal_2016, p. 40]. $\square$
:::

Kontrolliert man  die *Wirkung* `m`, so entsteht eine *Scheinkorrelation* zwischen den Ursachen `x` und `y`.
Kontrolliert man die Wirkung *nicht*, so entsteht *keine Scheinkorrelation* zwischen den Ursachen, s. @fig-coll1-dag, vgl. @rohrer_thinking_2018.


::: callout-important
Man kann also zu viele oder falsche Pr√§diktoren einer Regression hinzuf√ºgen, so dass die Koeffizienten nicht die kausalen Effekte zeigen, sondern durch Scheinkorrelation verzerrte Werte.
:::

::: callout-tip
üôÖ‚Äç‚ôÄÔ∏è Kontrollieren Sie keine Kollisionsvariablen. $\square$
:::




### Einfaches Beispiel zur Kollision



In der Zeitung *Glitzer* werden nur folgende Menschen gezeigt:

- Sch√∂ne Menschen ü™û
- Reiche Menschen ü§ë 
    
Sehen wir davon aus, dass Sch√∂nheit und Reichtum unabh√§ngig voneinander sind.


:::{#exr-coll1}
Wenn ich Ihnen sage, dass Don nicht sch√∂n ist, aber in der Glitzer h√§ufig auftaucht, was lernen wir dann √ºber seine finanzielle Situation?^[Don muss reich sein.] $\square$
:::




>   "Ich bin sch√∂n, unglaublich sch√∂n, und gro√ü, gro√üartig, tolle Gene!!!" üßë





### Noch ein einfaches Beispiel zur Kollision

>   "So langsam check ich's!" üßë^[Super, Don!]



Sei Z = X + Y, wobei X und Y unabh√§ngig sind.

Wenn ich Ihnen sage, X = 3, lernen Sie nichts √ºber Y, da die beiden Variablen unabh√§ngig sind
 Aber: Wenn ich Ihnen zuerst sage, Z = 10, und dann sage, X = 3, wissen Sie sofort, was Y ist (Y = 7).

Also: X und Y sind abh√§ngig, gegeben Z: $X \not\indep Y \,|\, Z$.^[Der horizontale Balken "|" bedeutet "gegeben, dass". Ein Beispiel lautet $Pr(A|B)$: "Die Wahrscheinlichkeit von A, gegeben dass B der Fall ist.]


### Durch Kontrollieren entsteht eine Verzerrung bei der Kollision


@fig-coll1-dag zeigt: Durch Kontrollieren entsteht eine Kollision,
eine Scheinkorrelation zwischen den Ursachen.

*Kontrollieren* kann z.B. bedeuten:

- *Stratifizieren*: Aufteilen von `date` in zwei Gruppen und dann Analyse des Zusammenhangs von `talent` und `looks` in jeder Teilgruppe von `date`
- *Kontrollieren mit Regression*: Durch Aufnahme von `date` als Pr√§diktor in eine Regression zus√§tzlich zu `looks` mit `talent` als Pr√§dikotr


*Ohne* Kontrolle von `date` entsteht *keine* Scheinkorrelation zwischen `Looks` und `Talent`. Der Pfad ("Fluss") von `Looks` √ºber `date` nach `Talent` ist blockiert.

Kontrolliert man `date`, so *√∂ffnet* sich der Pfad `Looks -> date -> talent` und die Scheinkorrelation entsteht: Der Pfad ist nicht mehr "blockiert", die
Korrelation kann "flie√üen" - was sie hier nicht soll,
denn es handelt sich um Scheinkorrelation.

Das Kontrollieren von `date` geht zumeist durch Bilden einer Auswahl einer Teilgruppe von sich.




### IQ, Fleiss und Eignung f√ºrs Studium


Sagen wir, √ºber die *Eignung* f√ºr ein Studium w√ºrden nur (die individuellen Auspr√§gungen) von Intelligenz (IQ) und Fleiss entscheiden, s. den DAG in @fig-coll3-dag.

```{r coll32-dag}
#| echo: false
#| label: fig-coll3-dag
#| fig-cap: Kollisionsstruktur im Dag zur Studiumseignung
coll2_dag <- ggdag::dagify(s ~ d + iq,
                      outcome = "s",
                      labels = c("s" = "Studium",
                                 "iq" = "Intelligenz",
                                 "d" = "Fleiss"))

p_coll_dag2 <- ggdag(coll2_dag, use_labels = "label")  + theme_dag_blank()
p_coll_dag2

# coll2_dag <-
#   dagify(eignung ~ fleiss + iq)
# 
# p_coll_dag2 <- 
# coll2_dag %>% 
#   ggdag() +
#   theme_dag()
# 
# p_coll_dag2
```

Bei positiver `eignung` wird ein Studium aufgenommen (`studium = 1`) ansonsten nicht (`studium = 0)`. 


[Quelle](https://data-se.netlify.app/2020/04/16/simulation-berkson-s-paradox/)



`eignung` (f√ºrs Studium) sei definiert als die Summe von `iq` und `fleiss`, plus etwas Gl√ºck, s. @lst-studium.

```{r d-eignung, echo = TRUE}
#| lst-label: lst-studium
#| lst-cap: Eignung ist die Summe von Fleiss und Intelligenz, plus ein Quentchen Gl√ºck
set.seed(42)  # Reproduzierbarkeit
N <- 1e03  

d_eignung <-
tibble(
  iq = rnorm(N),  # normalverteilt mit MW=0, sd=1
  fleiss = rnorm(N),
  glueck = rnorm(N, mean = 0, sd = .1),
  eignung = 1/2 * iq + 1/2 * fleiss + glueck,
  # nur wer geeignet ist, studiert (in unserem Modell):
  studium = ifelse(eignung > 0, 1, 0) 
  )
```

Laut unserem Modell setzt sich Eignung zur H√§lfte aus Intelligenz und zur H√§lfte aus Fleiss zusammen, plus etwas Gl√ºck.



### Schlagzeile "Flei√ü macht bl√∂d!"

Eine Studie untersucht den Zusammenhang von Intelligenz (iq) und Flei√ü (f) bei Studentis (s).
Ergebnis: Ein *negativer* Zusammenhang!?



Berechnen wir das "Eignungsmodell", aber nur mit Studis (`studium == 1`, also ohne Nicht-Studis), 
s. @tbl-m-eignung.

```{r}
#| results: hide
m_eignung <-
  stan_glm(iq ~ fleiss, data = d_eignung %>%  filter(studium == 1), refresh = 0)

hdi(m_eignung)
```

```{r}
#| tbl-cap: "Zum Zusammenhang von Fleiss und Talent"
#| label: tbl-m-eignung
#| echo: false
hdi(m_eignung) |> display()
```




@fig-eignung zeigt das Modell und die Daten.

```{r}
#| echo: false
#| label: fig-eignung
#| fig-cap: "Der Zusammenhang von Fleiss und IQ"

d_eignung %>% 
  filter(studium == 1) %>% 
  ggplot(aes(x = fleiss, y = iq)) +
  geom_point(alpha = .5) +
  geom_smooth(method = "lm") +
  labs(title = "Negativer Zusammenhang von Fleiss und IQ bei Studentis",
       subtitle = "Macht Fleiss bl√∂d?")
```


IQ ist *nicht* unabh√§ngig von Flei√ü in unseren Daten, sondern abh√§ngig.
Nichtwissenschaftliche Berichte, etwa in einigen Medien,
greifen gerne Befunde √ºber Zusammenh√§nge auf und interpretieren die Zusammenh√§nge
-- oft vorschnell -- als kausal.^[Ehrlicherweise muss man zugeben, dass auch wissenschaftliche Berichte Daten √ºber Zusammenh√§nge gerne kausal interpretieren, oft vorschnell.]





### Kollisionsverzerrung nur bei Stratifizierung

:::{#def-stratifizieren}
### Stratifizieren
Durch Stratifizieren wird eine Stichprobe in (homogene) Untergruppen unterteilt (sog. *Strata*). $\square$
:::

Nur durch das Stratifizieren (Aufteilen in Subgruppen, Kontrollieren, Adjustieren) tritt die Scheinkorrelation auf, s. @fig-eignung-strat.


:::callout-note
*Ohne* Stratifizierung tritt *keine* Scheinkorrelation auf.
*Mit* Stratifizierung tritt Scheinkorrelation auf.
:::



```{r}
#| echo: false
#| fig-cap: "Stratifizierung und Scheinkorrelation. A: Keine Stratifizierung und keine Scheinkorrelation. B: Stratifizierung und Scheinkorrelation"
#| label: fig-eignung-strat
p1 <- d_eignung %>% 
 ggplot(aes(x = fleiss, y = iq)) +
  geom_point(alpha = .5) +
  geom_smooth(method = "lm")

p2 <- 
d_eignung %>% 
  mutate(studium = factor(studium)) %>% 
  ggplot(aes(x = fleiss, y = iq, color = studium)) +
  geom_point(alpha = .5) +
  geom_smooth(method = "lm") +
  scale_color_okabeito() +
  scale_fill_okabeito()


plots(p1, p2,
      tags = "A",
      title = c("Kein Stratifizierung, keine Scheinkorrelation",
               "Mit Stratifizierung gibt es Scheinkorrelation"))
```




Wildes Kontrollieren einer Variablen - Aufnehmen in die Regression - kann genausog ut schaden wie n√ºtzen.

*Nur* Kenntnis des DAGs verr√§t die richtige Entscheidung: ob man eine Variable kontrolliert oder nicht.


:::callout-note
Nimmt man eine Variable als zweiten Pr√§diktor auf,
so "kontrolliert" man diese Variable. Das Regressiongewicht des ersten Pr√§diktors wird "bereinigt" um den Einfluss des zweiten Pr√§diktors; insofern ist der zweite Pr√§diktor dann "kontrolliert".
:::




### Einfluss von Gro√üeltern und Eltern auf Kinder





Wir wollen hier den (kausalen) Einfluss der Eltern `E` und Gro√üeltern `G` auf den *Bildungserfolg* der Kinder `K` untersuchen.

Wir nehmen folgende Effekte an:

- indirekter Effekt von `G` auf `K`: $G \rightarrow E \rightarrow K$
- direkter Effekt von `E` auf `K`: $E \rightarrow K$
- direkter Effekt von `G` auf `K`: $G \rightarrow K$


Wir sind v.a. interessiert an $G \rightarrow K$, dem *direkten kausalen* Effekt von Gro√üeltern auf ihre Enkel, s. @fig-dag-grannies [@kurz_statistical_2021], $G \rightarrow K$.





```{r}
#| echo: false
#| label: fig-dag-grannies
#| fig-cap: "Der kausale Effekt von Gro√üeltern auf Enkel. Ein verlorener Fall, zumindest was den DAG betrifft"
dag_coords <-
  tibble(name = c("G", "E", "K"),
         x    = c(1, 2, 2),
         y    = c(2, 2, 1))

dagify(E ~ G,
       K ~ E + G,
       coords = dag_coords) %>%
  ggdag() +
  theme_dag()
```


Aber was ist, wenn wir vielleicht eine *u*nbekannte Variable √ºbersehen haben? (S. n√§chster Abschnitt). üëª







### Der Gespenster-DAG

üëª Es gibt "unheilbare" DAGs, nennen wir sie "Gespenster-DAGs",
in denen es nicht m√∂glich ist, einen (unverzerrten) Kausaleffekt zu bestimmen,
s. @fig-dag-ghost.
Letztlich sagt uns der DAG bzw. unsere Analyse zum DAG:
"Deine Theorie ist nicht gut, zur√ºck an den Schreibtisch und denk noch mal gut nach. 
Oder sammele mehr Daten."



```{r}
#| echo: false
#| label: fig-dag-ghost
#| fig-cap: "Der Gespenster-DAG: Eine Identifikation der Kausaleffekt ist nicht (vollst√§ndig) m√∂glich."
#| out-width: "50%"

source("funs/gg_fancy_dag.R")

coll4_dag <-
  dagitty("dag
          {
          G -> E
          E -> K
          G -> K
          U -> E
          U -> K
          }
          ")

dag_coords <-
  tibble(name = c("G", "E", "K", "U"),
         x    = c(1, 2, 2, 2.5),
         y    = c(2, 2, 1, 1.5))

dagify(E ~ G + U,
       K ~ E + G + U,
       coords = dag_coords) %>% 
  gg_fancy_dag(x = 2.5, y = 1.5, circle = "U")
```



`U` k√∂nnte ein ungemessener Einfluss sein, der auf `E` und `K` wirkt, etwa *Nachbarschaft*.
Die Gro√üeltern wohnen woanders (in Spanien), daher wirkt die Nachbarschaft der Eltern und Kinder nicht auf sie.
`E` ist sowohl f√ºr `G` als auch f√ºr `U` eine Wirkung, also eine Kollisionsvariable auf diesem Pfad.
Wenn wir `E` kontrollieren, wird es den Pfad $G \rightarrow K$ verzerren, auch wenn wir niemals `U` messen.


Die Sache ist in diesem Fall chancenlos. Wir m√ºssen diesen DAG verloren geben, @mcelreath2020, S. 180; ein Gespenster-DAG. üëª 


















## Die Hintert√ºr schlie√üen



:::{#def-hintertuer}
### Hintert√ºr

Eine "Hintert√ºr"  ist ein nicht-kausaler Pfad zwischen einer UV und einer AV. 
Ein Hintert√ºrpfad entsteht, wenn es eine alternative Route √ºber eine oder mehrere Variable gibt, die UV mit der AV verbindet. Dieser Pfad verzerrt die Sch√§tzwerte des kausalen Einflusses, wenn er nicht kontrolliert wird. $\square$
:::



### Zur Erinnerung: Konfundierung





*Forschungsfrage*: Wie gro√ü ist der (kausale) Einfluss der Schlafzimmerzahl auf den Verkaufspreis des Hauses?

`a:` livingArea, `b`: bedrooms, `p`: prize

UV: `b`, AV: `p`


Das Kausalmodell ist in @fig-dag-don1 dargestellt.

```{r}
#| echo: false
#| label: fig-dag-don1
#| fig-cap: "Der Preis wird sowohl von der Zimmerzahl als auch der Wohnfl√§che beeinflusst"
#| out-width: "50%"

source("funs/gg_simple_dag.R")


dag_coords <-
  tibble(name = c("a", "b", "p"),
         x    = c(0, -1, 1),
         y    = c(1, 0, 0))

dagify(p ~ a + b,
       b ~ a,
       coords = dag_coords) %>%
  gg_simple_dag()
```



 Im Regressionsmodell `p ~ b` wird der kausale Effekt verzerrt sein durch die Konfundierung mit `a`.
 Der Grund f√ºr die Konfundierung sind die zwei Pfade zwischen `b` und `p`:
 
1. $b \rightarrow p$
2. $b \leftarrow a \rightarrow p$

Beide Pfade erzeugen (statistische) Assoziation zwischen `b` und `p`.
Aber nur der erste Pfad ist kausal; der zweite ist nichtkausal.
G√§be es nur nur den zweiten Pfad und wir w√ºrden `b` √§ndern, so w√ºrde sich `p` *nicht* √§ndern.



### Gute Experimente zeigen den echten kausalen Effekt

@fig-dag-tuer-zu zeigt eine erfreuliche Situation:
Die "Hintert√ºr" zu unserer UV (Zimmerzahl) ist geschlossen!

Ist die Hintert√ºr geschlossen - f√ºhren also keine Pfeile in unserer UV -
so kann eine Konfundierung ausgeschlossen werden.

```{r}
#| echo: false
#| label: fig-dag-tuer-zu
#| fig-cap: "Unverzerrte Sch√§tzung des kausalen Effekts unserer UV (Zimmerzahl). Das Regressionsgewicht ist hier der unverzerrte Kausaleffekt. Es spielt keine Rolle, ob der andere Pr√§diktor im Modell enthalten ist. Da die beiden Pr√§diktoren unkorreliert sind, hat die Aufnahme des einen Pr√§diktors keinen Einfluss auf das Regressionsgewicht des anderen."
#| out-width: "50%"
dag_coords <-
  tibble(name = c("a", "b", "p"),
         x    = c(0, -1, 1),
         y    = c(1, 0, 0))

dagify(p ~ a + b,
       coords = dag_coords) %>%
  gg_simple_dag()
```

Die "Hintert√ºr" der UV (`b`) ist jetzt zu!
Der einzig verbleibende, erste Pfad ist der kausale Pfad und die Assoziation zwischen `b` und `p` ist jetzt komplett kausal.


Eine ber√ºhmte L√∂sung, den kausalen Pfad zu isolieren, ist ein (randomisiertes, kontrolliertes^[engl. randomized, controlled trial, RCT]) Experiment.
Wenn wir den H√§usern zuf√§llig (randomisiert) eine Anzahl von Schlafzimmern (`b`) zuweisen k√∂nnten (unabh√§ngig von ihrer Quadratmeterzahl, `a`), w√ºrde sich der Graph so √§ndern.
Das Experiment *entfernt* den Einfluss von `a` auf `b`.
Wenn wir selber die Werte von `b` einstellen im Rahmen des Experiments, so kann `a` keine Wirkung auf `b` haben.
Damit wird der zweite Pfad, $b \leftarrow a \rightarrow p$ geschlossen ("blockiert").


::: callout-important
Die St√§rke von (gut gemachten) Experimente ist, dass sie kausale Hintert√ºren schlie√üen. Damit erlauben sie (korrekte) Kausalaussagen. $\square$
:::



### Hintert√ºr schlie√üen auch ohne Experimente

Konfundierende Pfade zu blockieren zwischen der UV und der AV nennt man auch *die Hintert√ºr schlie√üen* (backdoor criterion).
Wir wollen die Hintert√ºre schlie√üen, da wir sonst nicht den wahren, kausalen Effekt bestimmen k√∂nnen.

Zum Gl√ºck gibt es neben Experimenten noch andere Wege, die Hintert√ºr zu schlie√üen, wie die Konfundierungsvariable `a` in eine Regression mit aufzunehmen.

::: callout-tip
Kontrollieren Sie Konfundierer, um kausale Hintert√ºren zu schlie√üen. $\square$
:::


Warum blockt das Kontrollieren von `a`den Pfad $b \leftarrow a \rightarrow p$?
Stellen Sie sich den Pfad als eigenen Modell vor.
Sobald Sie `a` kennen, bringt Ihnen Kenntnis √ºber `b` kein zus√§tzliches Wissen √ºber `p`.
Wissen Sie hingegen nichts √ºber `a`, lernen Sie bei Kenntnis von `b` auch etwas √ºber `p`.
Konditionieren ist wie "gegeben, dass Sie `a` schon kennen...".

$b \indep p \,|\,a$



## Die vier Atome der Kausalanalyse


@fig-four-atoms stellt die vier "Atome" der Kausalinferenz dar.
Mehr gibt es nicht!
Kennen Sie diese vier Grundbausteine,
so k√∂nnen Sie jedes beliebige Kausalsystem (DAG) entschl√ºsseln.




```{r}
#| echo: false
p_conf <- confounder_triangle(x = NULL, y = NULL, z = NULL, x_y_associated = FALSE) %>% 
  gg_simple_dag() +
  labs(title = "Die Konfundierung")
```

```{r}
#| echo: false
p_med <- 
  mediation_triangle(x = NULL, y = NULL, m = NULL, x_y_associated = FALSE) %>% 
  gg_simple_dag() +
  labs(title = "Die Mediation")
```


```{r}
#| echo: false
p_coll <- collider_triangle(x = NULL, y = NULL, m = NULL, x_y_associated = FALSE) %>% 
  gg_simple_dag() +
  labs(title = "Die Kollision")
```



```{r}
#| echo: false
dag_desc <- 
  dagitty('
          dag{
          
          m [pos="1.000,0.000"]
          x [exposure,pos="0.000,1.000"]
          y [outcome,pos="2.000,1.000"]
          d [pos="1,1"]

          x -> m
          y -> m
          m -> d
          }')

p_desc <-
  dag_desc %>%
  gg_simple_dag() +
  labs(title ="Der Nachfahre")
```





```{r}
#| echo: false
#| label: fig-four-atoms
#| fig-cap: Die vier Atome der Kausalinferenz
#| fig-width: 9
plots(p_conf, p_med, p_coll, p_desc, n_rows = 2)
```





### Mediation


:::{#def-mediator}
### Mediator
Einen Pfad mit drei Knoten (Variablen), die √ºber insgesamt zwei Kanten verbunden sind, wobei die Pfeile von UV zu Mediator und von Mediator zur AV zeigen, nennt man *Mediation*. Der *Mediator* ist die Variable zwischen UV und AV [@pearl_causal_2016; p. 38]. $\square$
:::

Die *Mediation* (synonym: Wirkkette, Rohr, Kette, chain) beschreibt Pfade, in der die Kanten (eines Pfades) die gleiche Wirkrichtung haben: $x \rightarrow m \rightarrow y$. 
Anders gesagt: Eine Mediation ist eine Kausalabfolge der Art $x \rightarrow m \rightarrow y$, s. @fig-med1.
Die Variable in der Mitte $m$ der Kette wird auch *Mediator* genannt,
weil sei die Wirkung von X auf Y "vermittelt" oder √ºbertr√§gt.
Die Erforschung von Mediation spielt eine recht wichtige Rolle in einigen Wissenschaften,
wie der Psychologie.

```{r}
#| echo: false
#| label: fig-med1
#| fig-cap: Das Kausalmodell der Mediation mit x als UV, m als Mediator und Y als AV.
p_med
```


:::{#exm-mediator}
### Mediator kontrollieren?
Sollte man den Mediator `m` in @fig-med1 kontrollieren, wenn man den Kausaleffekt von `x` auf `y` sch√§tzen m√∂chte?^[Nein, durch das Kontrollieren von `m` wird der Kausalpfad von `x` zu `y` geschlossen. Es kann keine kausale Assoziation von `x` auf `y` mehr "flie√üen".] $\square$
:::


Ohne Kontrollieren ist der Pfad offen: Die Assoziation "flie√üt" den Pfad entlang (in beide Richtungen).
Kontrollieren blockt (schlie√üt) die Kette (genau wie bei der Gabel).

::: callout-tip
Kontrollieren Sie den Mediator *nicht*. Der Pfad √ºber den Mediator ist ein "echter" Kausalpfad, keine Scheinkorrelation. $\square$
:::

::: callout-important
Das Kontrollieren eines Mediators ist ein Fehler, wenn man am gesamten (totalen) Kausaleffekt von UV zu AV interessiert ist. $\square$
:::



Es kann auch angenommen werden, dass der Mediator nicht der einzige Weg von X zu Y ist, s. @fig-part-med.
In @fig-part-med gibt es zwei kausale Pfade von X zu Y: $x\rightarrow m \rightarrow y$ und $x \rightarrow y$. 

:::{#def-effekt}
### Effekt
Gibt es eine (von (praktisch) Null verschiedene) kausale Assoziation der UV auf die AV, so h√§ngt die AV von der UV (kausal) ab. Man spricht von einem Effekt (der UV auf die AV). $\square$
:::

:::{#def-tce}
### Totaler Effekt
Die Summe der Effekte aller (kausalen) Pfade von UV zu AV nennt man den *totalen (kausalen) Effekt*. $\square$
:::

:::{#def-ind-eff}
### Indirekter Effekt
Den (kausalen) Effekt √ºber den Mediatorpfad (von $X$ √ºber $M$ zu $Y$) nennt man den *indirekten (kausalen) Effekt*. $\square$
:::

:::{#def-dir-eff}
### Direkter Effekt
Ein Effekt, der nur aus dem Pfad $x\rightarrow y$ besteht, also ohne keine Zwischenglieder, nennt man in Abgrenzung zum indirekten Effekt, *direkten (kausalen) Effekt*. $\square$
:::


```{r}
#| label: fig-part-med
#| fig-cap: "Partielle Mediation: Es gibt einen direkten Effekt (X->Y) und einen indirekten Effekt (X->M->Y)."
#| echo: false
dag <- 
  dagitty('
          dag{
          
          m [pos="1.000,0.000"]
          x [pos="0.000,1.000"]
          y [pos="2.000,1.000"]
         

          x -> m
          x -> y
          m -> y
          }')

gg_fancy_dag(dag)
```


### Der Nachfahre



:::{#def-Nachfahre}
### Nachfahre
Ein *Nachfahre* (engl. descendent) ist eine Variable, die von einer anderen Variable beeinflusst^[beeinflussen ist grunds√§tzlich kausal zu verstehen] wird, s. @fig-dag-nachfahre. $\square$
:::

Kontrolliert man einen Nachfahren `d`, so kontrolliert man damit zum Teil den Vorfahren (die Ursache), `m`.
Der Grund ist, dass `d` Information beinhaltet √ºber `m`.
Hier wird das Kontrollieren von `d` den Pfad von `x` nach `y` teilweise √∂ffnen, da `m` eine Kollisionsvariable ist.



```{r}
#| echo: false
#| label: fig-dag-nachfahre
#| fig-cap: "Ein Nachfahre verh√§lt sich √§hnlich wie sein Vorfahre..."
#| out-width: "50%"
p_desc
```





### Kochrezept zur Analyse von DAGs üë®‚Äçüç≥ 

Wie kompliziert ein DAG auch aussehen mag, er ist immer aus diesen vier Atomen aufgebaut.

Hier ist ein Rezept, das garantiert, dass Sie welche Variablen Sie kontrollieren sollten und welche nicht: üìÑ 

1. Listen Sie alle Pfade von UV (`X`) zu AV (`Y`) auf.
2. Beurteilen Sie jeden Pfad, ob er gerade geschlossen oder ge√∂ffnet ist.
3. Beurteilen Sie f√ºr jeden Pfad, ob er ein Hintert√ºrpfad ist (Hintert√ºrpfade haben einen Pfeil, der zur UV f√ºhrt).
4. Wenn es ge√∂ffnete Hinterpfade gibt, pr√ºfen Sie, welche Variablen mann kontrollieren muss, um den Pfad zu schlie√üen (falls m√∂glich).




## Schlie√üen Sie die Hintert√ºr (wenn m√∂glich)!





üì∫ [Hintert√ºr schlie√üen](https://www.youtube.com/watch?v=Hns1UIYKTds)


### Hintert√ºr: ja oder nein?

#### Fall 1: `x->`

$\boxed{X \rightarrow}$

Alle Pfade, die von der UV (`X`) *wegf√ºhren*, sind entweder "gute" Kausalpfade oder automatisch geblockte Nicht-Kausal-Pfade. In diesem Fall m√ºssen wir nichts tun.^[Denken Sie daran, dass Sie keine Nachkommen der UV kontrollieren d√ºrfen, da das den Kausalpfad von der UV zur AV blockieren k√∂nnte.]

#### Fall 2: `->x`
$\boxed{\rightarrow X}$

Alle Pfade, die zur UV *hinf√ºhren*, sind immer Nicht-Kausal-Pfade, *Hintert√ºren*. Diese Pfade k√∂nnen offen sein, dann m√ºssen wir sie schlie√üen. Sie k√∂nnen auch geschlossen sein, dann m√ºssen wir nichts tun.

::: callout-tip
Schlie√üen Sie immer offene Hintert√ºren, um Verzerrungen der Kausaleffekte zu verhinden. $\square$
:::

### `bsp1`

UV: $X$, AV: $Y$, drei Kovariaten (A, B, C) und ein ungemessene Variable, U






```{r gg-fancey-dag, out.width="50%"}
#| echo: false
#| label: fig-dag-fancy
#| fig-cap: "Puh, ein schon recht komplizierter DAG"
dag_coords <-
  tibble(name = c("A", "B", "C", "U", "X", "Y"),
         x    = c(2, 2, 3, 1, 1, 3),
         y    = c(4, 2, 3, 3, 1, 1))

dagify(B ~ C + U,
       C ~ A,
       U ~ A,
       X ~ U,
       Y ~ C + X,
       coords = dag_coords) %>%
  gg_fancy_dag(x = 1, y = 3, circle = "U")
```



Es gibt zwei Hintert√ºrpfade in @fig-dag-fancy:

1. $X \leftarrow U \leftarrow A \rightarrow C \rightarrow Y$, offen
2. $X \leftarrow U \rightarrow B \leftarrow C \rightarrow Y$, geschlossen

Kontrollieren von $A$ oder (auch) $C$ schlie√üt die offene Hintert√ºr.



@mcelreath2020, @kurz2021, s.S. 186.





### Schlie√üen Sie die Hintert√ºr (wenn m√∂glich)!, `bsp2`

S. DAG in @fig-dag-bsp2: UV: $W$, AV: $D$

```{r bsp2}
#| echo: false
#| label: fig-dag-bsp2
#| fig-asp: 0.5
#| fig-cap: "Welche Variablen muss man kontrollieren, um den Effekt von W auf D zu bestimmen?"
dag_coords <-
  tibble(name = c("A", "D", "M", "S", "W"),
         x    = c(1, 3, 2, 1, 3),
         y    = c(1, 1, 2, 3, 3))

dagify(A ~ S,
       D ~ A + M + W,
       M ~ A + S,
       W ~ S,
       coords = dag_coords) %>%
  gg_simple_dag()
```



Kontrollieren Sie diese Variablen, um die offenen Hintert√ºren zu schlie√üen:

- entweder $A$ und $M$
- oder $S$

[Mehr Infos](https://bookdown.org/content/4857/the-haunted-dag-the-causal-terror.html#backdoor-waffles.)



Details finden sich bei @mcelreath2020 oder @kurz2021, S. 188.



### Implizierte bedingte Unabh√§ngigkeiten von `bsp2`

<!-- Ein Graph ohne `U`s ist eine starke - oft zu starke (unrealistisch optimistische) - Annahme. -->
Auch wenn die Daten nicht sagen k√∂nnen, welcher DAG der richtige ist, 
k√∂nnen wir zumindest lernen, welcher DAG falsch ist.
Die vom Modell implizierten bedingten Unabh√§ngigkeiten geben uns M√∂glichkeiten, 
zu pr√ºfen, ob wir einen DAG verwerfen (ausschlie√üen) k√∂nnen.
Bedingten Unabh√§ngigkeit zwischen zwei Variablen sind Variablen, 
die nicht assoziiert (also stochastisch unabh√§ngig) sind, 
wenn wir eine bestimmte Menge an Drittvariablen kontrollieren.

`bsp2` impliziert folgende bedingte Unabh√§ngigkeiten:

```{r bsp2-cond-independence}
#| echo: false
dag_6.2 <- 
  dagitty(
    "dag {
    A -> D
    A -> M -> D
    A <- S -> M
    S -> W -> D
    }"
  )

impliedConditionalIndependencies(dag_6.2)
```



## Fazit

### Ausstieg

üì∫ [Musterl√∂sung f√ºr eine DAG-Pr√ºfungsaufgabe](https://www.youtube.com/watch?v=lHUEURFjW78)

üì∫ [Musterl√∂sung f√ºr schwierige DAG-Pr√ºfungsaufgaben](https://www.youtube.com/watch?v=5HKmOdzuDXw)

:::{#exm-pmi-kausal1}
### PMI zum heutigen Stoff
Der Kreativit√§tsforscher Edward de Bono hat verschiedene "Denkmethoden" vorgestellt, die helfen sollen, Probleme besser zu l√∂sen.
Eine Methode ist die "PMI-Methode". 
PMI steht f√ºr *Plus*, *Minus*, *Interessant*. 
Bei Plus und Minus soll man eine Bewertung von Positiven bzw. Negativen bzgl. eines Sachverhaltes anf√ºhren. Bei *Interessant* verzichtet man aber explizit auf eine Bewertung (im Sinne von "gut" oder "schlecht") und fokussiert sich auf Interessantes, √úberraschendes, Bemerkenswertes.

F√ºhren Sie die PMI-Methode zum heutigen Stoff durch!

1. *Plus*: Was fanden Sie am heutigen Stoff gut, sinnvoll, n√ºtzlich?
2. *Minus*: Was finden Sie am heutigen Stoff nicht gut, sinvoll, n√ºtzlich?
3. *Interessant*: Was finden Sie am heutigen Stoff bemerkenswert, interessant, nachdenkenswert?

Reichen Sie die Antworten an der von der Lehrkraft angezeigten Stelle ein! $\square$
:::


### Zusammenfassung


üì∫ [Kausalmodelle √ºberpr√ºfen](https://www.youtube.com/watch?v=JEHgAJEdIa0)

Wie (und sogar ob) Sie statistische Ergebnisse (z.B. eines Regressionsmodells) interpretieren k√∂nnen, h√§ngt von der *epistemologischen Zielrichtung* der Forschungsfrage ab:

- Bei *deskriptiven* Forschungsfragen k√∂nnen die Ergebnisse (z.B. Regressionskoeffizienten) direkt interpretiert werden. Z.B. "Der Unterschied zwischen beiden Gruppen betr√§gt etwa ...". Allerdings ist eine kausale Interpretation nicht zul√§ssig.
- Bei *prognostischen* Fragestellungen (Vorhersagen) spielen die Modellkoeffizienten keine Rolle, stattdessen geht es um vorhergesagten Werte, $\hat{y}_i$, z.B. auf Basis der PPV. Kausalaussagen sind zwar nicht m√∂glich, aber auch nicht von Interesse.
- Bei *kausalen* Forschungsfragen d√ºrfen die Modellkoeffizienten nur auf Basis eines Kausalmodells (DAG) oder eines (gut gemachten) Experiments interpretiert werden.
    
Modellkoeffizienten √§ndern sich (oft), wenn man Pr√§diktoren zum Modell hinzuf√ºgt oder wegnimmt.
Entgegen der verbreiteten Annahme ist es falsch, m√∂glichst viele Pr√§diktoren in das Modell aufzunehmen, wenn das Ziel eine Kausalaussage ist.
Kenntnis der "kausalen Atome" ist Voraussetzung zur Ableitung von Kausalschl√ºsse in Beobachtungsstudien.


### Vertiefung

An weiterf√ºhrender Literatur sei z.B. @cummiskey2020, @lubke2020a,
@pearl2016 und @dablander2020 empfohlen.
Ein gutes Lehrbuch, das auf Kausalinferenz eingeht, ist @huntington-klein2022. Praktischerweise ist es [√∂ffentlich lesbar](https://theeffectbook.net/ch-CausalPaths.html).
Das Web-Buch [Causal Inference for the Brave and True](https://matheusfacure.github.io/python-causality-handbook/landing-page.html) sieht auch vielversprechend aus.
Es gibt viele Literatur zu dem Thema; relevante Google-Suchterme sind z.B. "DAG", "causal" oder "causal inference".


## Aufgaben


- [Sammlung "kausal"](https://datenwerk.netlify.app/#category=causal)








## ---



![](img/outro-14.jpg){width=100%}








